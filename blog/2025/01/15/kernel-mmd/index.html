<!DOCTYPE html>
<html>
<head>
    <title>Kernel-MMD-Hypothesis Testing without assumptions</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>
    <meta name='description' content='Gregory Gundersen is a PhD candidate at Princeton.'>
    <meta name='keywords' content=''>
    <meta name='author' content='Gregory Gundersen'>
    
    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>
    
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>
    
    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
</head>
<body>
    <div class='content'>
        <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
        <div class='front-matter'>
            <div class='wrap'>
                <h1>Kernel-MMD-Hypothesis Testing without assumptions</h1>
                <h4>What to do if you can't assume anything about your distributions.</h4>
                <div class='bylines'>
                    <div class='byline'>
                        <h3>Published</h3>
                        <p>15 January 2025</p>
                    </div>
                </div>
                <div class='clear'></div>
            </div>
        </div>
        <div class='wrap article'>
            <ul><li><a href="#introduction">Introduction</a></li><li><a href="#maximum-mean-discrepancy-mmd">Maximum Mean Discrepancy (MMD)</a></li><li><a href="#reproducing-kernel-hilbert-spaces">Reproducing Kernel Hilbert Spaces</a></li><li><a href="#kernel-mmd">Kernel MMD</a></li></ul>
            <h1 id="introduction">Introduction</h1>

<p><em>The</em> basic problem in statistics is comparing two samples of measurements. The basic issue is that you want to know whether these measurements came from the same source, or from different sources. In statistical language, we are comparing the null distribution—that the measurements came from the same source—against the alternative hypothesis —that the measurements come from distinct sources.</p>

<p>Very often statisticians assume some parametric form for the distributions. For example, normal distributions with known unknown means and the same variance. So we would be comparing:</p>

\[H_0: \mu_1 = \mu_2\]

<p>Against</p>

\[H_0: \mu_1 \neq \mu_2\]

<p>There are some downsides to this approach. Firstly, we are imposing an assumption on the data—that the measurements come from a distribution with a specific form—and we can never check the accuracy of that assumption. That is if we have gotten the correct parametric form the distributions. Secondly, the specific distributional form we assume is also unknowable. So we can never really know how wrong we are.</p>

<p>The issue then is to find solutions to the following problem, without making assumptions about the data generating process of the measurements.</p>

<p><em>Problem 1</em> Let $x$ and $y$ be random variables defined on some topological space $\mathcal{X}$, with respective probability measures $p$ and $q$. Given two sets of observations $X = \left(x_1, x_2, \ldots, x_m \right)$ and $Y = \left(_1, y_2, \ldots, y_n \right)$ drawn i.i.d from $p$ and $q$ respectively, can we decide whether $p \neq q$?</p>

<h1 id="maximum-mean-discrepancy-mmd">Maximum Mean Discrepancy (MMD)</h1>

<p>In general, MMD is defined by the idea of representing distances between distributions as distances between mean embeddings of features. We can define the notion of distance between probability distirbutions with the following lemma (from <a class="citation" href="#dudley2018real">(Dudley, 2018)</a>).</p>

<p><em>Lemma 1</em> Let $\left(\mathcal{X}, d\right)$ be a metric space, and let $p$ and $q$ be two probability measures defined on $\mathcal{X}$. Then $p = q$ if and only if $\mathcal{E}_x\left(f\left(x\right)\right) = \mathcal{E}_y\left(f\left(y\right)\right)$ for all $f \in C\left(\mathcal{X}\right)$ where $C\left(\mathcal{X}\right)$ is the collection of bounded, continuous functions on $\mathcal{X}$.</p>

<p>The maximum mean discrepancy (MMD) is then defined based on a this class of functions:</p>

<p><em>Definition 1</em> Let $\mathcal{F}$ be a class of functions $f: \mathcal{F} \rightarrow \mathcal{R}, and let $p, q, x, y, X, Y$ be defined as earlier. The MMD is then:</p>

\[MMD(p,q) := \mathrm{sup}_{f \in \mathcal{F}} \left( \mathcal{E}_x[f(x)] - \mathcal{E}_y[f(y)] \right)\]

<p>An empirical estimate of the MMD can be found by replacing the population expectations with their sample expectations based on the samples $X$ and $Y$ (although this is slightly biased):</p>

\[\hat{MMD(p,q)} := \mathrm{sup}_{f \in \mathcal{F}} \left( \frac{1}{m}\sum_{i=1}^m f(x_i) - \frac{1}{n}\sum_{i=1}^n f(y_i) \right)\]

<p>Although $C\left(\mathcal{X}\right)$ allows us to identify if $p = q$, the space is too rich. It is not computationally practical to work in this space. Instead we will work with a function class which can identify whether $p = q$ but is restrictive enough to provide useful estimates with finite samples.</p>

<h1 id="reproducing-kernel-hilbert-spaces">Reproducing Kernel Hilbert Spaces</h1>

<p>A Reproducing Kernel Hilbert Space (RKHS) is a space $\mathcal{X}$ of functions, equipped with a norm (i.e. a <a href="https://en.wikipedia.org/wiki/Hilbert_space">Hilbert space</a>). There is a function $\phi(x)$ which takes points in $\mathcal{X}$ to $\mathcal{R}$. We denote this function $ f(x) = \langle f, \phi(x) \rangle$. We can write $\phi(x) = k(x, \dot)$. In particular</p>

\[k(x, y) = \langle \phi(x), \phi(y) \rangle\]

<p>Speaking informally, an RKHS is just a space where every point in the space is a linear combination of (positive-definite) kernels. This allows us to replace the inner product calculation in this space with the kernel evaluation. We can, in particular, extend the notion of a feature map to the embedding of a probabuility distribution. Let $ \mu_p \in \mathcal{H}$ be such that $\mathcal{E}<em>x[f] = \langle f, \mu_p \rangle$. We call this the _mean embedding</em> of $p$. Then, the MMD may be expressed as the distance between mean embeddings in $\mathcal{H}$/</p>

<h1 id="kernel-mmd">Kernel MMD</h1>

<p>$\phi:\mathcal{X} \rightarrow \mathcal{H}$, where $\mathcal{H}$ is some Hilbert space; this corresponds to a kernel by $k\left(x,y\right)=\left⟨\phi(x),\phi(y)\right⟩$. The MMD is:</p>

\[MMD^2\left[p, q \right] = \left\lVert \mu_p - \mu_q \right\rVert^2\]

<p>And we can obtain the MMD in terms of the RKHS kernel functions:</p>

\[MMD^2\left[p, q \right] =  \mathcal{E}_{x, x'}[k(x, x')] - 2\mathcal{E}_{x, y}[k(x, y)] + \mathcal{E}_{y, y'}[k(y, y')]\]

<p>This is relatively easy to compute (implmentation from <a href="https://torchdrift.org/notebooks/note_on_mmd.html">torchdrift</a>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">mmd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="c1"># compare kernel MMD paper and code:
</span>    <span class="c1"># A. Gretton et al.: A kernel two-sample test, JMLR 13 (2012)
</span>    <span class="c1"># http://www.gatsby.ucl.ac.uk/~gretton/mmd/mmd.htm
</span>    <span class="c1"># x shape [n, d] y shape [m, d]
</span>    <span class="c1"># n_perm number of bootstrap permutations to get p-value, pass none to not get p-value
</span>    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">d2</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">d</span> <span class="o">==</span> <span class="n">d2</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">detach</span><span class="p">()],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">xy</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="c1"># we are a bit sloppy here as we just keep the diagonal and everything twice
</span>    <span class="c1"># note that sigma should be squared in the RBF to match the Gretton et al heuristic
</span>    <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">dists</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="mf">1e-5</span>
    <span class="n">k_x</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="n">n</span><span class="p">]</span>
    <span class="n">k_y</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="n">n</span><span class="p">:,</span> <span class="n">n</span><span class="p">:]</span>
    <span class="n">k_xy</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">:]</span>
    <span class="c1"># The diagonals are always 1 (up to numerical error, this is (3) in Gretton et al.)
</span>    <span class="c1"># note that their code uses the biased (and differently scaled mmd)
</span>    <span class="n">mmd</span> <span class="o">=</span> <span class="n">k_x</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">k_y</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k_xy</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mmd</span>
</code></pre></div></div>

<p><a class="citation" href="#gretton2012kernel">(Gretton et al., 2012)</a> recommends to set the $\sigma$ parameter to the median distance between points:</p>

\[\sigma = \frac{\mathrm{Median}\left(z_i - z_j\right)}{2}\]

<p>where $Z$ is the combined sample of and $X$ and $Y$. We have also used the Gaussian Radial Basis as the choice of kernel.</p>

<p>We can extend this implementation to any kernel with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">pdist</span><span class="p">,</span> <span class="n">squareform</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">mmd_biased</span><span class="p">(</span><span class="n">XX</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">YY</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">XY</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="s">"""
    Compute biased MMD^2 statistic.
    
    Args:
        XX: Kernel matrix for first sample
        YY: Kernel matrix for second sample
        XY: Cross kernel matrix between samples
    
    Returns:
        float: Biased MMD^2 statistic
    """</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">XX</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">YY</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">YY</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">n</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">XY</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mmd_unbiased</span><span class="p">(</span><span class="n">XX</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">YY</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">XY</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="s">"""
    Compute unbiased MMD^2 statistic.
    
    Args:
        XX: Kernel matrix for first sample
        YY: Kernel matrix for second sample
        XY: Cross kernel matrix between samples
    
    Returns:
        float: Unbiased MMD^2 statistic
    """</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">XX</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">YY</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">term1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="n">XX</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">m</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">term2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">YY</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="n">YY</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">term3</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">n</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">XY</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">term1</span> <span class="o">+</span> <span class="n">term2</span> <span class="o">-</span> <span class="n">term3</span>

<span class="k">def</span> <span class="nf">mmd2test</span><span class="p">(</span><span class="n">K</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">],</span> 
             <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"b"</span><span class="p">,</span> <span class="n">mc_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">999</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="s">"""
    Kernel Two-sample Test with Maximum Mean Discrepancy.
    
    Maximum Mean Discrepancy (MMD) as a measure of discrepancy between samples
    is employed as a test statistic for two-sample hypothesis test of equal 
    distributions. Kernel matrix K is a symmetric square matrix that is positive
    semidefinite.
    
    Args:
        K: Kernel matrix (symmetric, positive semidefinite)
        label: Label vector of class indices
        method: Type of estimator to be used. "b" for biased and "u" for unbiased
        mc_iter: Number of Monte Carlo resampling iterations
    
    Returns:
        dict: Dictionary containing test results with keys:
            - statistic: Test statistic
            - p_value: p-value under H0
            - alternative: Alternative hypothesis
            - method: Name of the test
    """</span>
    <span class="c1"># Preprocessing
</span>    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">K</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"K should be a square matrix"</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"K should be symmetric"</span><span class="p">)</span>
    
    <span class="c1"># Check if K is positive semidefinite
</span>    <span class="n">min_eigenval</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">linalg</span><span class="p">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">min_eigenval</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Warning: K may not be PD. Minimum eigenvalue is </span><span class="si">{</span><span class="n">min_eigenval</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    
    <span class="c1"># Process labels
</span>    <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">unique_labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"label should contain exactly 2 classes"</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">)</span> <span class="o">!=</span> <span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Length of label must match size of kernel matrix"</span><span class="p">)</span>
    
    <span class="c1"># Compute statistic
</span>    <span class="n">id1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="n">unique_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">id2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="n">unique_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">id1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">id2</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">method</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s">"b"</span><span class="p">:</span>
        <span class="n">stat</span> <span class="o">=</span> <span class="n">mmd_biased</span><span class="p">(</span><span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">id1</span><span class="p">,</span> <span class="n">id1</span><span class="p">)],</span> <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">id2</span><span class="p">,</span> <span class="n">id2</span><span class="p">)],</span> <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">id1</span><span class="p">,</span> <span class="n">id2</span><span class="p">)])</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># method == "u"
</span>        <span class="n">stat</span> <span class="o">=</span> <span class="n">mmd_unbiased</span><span class="p">(</span><span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">id1</span><span class="p">,</span> <span class="n">id1</span><span class="p">)],</span> <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">id2</span><span class="p">,</span> <span class="n">id2</span><span class="p">)],</span> <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">id1</span><span class="p">,</span> <span class="n">id2</span><span class="p">)])</span>
    
    <span class="c1"># Monte Carlo iterations
</span>    <span class="n">iter_vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mc_iter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mc_iter</span><span class="p">):</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">tmp_id1</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[:</span><span class="n">m</span><span class="p">]</span>
        <span class="n">tmp_id2</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="n">m</span><span class="p">:]</span>
        
        <span class="k">if</span> <span class="n">method</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s">"b"</span><span class="p">:</span>
            <span class="n">iter_vals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmd_biased</span><span class="p">(</span><span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">tmp_id1</span><span class="p">,</span> <span class="n">tmp_id1</span><span class="p">)],</span> 
                                    <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">tmp_id2</span><span class="p">,</span> <span class="n">tmp_id2</span><span class="p">)],</span> 
                                    <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">tmp_id1</span><span class="p">,</span> <span class="n">tmp_id2</span><span class="p">)])</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># method == "u"
</span>            <span class="n">iter_vals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmd_unbiased</span><span class="p">(</span><span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">tmp_id1</span><span class="p">,</span> <span class="n">tmp_id1</span><span class="p">)],</span> 
                                      <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">tmp_id2</span><span class="p">,</span> <span class="n">tmp_id2</span><span class="p">)],</span> 
                                      <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">tmp_id1</span><span class="p">,</span> <span class="n">tmp_id2</span><span class="p">)])</span>
    
    <span class="n">p_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">iter_vals</span> <span class="o">&gt;=</span> <span class="n">stat</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">mc_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s">'statistic'</span><span class="p">:</span> <span class="p">{</span><span class="s">'MMD'</span><span class="p">:</span> <span class="n">stat</span><span class="p">},</span>
        <span class="s">'p_value'</span><span class="p">:</span> <span class="n">p_value</span><span class="p">,</span>
        <span class="s">'alternative'</span><span class="p">:</span> <span class="s">"two distributions are not equal"</span><span class="p">,</span>
        <span class="s">'method'</span><span class="p">:</span> <span class="s">"Kernel Two-sample Test with Maximum Mean Discrepancy"</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>We can create an example to explain how to use the code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create Beta distributions and generate samples
</span><span class="n">x</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>

<span class="c1"># Points for plotting the density curves
</span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">density_x</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">).</span><span class="n">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> 
<span class="n">density_y</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">).</span><span class="n">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># Plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s">'+'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">density_x</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">density_y</span><span class="p">)</span> 
</code></pre></div></div>

<p><img src="/assets/images/kernel-mmd/example_mmd_data.png" alt="Two distribution" title="Example MMD data" /></p>

<p>Now we analyse the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reshape the data to be 2D arrays
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape becomes (15, 1)
</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape becomes (15, 1)
</span>
<span class="c1"># Combine data and compute distance matrix
</span><span class="n">combined_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
<span class="n">distances</span> <span class="o">=</span> <span class="n">squareform</span><span class="p">(</span><span class="n">pdist</span><span class="p">(</span><span class="n">combined_data</span><span class="p">))</span>

<span class="c1"># Compute median distance for kernel bandwidth
</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">median</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using median bandwidth: </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Build Gaussian kernel matrix with scaled distances and small regularization
</span><span class="n">kernel_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">distances</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">kernel_matrix</span> <span class="o">+=</span> <span class="mf">1e-15</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_matrix</span><span class="p">))</span>  <span class="c1"># Small regularization
</span>
<span class="c1"># Create labels
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Run the test
</span><span class="n">result</span> <span class="o">=</span> <span class="n">mmd2test</span><span class="p">(</span><span class="n">kernel_matrix</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Test Results:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"MMD statistic: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s">'statistic'</span><span class="p">][</span><span class="s">'MMD'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"p-value: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s">'p_value'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Which gives</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Test</span> <span class="n">Results</span><span class="p">:</span>
<span class="n">MMD</span> <span class="n">statistic</span><span class="p">:</span> <span class="mf">0.416771</span>
<span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="p">:</span> <span class="mf">0.003000</span>
</code></pre></div></div>

        </div>
        
    </div>
</body>
</html>