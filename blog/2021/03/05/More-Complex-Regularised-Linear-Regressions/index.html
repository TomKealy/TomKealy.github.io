<!DOCTYPE html>
<html>
<head>
    <title>More Complex (Linear) Regressions</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>
    <meta name='description' content='Gregory Gundersen is a PhD candidate at Princeton.'>
    <meta name='keywords' content=''>
    <meta name='author' content='Gregory Gundersen'>
    
    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>
    
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>
    
    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
</head>
<body>
    <div class='content'>
        <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
        <div class='front-matter'>
            <div class='wrap'>
                <h1>More Complex (Linear) Regressions</h1>
                <h4>We extend our ideas of how to do regression to a more complex class of functions.</h4>
                <div class='bylines'>
                    <div class='byline'>
                        <h3>Published</h3>
                        <p>05 March 2021</p>
                    </div>
                </div>
                <div class='clear'></div>
            </div>
        </div>
        <div class='wrap article'>
            <h2 id="introduction">Introduction</h2>

<p>We have previously considered models of the form:</p>

\[\hat{y} = \beta X + w\]

<p>where we have measured how well the model is doing by minimising the function:</p>

\[J\left( \beta \right) = \frac{1}{n} \lVert y - \hat{y} \rVert\]

<p>This is <strong>linear regression</strong>—the fundamental tool in statistical modeling. It’s the swiss army knife of statistical modeling, and using it can get you really far. Real-world applications often require more sophisticated approaches. In this post, we’ll explore how to extend basic linear regression using regularization techniques, and we’ll see how these methods help us handle ill-posed problems and noisy data.</p>

<h2 id="ridge-regression">Ridge Regression</h2>

<p>The major downside of linear regression is that it doesn’t allow us to encode some more sophisticated ideas we may have about \(\beta\).</p>

<p>In least squares regression we are (essentially) solving a series of equations:</p>

\[y = X \beta\]

<p>but the problem may be ill posed: there may be no \(\beta\), or many, which satisfy the above equation. Also, many systems we are interested in moddeling act like low-pass filters going in the direction \(X \beta\), so inverting the system naively will act like a high-pass filter and will amplify noise.</p>

<p>To address these challenges, we can modify our objective function by adding a regularization term:</p>

\[J\left( \theta \right) = \frac{1}{n} \lVert y - \hat{y} \rVert_2^2 + \lVert \Gamma \beta \rVert_2^2\]

<p>Luckily, this equation has a closed form solution:</p>

\[\hat{\beta} = \left(X^T X + \Gamma^T \Gamma \right)^{-1} X^T y\]

<p>which can be found the same way as the closed form solution for linear regression (by differentiating the objective). A particularly important case is \(\Gamma = \lambda 1\) (a constant times the identity matrix), which is known by the name of Ridge Regression.</p>

<h2 id="alternative-regularization-approaches">Alternative Regularization Approaches</h2>

<p>Sometimes we have more complex priors about which solutions we require from any particular optimisation problem, and many cannot be solved by simply taking the gradient. For example</p>

<p>1.Lasso Regression (L1 regularization):
\(J(\theta) = \frac{1}{n} \lVert y - \hat{y} \rVert_2^2 + \lVert \beta \rVert_1\)</p>
<ol>
  <li>Total Variation (gradient-based regularization):
\(J(\theta) = \frac{1}{n} \lVert y - \hat{y} \rVert_2^2 + \lVert \nabla \beta \rVert_1\)</li>
  <li>$L_0$ Regularization (sparsity-inducing):
\(J(\theta) = \frac{1}{n} \lVert y - \hat{y} \rVert_2^2 + \lVert \beta \rVert_0\), where where $\lVert \beta \rVert_0$ counts the non-zero elements in $\beta$.</li>
</ol>

<p>None of these optimisation problems can be solved in the straightforward way that we solved Ridge regression.</p>

<p>These optimization problems can’t be solved using simple gradient descent because they’re either non-differentiable (L1 norm) or discrete (L0 norm).</p>

<h3 id="the-solution">The Solution</h3>

<p>These optimisation problem can be solved by using the following trick, set:</p>

\[z = \beta\]

<p>in the second term, and then optimise the following function (the last term is to enforce the constraint we introduced):</p>

\[J\left( \beta \right) = \frac{1}{n} \lVert y - \beta^T X\rVert_2^2 + \lambda \lVert z \rVert_2^2 + \nu^T \left(\beta - z\right) + \frac{\rho}{2} \lVert\beta -z\rVert_2^2\]

<p>This is cleverer than it looks, because</p>

\[\frac{\partial J}{\partial \beta} = -X^T \left(y - X\beta\right) + \rho\left(\beta - z\right) + \nu^T\]

<p>and</p>

\[\frac{\partial J}{\partial z} = \lambda - \nu^T - \rho\left( \beta - z\right)\]

<p>for \( z &gt; 0 \), and</p>

\[\frac{\partial J}{\partial z} = - \lambda - \nu^T + \rho\left( \beta - z\right)\]

<p>for \( z &lt; 0 \), and</p>

\[-\frac{\lambda}{\rho} \leq x + \frac{\nu}{\rho} \leq \frac{\lambda}{\rho}\]

<p>combining these we find:</p>

\[z = \mathrm{sign}\left(X + \frac{\nu}{\rho}\right) \mathrm{max} \left(\mid X + \frac{\nu}{\rho} \mid - \frac{\lambda}{\rho}, 0 \right)\]

<p>we can then update our weights by the following set of iterates:</p>

\[X^{k+1} = \left(X^T X + \rho I\right)^{-1} \left(X^t y + \rho \left(z^{k} - \nu^{k}\right)\right)\]

\[z^{k+1} = S_{\frac{\lambda}{\rho}}\left(X^{k+1} + \nu^{k}/\rho\right)\]

\[\nu^{k+1} = n^{k} + \rho \left(x^{k+1} - z^{k+1} \right)\]

<p>This is implemented in the code below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="k">def</span> <span class="nf">l2prox</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">mu</span><span class="p">))</span> <span class="o">*</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">l1prox</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">-</span><span class="n">mu</span><span class="o">/</span><span class="mf">2.0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ADMM</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">prox</span><span class="p">):</span>
    <span class="s">"""Alternating Direction Method of Multipliers

    This is a python implementation of the Alternating Direction
    Method of Multipliers - a method of constrained optimisation
    that is used widely in statistics (http://stanford.edu/~boyd/admm.html).
    """</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">A_t_A</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A_t_A</span><span class="p">)</span>
    <span class="n">MAX_ITER</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="c1">#Function to caluculate min 1/2(y - Ax) + l||x||
</span>    <span class="c1">#via alternating direction methods
</span>    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">z_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1">#Calculate regression co-efficient and stepsize
</span>    <span class="c1"># r = np.amax(np.absolute(w))
</span>    <span class="c1"># l_over_rho = np.sqrt(2*np.log(n)) * r / 2.0 # I might be wrong here
</span>    <span class="c1"># rho = mu/r
</span>
    <span class="c1">#Pre-compute to save some multiplications
</span>    <span class="n">A_t_y</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">A_t_A</span> <span class="o">+</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
    <span class="n">Q_dot</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">dot</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_ITER</span><span class="p">):</span>
        <span class="c1">#x minimisation step via posterier OLS
</span>        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">Q_dot</span><span class="p">(</span><span class="n">A_t_y</span> <span class="o">+</span> <span class="n">rho</span><span class="o">*</span><span class="p">(</span><span class="n">z_hat</span> <span class="o">-</span> <span class="n">u</span><span class="p">))</span>
        <span class="n">z_hat</span> <span class="o">=</span> <span class="n">prox</span><span class="p">(</span><span class="n">x_hat</span> <span class="o">+</span> <span class="n">u</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
        <span class="c1">#mulitplier update
</span>        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span>  <span class="o">+</span> <span class="n">rho</span><span class="o">*</span><span class="p">(</span><span class="n">x_hat</span> <span class="o">-</span> <span class="n">z_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z_hat</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">computed</span><span class="p">):</span>
    <span class="s">"""Plot two vectors to compare their values"""</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Original'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">computed</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Estimate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">original</span> <span class="o">-</span> <span class="n">computed</span><span class="p">)</span>
    

    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="s">"""Test the ADMM method with randomly generated matrices and vectors"""</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="n">num_non_zeros</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">num_non_zeros</span><span class="p">)</span>
    <span class="n">amplitudes</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_non_zeros</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="n">positions</span><span class="p">]</span> <span class="o">=</span> <span class="n">amplitudes</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#+ np.random.randn(m, 1)
</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ADMM</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">l1prox</span><span class="p">))</span>

<span class="n">test</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>
<p>Regularization can help us solve ill-posed regression problems and handle noisy data. The ADMM algorithm provides a powerful framework for implementing various regularization schemes, and it can be implemented relatively quickly.</p>

<p>For practical applications, consider:</p>

<ul>
  <li>Ridge Regression when you want to prevent overfitting but don’t need sparse solutions.</li>
  <li>Using Lasso when you believe many features should have zero coefficients.</li>
  <li>Total Variation when you expect the coefficients to vary smoothly.</li>
</ul>

        </div>
        
    </div>
</body>
</html>