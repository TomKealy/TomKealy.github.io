<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://gregorygundersen.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://gregorygundersen.com/" rel="alternate" type="text/html" /><updated>2025-01-14T12:50:42+00:00</updated><id>http://gregorygundersen.com/feed.xml</id><entry><title type="html">Hypothesis Testing by Betting</title><link href="http://gregorygundersen.com/blog/2025/01/13/testing-by-betting/" rel="alternate" type="text/html" title="Hypothesis Testing by Betting" /><published>2025-01-13T00:00:00+00:00</published><updated>2025-01-13T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2025/01/13/testing-by-betting</id><content type="html" xml:base="http://gregorygundersen.com/blog/2025/01/13/testing-by-betting/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>The $p$-value is the most misunderstood, misused, and maligned scientific quantity. The $p$-value is merely a summary of your data; it tells you how likely you are to see data at least as extreme as the data you have collected when your null hypothesis is correct. However it often does double duty in statistical analyses: it can tell us whether or not to reject the null hypothesis (e.g. if $ p \leq \alpha$) as well as how confident we should be in the alternative hypothesis. Altogether, this makes the $p$-value a poor tool for scientific communication.</p>

<p>A simpler alternative to $p$-values is to report the result of a bet against the null hypothesis. This is <strong>testing by betting.</strong> This method is closely related to the likelihood method, and leads to alternatives for statistical confidence and power.</p>

<p>This post is a longer exposition of <em>Testing by Betting</em>, Gleen Schafer (2021) <a class="citation" href="#shafer2021testing">(Shafer, 2021)</a>. The point was to include more exposition and some code. Any resemblence to the original is therefore natural (and in some cases I’ve jsut changed the language);</p>

<p>Testing by betting is straightforward. First we select a payoff (which can be either be zero or a positive number). Then we buy this payoff for its (hypothesized) expected value. If this bet multiplies the money it risks, we have evidence against the hypothesis. The factor–called the <strong>betting score</strong>–measures the strength of this evidence. Multiplying our money by 5 merits attention; multiplying it by 100 or by 1000 might be considered conclusive.</p>

<p>Testing by betting is simpler than reporting a $p$-value, because a $p$-value represents the probability of obtaining test results at least as extreme as those observed, assuming the null hypothesis is true. The key phrase here is “at least as extreme”–you’re not just looking at the probability of getting exactly your observed result, but rather the probability of getting your result or any more extreme result. A $p$-value is thus the result of a <em>family</em> of tests:</p>

<ul>
  <li>For any observed test statistic, you need to consider all the possible outcomes that would be “more extreme”</li>
  <li>You have to decide what counts as “more extreme” (one-tailed vs two-tailed tests)</li>
  <li>You’re essentially conducting multiple implicit hypothesis tests - one for your actual result and one for each more extreme possible outcome.</li>
</ul>

<p>The betting score  is simpler because it just looks at one specific bet and its outcome–did you win or lose, and by how much? There’s no need to consider a family of hypothetical outcomes or define what counts as “more extreme.” To take an example:</p>

<ol>
  <li>With a $p$-value: If you observe a z-score of 2.5, you need to calculate the probability of observing a z-score ≥ 2.5 (or ≤ -2.5 for a two-tailed test)</li>
  <li>With a betting score: You just calculate how much money your specific bet made or lost (e.g. 10x the initial bet).</li>
</ol>

<p>Betting scores also have a number of other advantages:</p>

<ol>
  <li>There is less uncertainty when you get a large betting score, compared with a small $p$-value. You will not forget that a long shot can succeed by sheer luck.</li>
  <li>When we make a bet, we create an implied alternative hypothesis. The betting score is the likelihood ratio with respect to this alternative. What the betting score means is aligned with what a likelihood ratio means.</li>
  <li>As well as implying an alternative hypothsis, the bet also implies a <strong>target.</strong> This is the value we desire to make the bet “worthwhile.” Implied targets are useful than power calculations, because an implied target along with an actual betting score tells a coherent story. To interpret a statistical test, you need to know the test’s actual statistical power. This, in turn, requires a fixed significance level. In traditional statistical inference, there’s a  awkward relationship between power calculations and $p$-values: Power calculations are done before the study; but after running the study, you report a $p$-value, which might be any value between 0 and 1. Betting scores are superior because they have a more coherent narrative -both your planning and your results are in the same units (betting scores for your money).</li>
  <li>Testing by betting is a more agile approach to science, because you don’t have to plan your entire analysis in advance.</li>
</ol>

<h1 id="testing-by-betting">Testing by betting</h1>

<p>We are interested in phenomenon which we model with a probability distribution, $P$, with an associated set of random variables. $X$. For example:</p>

<ol>
  <li>The ammount of traffic per hour at an intersection. We could model this as a <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a> distribution, and we want to know the mean parameter.</li>
  <li>The height of pupils in a school. We could model this as a normal distribution, and we would like know the mean and standard deviation.</li>
</ol>

<p>We will later see the actual value of the parameter $X$, which we denote $x$. How can we give more nuanced content to our claims (e.g. that traffic intensity is best modelled by a Poisson distribution), and how could we challenge them?</p>

<p>The way we choose to proceed is to interpret our claim as a collection of betting offers with Reality<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. Reality offers to sell me any payoff $S\left(x\right)$ for its expected value $\mathcal{E}_P\left(x\right)$. I can choose a (nonnegative) payoff $S$, so that $\mathcal{E}_P\left(x\right)$ is all that I risk.</p>

<p>My betting score is:</p>

\[\frac{S\left(x\right)}{\mathcal{E}_P\left(x\right)}\]

<p>The score doesn’t change when I scale my bet, so I can take $\mathcal{E}_P\left(x\right) = 1$, and so the betting score is simply my stake.</p>

<p>The standard way of testing a probability distribution $P$ is to select a significance level $α \in \left(0,1\right)$, usually small, and a set $E$ of possible values of $X$ such that $\mathcal{P}\left(X \in E\right) = \alpha$. The event $E$ is called the rejection region. The probability distribution $P$ is discredited (or rejected) if the actual value $x$ is in $E$.</p>

<p>For example, we could test the mean $X$ of a series of measurements $M_1, M_2, \ldots, M_k$ to see if the distribution $P$ has mean zero or not. We believe that $X$ comes from a standard normal distribution, and so we test (with a $z$-test for instance) that $X$ is in the region $X \in \left(-1.96, 1.96\right)$ (i.e. $X$ is within 2 standard deviations of the standard normal distribution). If we see a $x$ (the actual mean) that is bigger than 1.96 (i.e. $\mid x\mid &gt; 1.96$) then we conclude that the $X$ does not have mean 0.</p>

<p>Textbooks seldom make the idea explicit, a standard test is often thought of as a bet: I pay $1 for the payoff $S_E$ defined by</p>

\[\begin{eqnarray} 
S_E     \nonumber \\
&amp;=&amp; \frac{1}{\alpha} \text{ if} x \in E \nonumber \\
&amp; 0 \text{ otherwise}   \nonumber
\end{eqnarray}\]

<p>If $E$ happens, I have multiplied the $1 I risked by $\frac{1}/\alpha}$. This makes standard testing a special case of testing by betting, the special case where the bet is all-or-nothing. In return for $1, I get either $$\frac{1}/\alpha}$ or $0.</p>

<p>So a betting score $S\left(x\right)$ appraises the evidence against $P$. The larger $S\left(x\right)$, the stronger the evidence.</p>

<h2 id="betting-scores-are-likelihood-ratios">Betting scores are likelihood ratios</h2>

<p>Betting against a hypothesis is equivalent to proposing an alternative hypothesis $Q$ and comparing likelihoods. We show this this by first showing a betting score is a likelihood ratio, and then by showing that likelihood ratios are betting scores.</p>

<p>Earlier we defined our betting score as:</p>

\[\frac{S\left(x\right)}{\mathcal{E}_P\left(x\right)}\]

<p>Where our payoff in the bet is denoted by $S\left(x\right)$ and my stake is simplly the expected value $\mathcal{E}_P\left(x\right)$. The betting score is what you win divided by what you paid. I can choose a (nonnegative) payoff $S$, so that $\mathcal{E}_P\left(x\right)$ is all that I risk (i.e. I pay the expected value of this payoff $S$ under probability distribution $P$).</p>

<p>We also noted that the score doesn’t change when I scale my bet (doubling both your payoff and stake gives the same score), so I can take $\mathcal{E}_P\left(y\right) = 1$. The assumption $\mathbb{E}_P(S) = 1$. Another way of saying this is that $\sum_x S(x)P(x) = 1$. Because $S(x)$ and $P(x)$ are:</p>

<ol>
  <li>nonnegative for all $x$ (and we are assuming that $P &gt; 0$)</li>
  <li>$\sum_x S(x)P(x) = 1$</li>
</ol>

<p>$SP$ is a probability distribution. We <em>define</em></p>

\[Q := SP\]

<p>and call $Q$ the alternative <em>implied</em> by the bet $S$. If $P$ is wrong and $Q$ is actually the true distribution, then your bet would be expected to make money. So $Q$ represents what you “think might actually be true” when you make your bet against $P$.</p>

<p>Because we assumed $P(x) &gt; 0$ for all $x$, we can rearrange, and express our payoff as the ratio of these two probability distributions:</p>

\[S(x) = \frac{Q(x)}{P(x)}\]

<p>i.e. the betting score is a simply a likelihood ratio.</p>

<p>A likelihood ratio is a betting score.</p>

<p>If you have two probability distributions $Q$ and $P$ then the likelihood ration $S = Q/P$ satisfies the requirements for a bet:</p>

<ol>
  <li>$Q/P$ is nonnegative since probabilities are nonnegative.</li>
  <li>$\sum_x \frac{Q(x)}{P(x)}P(x) = \sum_y Q(x) = 1$. I.e. The expected value of the bet under $P$ is 1.</li>
</ol>

<p>expected value of this bet under $Q$ is nonnegative: $\mathbb{E}_Q(S) \geq 1$ ($\mathbb{E}_Q(S)$ is how much you expect to win if $Q$ is actually true).</p>

<p>In fact $\mathbb{E}_Q(S) = \mathbb{E}_P(S^2)$. Since $S = Q/P$, we can write $\mathbb{E}_Q(S) = \sum_x S(x)Q(x)$. We substitute $S = Q/P$:</p>

\[\mathbb{E}_Q(S) &amp;=&amp; \sum_x \frac{Q(x)}{P(x)}Q(x) \\
&amp;= \sum_y \frac{Q(x)^2}{P(x)}\]

<p>Let’s consider $\mathbb{E}_P(S^2)$:</p>

\[\mathbb{E}_P(S^2) = \sum_y S(x)^2P(x)\]

<p>Again substitute $S = Q/P$:</p>

\[\mathbb{E}_P(S^2) = \sum_x (\frac{Q(x)}{P(x)})^2P(x) \\
&amp;= \sum_y \frac{Q(y)^2}{P(y)^2}P(y) \\
&amp;= \sum_y \frac{Q(y)^2}{P(y)}\\\]

<p>We can see that this is the same as what we got for $\mathbb{E}_Q(S)$, thus $\mathbb{E}_Q(S) = \mathbb{E}_P(S^2)$. When we square $S = Q/P$ and multiply by $P$ in $\mathbb{E}_P(S^2)$, we end up with the same expression as when we multiply $S = Q/P$ by $Q$ in $\mathbb{E}_Q(S)$.</p>

<p>A consequence of this insight is that because we know that $\mathbb{E}_P(S) = 1$ (this was one of our initial assumptions about the betting score), $\mathbb{E}_Q(S) = \mathbb{E}_P(S^2)$ implies:</p>

\[\mathbb{E}_Q(S) - 1
&amp;=&amp; \mathbb{E}_P(S^2) - 1$ (substituting \mathbb{E}_Q(S) = \mathbb{E}_P(S^2)) \\
&amp;= \mathbb{E}_P(S^2) - (\mathbb{E}_P(S))^2$ (substituting 1 = \mathbb{E}_P(S) (just square both sides)) \\
&amp;= \text{Var}_P(S)\]

<p>Here $\mathbb{E}_Q(S) - 1$ represents what I win if I am right, and not “Reality” ($P$). i.e the more variable your betting score is under the null hypothesis $P$, the more you stand to gain if your alternative $Q$ is actually true. If you make a more extreme bet (higher variance), you have more to gain if you’re right.</p>

<h2 id="how-much-should-i-bet">How much should I bet?</h2>

<p>We began with your claiming that $P$ describes the phenomenon $X$ and my making a bet $S$ satisfying $S \geq 0$ and, for simplicity, $\mathbb{E}_P(S) = 1$. Suppose, however, that I do have an alternative $Q$ in mind. I have a hunch that $Q$ is a valid description of $X$. In this case, should I use $\fra{Q}/{P}$ as my bet?</p>

<p>There are two schools of thought: opportunistic and repeated testing, and testing in a single ‘go.’ The first justification is related to <a href="https://en.wikipedia.org/wiki/Kelly_criterion">Kelly Betting</a>, and the second is related to the <a href="https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma">Neyman-Pearson Lemma</a>.</p>

<h3 id="kelly-betting">Kelly Betting</h3>
<p>The thought that I should is supported by Gibbs’s inequality which says that<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>:</p>

\[\mathbb{E}_Q \ln\frac{Q}{P} \geq \mathbb{E}_Q \ln\frac{R}{P} \\]

<p>for <strong>any</strong> probability distribution $R$ for $Y$. Because any bet $S$ is of the form $R/P$ for some such $R$, Gibb’s inequality tells us that $\mathbb{E}_Q(\ln S)$ is maximized over $S$ by setting $S := Q/P$. This tells that if we have a hunch that $Q$ is more correct than $P$ (or any other probability distribution $R$) then we should be $\frac{Q}{P}.</p>

<p>The quanitiy $\mathbb{E}_Q(\ln(Q/P))$ is known as the Kullback-Leibler divergence between $Q$ and $P$. It is the mean number of bits of information you gain when $Q$ is the true encoding distribution and not $P.</p>

<p>Why should I choose $S$ to maximize $\mathbb{E}_Q(\ln S)$? Why not maximize $\mathbb{E}_Q(S)$? Or perhaps $Q(S \geq 20)$ or $Q(S \geq 1/\alpha)$ for some other significance level $\alpha$?</p>

<p>Maximizing $\mathbb{E}(\ln S)$ makes sense in a scientific context where we combine successive betting scores by multiplication. i.e. when we are testing opportunistically. When $S$ is the product of many successive factors, maximizing $\mathbb{E}(\ln S)$ maximizes $S$’s rate of growth. Logarithms are <em>additive</em> in repeated bets.</p>

<h3 id="neyman-pearson">Neyman-Pearson</h3>
<p>Conversely choosing $S$ to maximize $Q(S \geq 1/\alpha)$ is appropriate when the hypothesis being tested will not be tested again.</p>

<p>For a given significance level $\alpha$, we choose a rejection region $E$ such that $Q(x)/P(x)$ is at least as large for all $x \in E$ as for any $x \notin E$, where $Q$ is an alternative hypothesis.</p>

<p>Let us call the bet $S_E$ with this choice of $E$ the <em>level-$\alpha$ bet</em> against $P$ with respect to $Q$. The <em>Neyman-Pearson lemma</em> says that this choice of $E$ maximizes</p>

\[Q(\text{test rejects }P) = Q(X \in E) = Q(S_E(X) \geq 1/\alpha)\]

<p>which we call the <em>power</em> of the test with respect to $Q$. In fact, $S_E$ with this choice of $E$ maximizes</p>

\[Q(S(Y) \geq 1/\alpha)\]

<p>over all bets $S$, not merely over all-or-nothing bets. <a class="citation" href="#shafer2021testing">(Shafer, 2021)</a> offers a proof of this statement.</p>

<h2 id="implied-targets">Implied targets</h2>

<p>When I get my betting score for a particular bet $S$ against $P$, how do I know if the score is good or not?</p>

<p>Choosing a payoff $S$ defines an alternative probability distribution, $Q : = SP$, and with $S$ being the bet against $P$ that maximizes $\mathbb{E}<em>Q(\ln S)$. We might hope for a betting score whose _logarithm</em> is in the ballpark of $\mathbb{E}_Q(\ln S)$. I.e a betting score like:</p>

\[S_{*} : = \exp{\mathbb{E}_Q(\ln S)}$\]

<p>We call $S_{∗}$ the <strong>implied target</strong> of the bet $S$. The implied target of the all-or-nothing bet is always $\frac{1}{\alpha}$.</p>

<p>The notion of an implied target is analogous to the notion of statistical power with respect to a particular alternative. But it has the advantage that we cannot avoid discussing it by refusing to specify a particular alternative. The implied alternative $Q$ and the implied target $S_{∗}$ are determined as soon as the distribution $P$ and the bet $S$ are specified. The implied target can be computed without even mentioning $Q$, because:</p>

\[\mathbb{E}_Q(\ln S) = \sum_x Q(x)\ln S(x) = \sum_y P(x)S(x)\ln S(x) = \mathbb{E}_P(S\ln S)\]

<p>(def expectation, then def of $Q$, collect like terms). i.e. this is expected log betting score under _either $Q$ (as $\mathbb{E}_Q(\ln S)$) or $P$ (as $\mathbb{E}_P(S\ln S)$).</p>

<h1 id="examples">Examples</h1>

<p>Here’s the example in markdown:</p>

<p><strong>Example 1.</strong></p>

<p>Suppose $P$ says that $X$ is normal with mean 0 and standard deviation 10, $Q$ says that $X$ is normal with mean 1 and standard deviation 10, and we observe $x = 30$.</p>

<ol>
  <li>
    <p>Statistician A simply calculates a p-value: $P(X \geq 30) \approx 0.00135$. She concludes that $P$ is strongly discredited.</p>
  </li>
  <li>
    <p>Statistician B uses the Neyman-Pearson test with significance level $\alpha = 0.05$, which rejects $P$ when $x &gt; 16.5$. Its power is only about 6%[^3].</p>
  </li>
</ol>

<p>Seeing $x = 30$, it does reject $P$. Had she formulated her test as a bet, she would have multiplied the money she risked by 20.</p>

<ol>
  <li>Statistician C uses the bet $S$ given by:</li>
</ol>

\[S(y) := \frac{q(x)}{p(x)} = \frac{\sqrt{10^2\pi}\exp(-(x-1)^2/200)}{\sqrt{10^2\pi}\exp(-x^2/200)} = \exp(\frac{2x-1}{200})\]

<p>So</p>

\[\mathbb{E}_Q(\ln(S)) = \frac{1}{200} = \frac{1}{200}\]

<p>I.e. the implied target is $\exp(1/200) \approx 1.005$. She does a little better than this very low target; she multiplies the money she risked by $\exp(59/200) \approx 1.34$.</p>

<p>The power and the implied target both told us in advance that the study was a waste of time. The betting score of 1.34 confirms that little was accomplished, while the low $p$-value and the Neyman-Pearson rejection of $P$ give a misleading verdict in favour of $Q$.</p>

<h1 id="hypotheis-testing-and-code">Hypotheis Testing and Code</h1>

<p>$-\sum_{i=1}^n p_i \log p_i \leq -\sum_{i=1}^n p_i \log q_i$ with equality if and only if $p_i = q_i$ for all $i$. The version here ($\mathbb{E}_Q \ln\frac{Q}{P} \geq \mathbb{E}_Q \ln\frac{R}{P}$) is equivalent
[^3]: To find the power, we need to calculate: 
$Q(X &gt; 16.5)$ where under $Q$, $X \sim N(1, 10^2)$
We can standardize this:
Under $Q$: $X \sim N(1, 100)$
$$
P(X &gt; 16.5) &amp;=&amp; P(\frac{X-1}{10} &gt; \frac{16.5-1}{10})
&amp;= P(Z &gt; 1.55)$ where $Z$ is standard normal
&amp;= 1 - \Phi(1.55)
&amp;\approx 0.06 or 6\%]</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>You will have to be generous with notions here. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>The standard form of Gibbs’s inequality (also known as the information inequality) states that: <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="hypothesis-testing" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">A short introduction to Martingales</title><link href="http://gregorygundersen.com/blog/2025/01/10/intro-to-martingales/" rel="alternate" type="text/html" title="A short introduction to Martingales" /><published>2025-01-10T00:00:00+00:00</published><updated>2025-01-10T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2025/01/10/intro-to-martingales</id><content type="html" xml:base="http://gregorygundersen.com/blog/2025/01/10/intro-to-martingales/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>Martingales are elegant and powerful tools to study sequences of dependent random variables. It is originated from gambling, where a gambler can adjust the bet according to the previous results.</p>

<p>Martingales are one of the more abstract parts of probability. Which is a pity, since they are also one of the most useful bits of probability. Martingales are one of the simplest models of dependent random variables, and that is why they come up in examples over and over again. If you can identify when a stocastic process as a martingale, then you know some rather general things about that process that may lead you to being able to get specific answers to your questions.</p>

<p>If you have a stocastic process whos expected value is time-dependent, then you can potentially convert your process into a martingale. All you need to do is to add the proper conditional expected value. Then you can apply all sorts of martingal theorems (convergence, optional stopping etc) which can lead to explict numerical answers. However approaching martingales with the attitude that you <strong>will</strong> calculate things will leave you dissapointed. There are very few explicit examples.</p>

<p>You should approach your problem from the point of view that “process \(X_t\) is a martingale, so any stopping strategy will not affect its expected value”. This realization is useful for an applied as it stops you ways wasting time optimising the expected value.</p>

<h2 id="the-maths-unfortunately">The maths, unfortunately</h2>

<p>We denote our <a href="https://en.wikipedia.org/wiki/Probability_space">sample space</a> as $S$ (for example $S = {+1, -1}$). Let $F_n$ be a filtration of the event space.</p>

<p>Let $X_0, X_1, X_2, \dots$ be a sequence of random variables. We will imagine that we are acquiring information about $S$ in stages. The random variable $X_n$ is what we know at stage $n$. If $Z$ is any random variable, let</p>

\[E\left[Z \mid F_nn\right]\]

<p>denote the conditional expectation of $Z$ given all the information that is available to us on the $n$th stage. If you saw all the information you could obtain by stage $n$, and you made a Bayesian update to your probability distribution on $S$ in light of this information, then $E\left[Z\mid F_n\right]$ would represent the expected value of $Z$ with respect to this revised probability.</p>

<p>If we don’t specify otherwise, we assume that the information available at stage n consists precisely of the values $X_0, X_1, X_2, \dots$ so that</p>

\[E\left[Z \mid F_nn\right] = E\left[Z \mid X_0, X_1, X_2, \dots, X_n].\]

<p>However in some applications, one could imagine there are other things known as well at stage $n$. For example, maybe $X_n$ represents the price of an asset on the $n$th day and $Y_n$ represents the price of asset $Y$ on the $n$th day. If you have access to the sequence $X_0, X_1, X_2, \dots, X_n]$ and the sequence $Y_0, Y_1, Y_2, \dots, Y_n]$. Then $E\left[Z \mid F_nn\right]$ would be our revised expectation of $Z$ after we have incorporated what we know about both sequences.</p>

<p>We say that sequence $X_n$ is a martingale if:</p>

<ol>
  <li>$E\left[\mid X_n \mid\right] &lt; \infty$ for all $n$.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$E\left[X_{n+1}</td>
          <td>F_n\right] = X_n$  for all $n$.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p>Informally $X_0, X_1, \ldots$ is a martingale if the following is true: taking into account all the information I have at stage $n$, the conditional expected value of $X_{n+1}$ is just $X_n$. Basically, if your process is a martingale, you can’t know anything about the future other that everything you know in the present moment (and the entire history of the process).</p>

<p>To motivate this definition, imagine that $X_n$ represents the price of a stock on day $n$. In this context, the martingale condition states informally that “The expected value of the stock tomorrow, given all I know today, is the value of the stock today.” After all, if the stock price today were 50 and I expected it to be 60 tomorrow, then I would have an easy way to make money in expectation (buy today, sell tomorrow). But if the public had the same information I had, then other investors would also try to cash in on this by buying the stock today at 50, and people holding the stock would be reluctant to sell for 50. Indeed, we’d expect the price to be quickly bid up to about 60 today.</p>

<h3 id="example">Example</h3>

<p>Let $S = {+1, -1}$ and let $X_0, X_1, X_2, \dots$ be a sequence of random variables taking values in $S$. $X_n$ is +1 with probability 0.5 and -1 with probability 0.5 We define:</p>

\[M_n = \sum_{i=0}^n X_n\]

<p>$M_n$ is a martingale since:</p>

\[E\left[M_{n+1} \mid F_n\right] = E\left[M_{n} + X_{n+1} \mid F_n\right]  = E\left[M_{n}\mid F_n\right] + E\left[X_{n+1} \mid F_n\right]\]

<p>Since $M_n$ is known at stage n, we have $ E\left[M_{n}\mid F_n\right] = M_n$. Since we know nothing more about $X_{n+1}$ at stage $n$ than we originally knew, we have $E\left[X_{n+1} \mid F_n\right] =0$. So</p>

\[E\left[M_{n+1} \mid F_n\right] = M_n\]

<p>So the sequence $M_n$ is a martingale.</p>

<h2 id="stopping-times-and-the-optional-stopping-theorem">Stopping times, and the Optional Stopping theorem</h2>

<p>A <strong>stopping time</strong> is a (non-negative integer-valued) random variable, $T$, such that for all $n$ the event that $T = n$ depends only on the information available to us at time $n$. Stopping times can be interprete as the time that you sell an asset, given that the sequence of prices is $X_0, X_1, X_2, \dots$</p>

<p>Saying that $T$ is a stopping time means that the decision to sell at time $n$ depends only the information we have up to time $n$, and not on future prices. Specifying a stopping time can be interpreted as specifying a strategy for deciding when to sell the asset.</p>

<p>For example, let $X_0, X_1, X_2, \dots$ be i.i.d. random variables equal to −1 with probability .5 and 1 with probability .5 and let $X_0 = 0$ and $M_n = \sum_{i=0}^n X_n $ for $n ≥ 0$. These four statements:</p>

<ol>
  <li>The smallest $T$ for which $\mid X_T \mid = 50$</li>
  <li>The smallest $T$ for which $X_tT \in {−30, 100}$</li>
  <li>The smallest $T$ for which $X_T = 17$.</li>
  <li>The $T$ at which the $X_n$ sequence achieves the value 17 for the 9th time.</li>
</ol>

<p>all define stopping times.</p>

<h3 id="optional-stopping-theorem">Optional Stopping Theorem</h3>

<p>The optional stopping theorem says that the expected value of a martingale at a stopping time is equal to its initial expected value. It tells us that you can’t make money (in expectation) by buying and selling an asset whose price is a martingale. Precisely, if you buy the asset at some time and adopt any strategy at all for deciding when to sell it, then the expected price at te time you sell is the price you originally paid. In other words, if the market price is a martingale, you cannot make money in expectation by “timing the market.”</p>

<p><strong>Definition</strong> Boundedness. We say a random sequence $X_0, X_1, X_2, \dots$ is <strong>bounded</strong> when there exists some $C &gt; 0$, we have that with probability one $\mid X_n \mid ≤ C$ for all $n \geq 0$. A stopping time $T$ is bounded if there exists some $C &gt; 0$ such that $T \geq C$ with probability one.</p>

<p><strong>Optional Stopping Theorem (first version)</strong>: Suppose that $X_$ is a known constant, that $X_0, X_1, X_2, \dots$ is a bounded martingale, and that $T$ is a stopping time. Then $E\left[X_T\right] = X_0$.</p>

<p><strong>Optional Stopping Theorem (second version):</strong> Suppose that $X_$ is a known constant, that $X_0, X_1, X_2, \dots$ is a martingale, and that $T$ is a bounded stopping time. Then $E\left[X_T\right] = X_0$.</p>

<p>These boundedness assumptions are actually very important. Without them the theorem would not be true.</p>

<p>For a counterexample, recall that if $X_0 = 0$ and $X_n$ goes up or down by 1 at each time step (each with probability .5) then $X_0, X_1, X_2, \dots$ is a martingale. If we let $T$ be the first $n$ for which $X_n = 100$, then $T$ is a finite number with probability one. (That is, with probability one $X_n$ reaches T eventually.) But then $X_T$ is always 100, which means that $E\left[X_T\right]  \neq X_0$.</p>]]></content><author><name></name></author><category term="time-series" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">A Grab Bag of Approaches to Frequentist Multiple Testing.</title><link href="http://gregorygundersen.com/blog/2024/10/09/approaches-to-multiple-testing/" rel="alternate" type="text/html" title="A Grab Bag of Approaches to Frequentist Multiple Testing." /><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/10/09/approaches-to-multiple-testing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/10/09/approaches-to-multiple-testing/"><![CDATA[<p>We show how to handle more than one hypothesis at a time.</p>

<h2 id="introduction">Introduction</h2>

<p>This note is to explain the statistical background behind multiple testing, which experimenters (be they researchers in academia, NGOs, or tech)  need to  consider when implementing their experiments. Understanding multiple testing is crucial because it determines the reliability of your results and influences how much your end users can trust your papers or products.</p>

<p>The presentation will be in terms of p-values as the algorithms for multiple metric adjustments can be done by hand. Their purpose is to develop intuition for the decision theoretic issues underlying the algorithms. Here we are focussing on the why, rather than on any specific how.</p>

<h2 id="the-challenge-of-multiple-testing">The Challenge of Multiple Testing</h2>

<p>When a researcher runs an experiment, they will typically choose a single metric to decide whether the intervention has been successful or not. This is called the primary decision metric or endpoint in the experimentation literature. Some examples include: the ratio of the total number of conversions to unique visitors on a website (the conversion rate); or the test score of pupils in a classroom (as in the STAR study).</p>

<p>If you run A/B tests on your website and regularly check ongoing experiments for significant results, you might be falling prey to what statisticians call repeated significance testing errors. As a result, even though your dashboard says a result is statistically significant, there’s a good chance that it’s actually insignificant.</p>

<p>To determine the outcome of the experiment, the researcher specifies a binary decision rule tied to their primary metric. For example the conversion rate of the test variation increases by at least 5% (a typical hypothesis in tech). Or that pupils in smaller classes have 20% higher scores on a standardised test at the end of the school year (STAR study).</p>

<p>When an A/B testing dashboard says there is a “95% chance of beating original” or “90% probability of statistical significance,” it’s asking the following question: Assuming there is no underlying difference between A and B, how often will we see a difference like we do in the data just by chance? The answer to that question is called the significance level, and “statistically significant results” mean that the significance level is low, e.g. 5% or 1%. Dashboards usually take the complement of this (e.g. 95% or 99%) and report it as a “chance of beating the original” or something like that.</p>

<p>However, the significance calculation makes a critical assumption that you have probably violated without even realizing it: that the sample size was fixed in advance. If instead of deciding ahead of time, “this experiment will collect exactly 1,000 observations,” you say, “we’ll run it until we see a significant difference,” all the reported significance levels become meaningless. This result is completely counterintuitive and all the A/B testing packages out there ignore it, but I’ll try to explain the source of the problem with a simple example.</p>

<p>There is more than one thing that researchers can measure, and alongside the primary metric many other things are measured about the experimentation subjects. This includes financial metrics in our conversion rate example, or the rates at which pupils dropped out of education altogether in the STAR study.</p>

<p>It’s tempting, then, to have multiple decision metrics to decide if an intervention has been successful. For example we might decide that a change in web page design is an improvement if it increases the conversion rate by at least 5%, and also increases the average revenue per user (ARPU) by 3%. This is an absolutely reasonable thing to do. When we add more metrics with statistical information to an experiment we think that we are increasing the number of endpoints by 1; however this is incorrect, we are increasing the number of endpoints for our experiment by an order of magnitude. Consider going from 1 to 2 metrics in an experiment.  Think of it like this: with one metric, under any binary decision rule you have two endpoints for the experiment. Yes, or no. 0 or 1. When you add a second metric, you now have four endpoints which the experiment can take: 00, 01, 10, 11. With three metrics you now have eight endpoints: 000, 001, 010, 100, 011, 101, 110, 111. Since statistical tests are probabilistic, with each independent test you add you increase the probability that one of them will say yes, even if there is no effect present in the data.  This is the challenge of multiple testing: how can you add more metrics whilst controlling the overall experiment-wise false probability rate?</p>

<p>There are a number of statistical approaches to handle this inflation of error rates, so that the conclusions from any statistical analysis are sound.</p>

<h3 id="what-is-a-false-positive-and-why-is-it-important">What is a false positive and why is it important?</h3>

<p>In hypothesis testing, a false positive occurs when a statistical result returns a ‘yes’ decision even when this is not warranted by the data. In other words, we conclude an effect is present in the data when it actually isn’t. It’s important to remember that statistical tests are not infallible, they are instead probabilistic and so, no matter how stringent you make the requirements of the analysis, they can always go awry. 
This type of error is crucial because it can lead to unnecessary actions or interventions based on a perceived effect that isn’t real. There is a feeling that false positives are costless, and that increasing the number of false positives is no big matter. However, consider that most statistical tests are two sided and that they do not distinguish between the upper and lower tails of the distribution. In this situation a false positive could be in the negative direction, and in fact lose the company money.</p>

<h4 id="what-is-the-family-wise-error-rate">What is the Family-wise error rate?</h4>

<p>The false positive rate (FPR) of a single metrics is the proportion of all negatives that still yield positive test outcomes. It’s the probability that a null hypothesis is incorrectly rejected given that it is true. The FPR is commonly set at 5% (0.05), meaning we are willing to accept a 5% chance of falsely claiming a significant effect or association.</p>

<p>When we start adding more metics to our statistical analyses the probability that any single negative yields a positive outcome is called the family-wise error rate (FWER). The FWER is the probability of making at least one Type I error (i.e., falsely rejecting a true null hypothesis) across a family of tests. In other words, it controls the total rate of false positive results to keep it below a predefined threshold. Many primary sources use FWER and the FPR for multiple tests interchangeably. Keeping the FWER at an acceptable level is challenging, as it is a very stringent requirement.</p>

<h3 id="family-wise-error-rate-fwer">Family Wise Error Rate (FWER)</h3>
<p>One of the best-known methods to control the FWER is the Bonferroni correction, which involves dividing the desired significance level by the number of comparisons. For example: if you set an overall alpha of 0.1 and you wanted statistical information on 10 metrics in your experiment, then you would then use an alpha of 0.01 (i.e. divide by 10) for all of the 10 independent tests. This way the overall FWER is constrained to 0.1.  This method is quite stringent (but by far the safest) as it dramatically reduces the window that any single test is a false positive. Though can reduce the risk of any false positives at the cost of potentially missing true positive results (increasing Type II errors). 
For a business AB testing platform, this means that we could reduce the number of rollouts by a factor proportional to the number of metrics we are looking at. In medicine this may be a good trade-off to make.</p>

<h4 id="what-is-the-false-discovery-rate">What is the false discovery rate?</h4>

<p>In maths one useful move that’s always available to you is to define yourself out of a problem. The false discovery rate is an example of one such move. The false discovery rate (FDR) is the expected proportion of false positives among all declared positives. Unlike the FPR, the FDR controls the expected proportion of false discoveries, rather than the chance of any false discoveries.</p>

<p>Colloquially, what this means is that you tolerate a cost of slightly more false positives overall, with the benefit that any adjustment scheme for the FDR is more forgiving.</p>

<h3 id="false-discovery-rate-fdr">False Discovery Rate (FDR)</h3>
<p>The FDR, on the other hand, is the expected proportion of Type I errors among all rejected hypotheses. Instead of trying to avoid any single false positive like FWER, the FDR control procedure tries to limit the proportion of false positives among all discoveries.</p>

<p>The Benjamini-Hochberg procedure is a common method used to control the FDR. It is generally less conservative than methods that control the FWER, allowing for more false positives but increasing the power to detect true positives. It works by calculating a sequence of increasing adjusted p-values for each different metric.</p>

<p>In essence, the key difference lies in what each rate seeks to control: FWER controls the probability of at least one false positive, while FDR controls the expected proportion of false positives. Your choice between the two would depend on the balance you wish to strike between avoiding false positives and not missing true positives.</p>

<h3 id="what-is-a-false-negative-and-why-is-it-important">What is a false negative and why is it important?</h3>

<p>A false negative, on the other hand, occurs when we fail to reject a false null hypothesis. In simpler terms, we conclude that there is no effect or association when there actually is. False negatives are important because they can prevent us from taking necessary actions or recognizing significant associations or effects.
This error is when we leave money on the table. Statistical tests are designed to give us the most statistical power (to minimise the number of false negatives), subject to constraints on the chance of detecting a false positive. Again, remember that no matter the parameters of your system (for example, you might increase the sample size dramatically to reduce the probabilities above) you will always be faced with some inherent uncertainty with a statistical test.</p>

<h3 id="how-do-they-vary-by-the-number-of-comparisons">How do they vary by the number of comparisons?</h3>

<p>As we perform more and more independent statistical comparisons, we increase the likelihood that our results are false positives (and also increase the number of false negatives we leave by the wayside). This is referred to as the “multiple testing problem.” For each individual test, the probability of a false positive or false negative may be small. However, when we perform multiple tests, these probabilities multiply, exponentially  increasing the chance of one or more false results. For example, if we have 10 independent tests, the probability that a single one is positive purely by change is 1 - (1-0.05)^10 ~ 40%.</p>

<p>There are a few ways to avoid this, but first we need to define a few terms:</p>

<h3 id="how-can-we-prevent-this-from-happening">How can we prevent this from happening?</h3>

<p>We can mitigate the issue of multiple testing by applying various correction methods. Some popular ones include the Bonferroni Correction for the FWER and the Benjamini-Hochberg procedure for the FDR. These techniques adjust our threshold for significance (e.g., lowering the p-value) based on the number of comparisons being made to maintain an appropriate error rate.</p>

<h4 id="methods-to-control-the-family-wise-error-rate">Methods to control the Family Wise Error Rate</h4>

<h4 id="methods-to-control-the-false-discovery-rate">Methods to control the False Discovery Rate</h4>

<h3 id="must-we-apply-the-adjustments-equally">Must we apply the adjustments equally.</h3>

<p>No. It’s not required that we apply adjustments equally. In fact the Bejamini-Hochberg algorithm explicitly does not apply equally to all metrics.</p>

<p>A good example is to consider the following situation. You have 11 metrics you would like to compute p-values for in an experiment, with an overall alpha of 0.1. However, for one of them you wish to allocate an alpha of 0.5, and for the rest of them you want to allocate a p-value of 0.005 (0.05/10). This is a perfectly reasonable situation, as it keeps the overall alpha to 0.1 (the sum of all the individual alpha values). You could also extend this: your primary metric gets an alpha of 0.5, some set of secondary metrics gets an alpha of 0.3 (and the p-value thresholds are calculated via the Benjamini-Hochberg procedure) and even more tertiary metrics have an alpha of 0.2 (and the p-value thresholds are calculated via the Benjamini-Hochberg procedure). This tiering strategy is also appropriate. What wouldn’t be appropriate would be leaving the primary metric unadjusted, and still claiming the overall alpha is 0.1. In that case, the overall alpha would be inflated (possibly as high as 0.2).</p>]]></content><author><name></name></author><category term="hypothesis-testing," /><category term="multiple-testing" /><summary type="html"><![CDATA[We show how to handle more than one hypothesis at a time.]]></summary></entry><entry><title type="html">An introduction to Compressive Sensing.</title><link href="http://gregorygundersen.com/blog/2024/10/09/compressive-sensing/" rel="alternate" type="text/html" title="An introduction to Compressive Sensing." /><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/10/09/compressive-sensing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/10/09/compressive-sensing/"><![CDATA[<h1 id="introduction-labelseccsinto">Introduction \label{sec:csinto}</h1>

<p>This post discusses Compressive Sensing \gls{cs}: an alternative signal acquisition method to Nyquist sampling, which is capable of accurate sensing at rates well below those predicted by the Nyquist theorem. This strategy hinges on using the structure of a signal, and the fact that many signals we are interested in can be compressed successfully. Thus, Compressive Sensing acquires the most informative parts of a signal directly.</p>

<p>The first section surveys the mathematical foundation of CS. It covers the Restricted Isometry Property and Stable Embeddings - necessary and sufficient conditions on which a signal can be successfully acquired. Informally, these conditions suggest that the sensing operator preserves pairwise distances between points when projected to a (relatively) low dimensional space from a high dimensional space. We then discuss which operators satisfy this condition, and why. In particular, random ensembles such as the Bernoulli/Gaussian ensembles satisfy this property - we discuss how this can be applied to the problem of wideband spectrum sensing. We also survey a small amount of the theory of Wishart matrices.</p>

<p>The section concludes with an overview of reconstruction algorithms for CS - methods for unpacking the original signal from its compressed representation. We give insight into the minimum number of samples for reconstruction. We survey convex, greedy, and Bayesian approaches; as well as algorithms which are blends of all three. The choice of algorithm is affected by the amount of undersampling required for system performance, the complexity of the algorithm itself, and the desired reconstruction accuracy. In general: the more complex the algorithm, the better the reconstruction accuracy. The allowable undersampling depends upon the signal itself, and the prior knowledge available to the algorithm. Greedy algorithms are the simplest class, making locally optimal updates within a predefined dictionary of signal atoms. Greedy algorithms have relatively poor performance, yet are the fastest algorithms. Convex algorithms are the next most complex, based upon minimising a global functional of the signal. This class is based upon generalised gradient descent, and has no fixed number of steps. Finally, Bayesian algorithms are the slowest and most complex, but offer the best performance - both in terms of undersampling (as these algorithms incorporate prior knowledge in an elegant way), and in terms of reconstruction accuracy.</p>

<p>We survey some distributed approaches to compressive sensing, in particular some models of joint sparsity, and joint sparsity with innovations.</p>

<p>Finally, we survey some of the approaches to wideband spectrum sensing based upon compressive sensing. In particular we survey the Random Demodulator and the Modulated Wideband converter. Both of these systems make use of low-frequency chipping sequences (also used in spread spectrum communication systems). These low-frequency sequences provide the basis for CS - several different sequences each convoluted with the signal are sufficient to accurately sense the signal.</p>

<h1 id="preliminaries-labelsecprelims">Preliminaries \label{sec:prelims}</h1>

<p>Compressive sensing is a modern signal acquisition technique in which randomness is used as an effective sampling strategy. This is in contrast to traditional, or Nyquist, sampling, which requires that a signal is sampled at regular intervals. The motivation for this new method comes from two disparate sources: data compression and sampling hardware design.</p>

<p>The work of Shannon, Nyquist, and Whittaker \cite{unser2000sampling,} has been an extraordinary success - digital signal processing enables the creation of sensing systems which are cheaper, more flexible, and which offer superior performance to their analogue counterparts. For example, radio dongles, such as those which support the RTLSDR standard, which can process millions of samples per second, can now be bought for as little as £12. However, the sampling rates underpinning these advances have a doubling time of roughly 6 years - this is due to physical limitations in Analogue to Digital conversion hardware. Specifically, these devices will always be limited in bandwidth and dynamic range (number of bits), whilst applications are creating a deluge of data to be processed downstream.</p>

<p>Data compression means that in practice many signals encountered ‘in the wild’ can be fully specified by much fewer bits than required by the Nyquist sampling theorem. This is either a natural property of the signals, for example, images have large areas of similar pixels, or as a conscious design choice, as with training sequences in communication transmissions. These signals are not statistically white, and so these signals may be compressed (to save on storage). For example, lossy image compression algorithms can reduce the size of a stored image to about 1% of the size required by Nyquist sampling. In fact, the JPEG standard uses Wavelets to exploit the inter-pixel redundancy of images.</p>

<p>Whilst this vein of research has been extraordinarily successful, it poses the question: if the reconstruction algorithm is able to reconstruct the signal from this compressed representation, why collect all the data in the first place when most of the information can be thrown away?</p>

<p>Compressed Sensing answers these questions by way of providing an alternative signal acquisition method to the Nyquist theorem. Specifically, situations are considered where fewer samples are collected than traditional sensing schemes. That is, in contrast to Nyquist sampling, Compressive Sensing is a method of measuring the informative parts of a signal directly without acquiring unessential information at the same time.</p>

<p>These ideas have not come out of the ether recently however. Prony, in 1795, \cite{prony1795essai}, proposed a method for estimating the parameters of a number of exponentials corrupted by noise. This work was extended by Caratheodory in 1907 \cite{Caratheodory1907}, who proposed in the 1900s a method for estimating a linear combination of \(k\) sinusoids for the state at time \(0\) and any other \(2k\) points. In the 1970s, Geophysicists proposed minimising the \(\ell_1\)-norm to reconstruct the structure of the earth from seismic measurements. Clarebuot and Muir proposed in 1973, \cite{claerbout1973robust}, using the \(\ell_1\)-norm as an alternative to Least squares. Whilst Taylor, Banks, and McCoy showed in \cite{taylor1979deconvolution} how to use the \(\ell_1\)-norm to deconvolve spike trains (used for reconstructing layers in the earth). Santosa and Symes in \cite{Santosa1986} introduced the constrained \(\ell_1\)-program to perform the inversion of band-limited reflection seismograms. The innovation of \gls{cs} is to tell us under which circumstances these problems are tractable.</p>

<p>The key insight in CS is that for signals which are sparse or compressible - signals which are non-zero at only a fraction of the indices over which they are supported, or signals which can be described by relatively fewer bits than the representation they are traditionally captured in - may be measured in a non-adaptive way through a measurement system which is orthogonal to the signal’s domain.</p>

<p>Examples of sparse signals are:</p>

<ol>
  <li>A sine wave at frequency \(\omega\) is defined as a single spike in the frequency domain yet has an infinite support in the time domain.</li>
  <li>An image will have values for every pixel, yet the wavelet decomposition of the image will typically only have a few non-zero coefficients.</li>
</ol>

<p>Informally, CS posits that for \(s\)-sparse signals \(\alpha \in \mathbb{R}^{n}\) (signals with \(s\) non-zero amplitudes at unknown locations) - \(\mathcal{O}(s \log{n})\) measurements are sufficient to exactly reconstruct the signal.</p>

<p>In practice, this can be far fewer samples than conventional sampling schemes. For example, a megapixel image requires 1,000,000 Nyquist samples but can be perfectly recovered from 96,000 compressive samples in the wavelet domain \cite{candes2008introduction}.</p>

<p>The measurements are acquired linearly, by forming inner products between the signal and some arbitrary sensing vector:</p>

<p>\(y_i = \langle \alpha, \psi_i \rangle\)
\label{inner-product-repr}</p>

<p>or</p>

<p>\(y = \Psi \alpha\)
\label{vector-repr}</p>

<p>where \(y_i\) is the \(i^{th}\) measurement, \(\alpha \in \mathbb{R}^n\) is the signal, and \(\psi_i\) is the \(i^{th}\) sensing vector. We pass to \eqref{vector-repr} from \eqref{inner-product-repr}, by concatenating all the \(y_i\) into a single vector. Thus the matrix \(\Psi\) has the vectors \(\psi_i\) as columns.</p>

<p>If \(\alpha\) is not \(s\)-sparse in the natural basis of \(y\), then we can always transform \(\alpha\) to make it sparse in some other basis:</p>

<p>\(x = \Phi \alpha\)
\label{vector-repr-2}</p>

<p>Note that the measurements may be corrupted by noise, in which case our model is:</p>

<p>\(y = Ax + e\)
\label{CSequation}</p>

<p>where \(e \in \mathbb{R}^m\), and each component is sampled from a \(\mathcal{N}(0, 1/n)\) distribution. Here we have \(A = \Psi \Phi \alpha \in \mathbb{R}^{n \times m}\), i.e., a basis in which the signal \(x \in \mathbb{R}^n\) will be sparse.</p>

<p>We require that sensing vectors satisfy two technical conditions (described in detail below): an Isotropy property, which means that components of the sensing vectors have unit variance and are uncorrelated, and an Incoherence property, which means that sensing vectors are almost orthogonal. Once the set of measurements have been taken, the signal may be reconstructed from a simple linear program. We describe these conditions in detail in the next section.</p>

<h2 id="rip-and-stable-embeddings">RIP and Stable Embeddings</h2>

<p>We begin with a formal definition of sparsity:</p>

<p>\begin{definition}[Sparsity]</p>

<p>A high-dimensional signal is said to be \(s\)-sparse, if at most \(s\) coefficients \(x_i\) in the linear expansion</p>

<p>\(\alpha = \sum_{i=1}^{n} \phi_i x_i\)
\label{sparse-basis-expansion}</p>

<p>are non-zero, where \(x \in \mathbb{R}\), \(\alpha \in \mathbb{R}\), and \(\phi_i\) are a set of basis functions of \(\mathbb{R}^n\).</p>

<p>We can write \eqref{sparse-basis-expansion} as:</p>

<p>\(\alpha = \Phi x\)
\label{def:alpha}</p>

<p>We can make the notion of sparsity precise by defining \(\Sigma_s\) as the set of \(s\)-sparse signals in \(\mathbb{R}^n\):</p>

\[\Sigma_s = \{ x \in \mathbb{R}^n : |\mathrm{supp}(x)| \leq s \}\]

<p>where \(\mathrm{supp}(x)\) is the set of indices on which \(x\) is non-zero.</p>

<p>\end{definition}</p>

<p>\begin{figure<em>}[h]
\centering
\includegraphics[height=7cm, width=\textwidth]{compressive_sensing_example.jpg}
\caption{A visualisation of the Compressive Sensing problem as an under-determined system. Image from \cite{cstutwatkin}}
\label{l1l2}
\end{figure</em>}</p>

<p>We may not be able to directly obtain these coefficients \(x\), as we may not possess an appropriate measuring device, or one may not exist, or there is considerable uncertainty about where the non-zero coefficients are.</p>

<p>Given a signal \(\alpha \in \mathbb{R}^n\), a matrix \(A \in \mathbb{R}^{m \times n}\), with \(m \ll n\), we can acquire the signal via the set of linear measurements:</p>

<p>\(y = \Psi \alpha = \Psi\Phi x = Ax\)
\label{cs-model}</p>

<p>where we have combined \eqref{vector-repr} and \eqref{def:alpha}. In this case, \(A\) represents the sampling system (i.e., each column of \(A\) is the product of \(\Phi\) with the columns of \(\Psi\)). We can work with the abstract model \eqref{cs-model}, bearing in mind that \(x\) may be the coefficient sequence of the object in the proper basis.</p>

<p>In contrast to classical sensing, which requires that \(m = n\) for there to be no loss of information, it is possible to reconstruct \(x\) from an under-determined set of measurements as long as \(x\) is sparse in some basis.</p>

<p>There are two conditions the matrix \(A\) needs to satisfy for recovery below Nyquist rates:</p>

<ol>
  <li>Restricted Isometry Property.</li>
  <li>Incoherence between sensing and signal bases.</li>
</ol>

<p>\begin{definition}[RIP]\label{def:RIP}
We say that a matrix \(A\) satisfies the RIP of order \(s\) if there exists a \(\delta \in (0, 1)\) such that for all \(x \in \Sigma_s\):</p>

\[(1 - \delta) \|x\|_2^2 \leq \|Ax\|_2^2 \leq (1 + \delta) \|x\|_2^2\]

<p>i.e., \(A\) approximately preserves the lengths of all \(s\)-sparse vectors in \(\mathbb{R}^n\).</p>

<p>\label{def:RIP}
\end{definition}</p>

<p>\begin{remark}
Although the matrix \(A\) is not square, the RIP (\ref{def:RIP}) ensures that \(A^TA\) is close to the identity, and so \(A\) behaves approximately as if it were orthogonal. This is formalised in the following lemma from \cite{shalev2014understanding}:</p>

<p>\begin{lemma}[Identity Closeness \cite{shalev2014understanding}]
Let \(A\) be a matrix that satisfies the RIP of order \(2s\) with RIP constant \(\delta\). Then for two disjoint subsets \(I, J \subset [n]\), each of size at most \(s\), and for any vector \(u \in \mathbb{R}^n\):</p>

\[\langle Au_I, Au_J \rangle \leq \delta \|u_I\|_2 \|u_J\|_2\]

<p>where \(u_I\) is the vector with component \(u_i\) if \(i \in I\) and zero elsewhere.</p>

<p>\end{lemma}</p>

<p>\end{remark}</p>

<p>\begin{remark} \label{rem:rip-delta-comment}
The restricted isometry property is equivalent to stating that all eigenvalues of the matrix \(A^TA\) are in the interval \([1 - \delta, 1 + \delta]\). Thus, the meaning of the constant \(\delta\) in (\ref{def:RIP}) is now apparent. \(\delta\) is called the \textit{restricted isometry constant} in the literature.</p>

<p>The constant \(\delta\) in \eqref{def:RIP} measures how close to an isometry the action of the matrix \(A\) is on vectors with a few non-zero entries (as measured in \(\ell_2\) norm). For random matrices \(A\) where the components are drawn from a \(\mathcal{N}(0, 1/n)\) distribution, \(\delta &lt; \sqrt{2} - 1\) \cite{candes2008restricted}.
\end{remark}</p>

<p>\begin{remark} [Information Preservation \cite{davenport2010signal}]
A necessary condition to recover all \(s\)-sparse vectors from the measurements \(Ax\) is that \(Ax_1 \neq Ax_2\) for any pair \(x_1 \neq x_2\), \(x_1, x_2 \in \Sigma_s\), which is equivalent to:</p>

\[\|A(x_1 - x_2)\|_2^2 &gt; 0\]

<p>This is guaranteed as long as \(A\) satisfies the RIP of order \(2s\) with constant \(\delta\). The vector \(x_1 - x_2\) will have at most \(2s\) non-zero entries, and so will be distinguishable after multiplication with \(A\). To complete the argument, take \(x = x_1 - x_2\) in definition \ref{def:RIP}, guaranteeing:</p>

\[\|A(x_1 - x_2)\|_2^2 &gt; 0\]

<p>and requiring the RIP order of \(A\) to be \(2s\).
\end{remark}</p>

<p>\begin{remark} [Stability \cite{davenport2010signal}]
We also require that the dimensionality reduction of compressed sensing preserves relative distances: that is, if \(x_1\) and \(x_2\) are far apart in \(\mathbb{R}^n\), then their projections \(Ax_1\) and \(Ax_2\) are far apart in \(\mathbb{R}^m\). This will guarantee that the dimensionality reduction is robust to noise.
\end{remark}</p>

<p>A requirement on the matrix \(A\) that satisfies both of these conditions is the following:</p>

<p>\begin{definition}[\(\delta\)-stable embedding \cite{davenport2010signal}]
We say that a mapping is a \(\delta\)-stable embedding of \(U,V \subset \mathbb{R}^n\) if</p>

\[(1 - \delta) \|u - v\|_2^2 \leq \|Au - Av\|_2^2 \leq (1 + \delta) \|u - v\|_2^2\]

<p>for all \(u \in U\) and \(v \in V\).</p>

<p>\label{def:d-stable}
\end{definition}</p>

<p>\begin{remark}[\cite{davenport2010signal}]
Note that a matrix \(A\), satisfying the RIP of order \(2s\), is a \(\delta\)-stable embedding of \(\Sigma_s, \Sigma_s\). 
\end{remark}</p>

<p>\begin{remark}[\cite{davenport2010signal}]
Definition \ref{def:d-stable} has a simple interpretation: the matrix \(A\) must approximately preserve Euclidean distances between all points in the signal model \(\Sigma_s\).
\end{remark}</p>

<h1 id="incoherence">Incoherence</h1>

<p>Given that we know a basis in which our signal is sparse, \(\phi\), how do we choose \(\psi\) so that we can accomplish this sensing task? In classical sensing, we choose \(\psi_k\) to be the set of \(T_s\)-spaced delta functions (or equivalently the set of \(1/T_s\) spaced delta functions in the frequency domain). A simple set of \(\psi_k\) would be to choose a (random) subset of the delta functions above.</p>

<p>In general, we seek waveforms in which the signals’ representation would be dense.</p>

<p>\begin{definition}[Incoherence]
A pair of bases is said to be incoherent if the largest projection of two elements between the sensing (\(\psi\)) and representation (\(\phi\)) basis is in the set \([1, \sqrt{n}]\), where \(n\) is the dimension of the signal. The coherence of a set of bases is denoted by \(\mu\).
\end{definition}</p>

<p>Examples of pairs of incoherent bases are:</p>

<ul>
  <li>Time and Fourier bases: Let \(\Phi = \mathbf{I}_n\) be the canonical basis and \(\Psi = \mathbf{F}\) with \(\psi_i = n^{-\frac{1}{2}} e^{i \omega k}\) be the Fourier basis, then \(\mu(\phi, \psi) = 1\). This corresponds to the classical sampling strategy in time or space.</li>
  <li>Consider the basis \(\Phi\) to have only entries in a single row, then the coherence between \(\Phi\) and any fixed basis \(\Psi\) will be \(\sqrt{n}\).</li>
  <li>Random matrices are incoherent with any fixed basis \(\Psi\). We can choose \(\Phi\) by creating \(n\) orthonormal vectors from \(n\) vectors sampled independently and uniformly on the unit sphere. With high probability \(\mu = \sqrt{n \log n}\). This extends to matrices whose rows are created by sampling independent Gaussian or Bernoulli random vectors.</li>
</ul>

<p>This implies that sensing with incoherent systems is good (in the sine wave example above, it would be better to sample randomly in the time domain as opposed to the frequency domain), and efficient mechanisms ought to acquire correlations with random waveforms (e.g., white noise).</p>

<p>\begin{theorem}[Reconstruction from Compressive measurements \cite{Candes2006}]
Fix a signal \(f \in \mathbb{R}^n\) with a sparse coefficient basis, \(x_i\) in \(\phi\). Then a reconstruction from \(m\) random measurements in \(\psi\) is possible with probability \(1 - \delta\) if:</p>

<p>\(m \geq C \mu^2(\phi, \psi) S \log \left( \frac{n}{\delta} \right)\)
\label{minsamples}</p>

<p>where \(\mu(\phi, \psi)\) is the coherence of the two bases, and \(S\) is the number of non-zero entries on the support of the signal.
\end{theorem}</p>

<h2 id="random-matrix-constructions-labelsecmtx-contruction">Random Matrix Constructions \label{sec:mtx-contruction}</h2>

<p>To construct matrices satisfying definition \eqref{def:d-stable}, given \(m, n\) we generate \(A\) by \(A_{ij}\) being i.i.d random variables from distributions with the following conditions \cite{davenport2010signal}:</p>

<p>\begin{condition}[Norm preservation]
\(\mathbb{E} A_{ij}^2 = \frac{1}{m}\)
\label{cond:norm-pres}
\end{condition}</p>

<p>\begin{condition}[sub-Gaussian]
There exists a \(C &gt; 0\) such that:
\(\mathbb{E}\left( e^{A_{ij}t} \right) \leq e^{C^2 t^2 /2}\)
\label{cond:sub-Gauss}
for all \(t \in \mathbb{R}\).
\end{condition}</p>

<p>\begin{remark}
The term \(\mathbb{E}\left( e^{A_{ij}t} \right)\) in \eqref{cond:sub-Gauss} is the <em>moment generating function</em> of the sensing matrix. Condition \eqref{cond:sub-Gauss} says that the moment-generating function of the distribution producing the sensing matrix is dominated by that of a Gaussian distribution, which is also equivalent to requiring that the tails of our distribution decay at least as fast as the tails of a Gaussian distribution. Examples of sub-Gaussian distributions include the Gaussian distribution, the Rademacher distribution, and the uniform distribution. In general, any distribution with bounded support is sub-Gaussian. The constant \(C\) measures the rate of fall off of the tails of the sub-Gaussian distribution.
\end{remark}</p>

<p>Random variables \(A_{ij}\) satisfying conditions \eqref{cond:norm-pres} and \eqref{cond:sub-Gauss} satisfy the following concentration inequality \cite{baraniuk2008simple}:</p>

<p>\begin{lemma}[sub-Gaussian \cite{baraniuk2008simple}]
\(\mathbb{P}\Big( \biggl\lvert \|Ax\|_2^2 - \|x\|_2^2 \biggr\rvert \geq \varepsilon \|x\|_2^2 \Big) \leq 2e^{-cM\varepsilon^2}\)
\label{cond:sub-Gauss concetration}
\end{lemma}</p>

<p>\begin{remark}
Lemma \ref{cond:sub-Gauss concetration} says that sub-Gaussian random variables are random variables such that for any \(x \in \mathbb{R}^n\), the random variable \(\|Ax\|_2^2\) is highly concentrated about \(\|x\|_2^2\).
\end{remark}</p>

<p>Then in \cite{baraniuk2008simple} the following theorem is proved:</p>

<p>\begin{theorem}
Suppose that \(m\), \(n\) and \(0 &lt; \delta &lt; 1\) are given. If the probability distribution generating \(A\) satisfies condition \eqref{cond:sub-Gauss}, then there exist constants \(c_1, c_2\) depending only on \(\delta\) such that the RIP \eqref{def:RIP} holds for \(A\) with the prescribed \(\delta\) and any \(s \leq \frac{c_1 n}{\log{n/s}}\) with probability \(\geq 1-2e^{-c_2n}\).
\end{theorem}</p>

<p>For example, if we take \(A_{ij} \sim \mathcal{N}(0, 1/m)\), then the matrix \(A\) will satisfy the RIP, with probability \cite{baraniuk2008simple}:</p>

\[\geq 1 - 2\left(\frac{12}{\delta}\right)^k e^{-c_0\frac{\delta}{2}n}\]

<p>where \(\delta\) is the RIP constant, \(c_0\) is an arbitrary constant, and \(k\) is the sparsity of the signal being sensed.</p>

<h2 id="wishart-matrices-labelsecwishart">Wishart Matrices \label{sec:wishart}</h2>

<p>Let \(\{X_i\}_{i=1}^r\) be a set of i.i.d. \(1 \times p\) random vectors drawn from the multivariate normal distribution with mean 0 and covariance matrix \(H\).</p>

\[X_i = \left(x_1^{(i)}, \ldots, x_p^{(i)}\right) \sim N\left(0, H\right)\]

<p>We form the matrix \(X\) by concatenating the \(r\) random vectors into an \(r \times p\) matrix.</p>

<p>\begin{definition}[Wishart Matrix]
Let</p>

\[W = \sum_{j=1}^r X_j X_j^T = X X^T\]

<p>Then \(W \in \mathbb{R}^{r \times r}\) has the Wishart distribution with parameters:</p>

\[W_r(H, p)\]

<p>where \(p\) is the number of degrees of freedom.
\end{definition}</p>

<p>\begin{remark}
This distribution is a generalization of the Chi-squared distribution if \(p = 1\) and \(H = I\).
\end{remark}</p>

<p>\begin{theorem}[Expected Value] \label{thm:wishart-mean}
\(\mathbb{E}(W) = rH\)
\end{theorem}</p>

<p>\begin{proof}
\begin{align<em>}
\mathbb{E}(W) &amp;= \mathbb{E}\left(\sum_{j=1}^r X_j X_j^T\right) <br />
&amp;= \sum_{j=1}^r \mathbb{E}(X_j X_j^T) <br />
&amp;= \sum_{j=1}^r \left( \mathrm{Var}(X_j) + \mathbb{E}(X_j) \mathbb{E}(X_j^T) \right) <br />
&amp;= rH
\end{align</em>}
Where the last line follows as \(X_j\) is drawn from a distribution with zero mean.
\end{proof}</p>

<p>\begin{remark}
The matrix \(M = A^T A\), where \(A\) is constructed by the methods from section \ref{sec:mtx-contruction}, will have a Wishart distribution. In particular, it will have:</p>

<p>\(\mathbb{E}(M) = \frac{1}{m} I_n\)
\label{remark: exp AtA}
\end{remark}</p>

<p>The joint distribution of the eigenvalues is given by \cite{levequeMatrices}:</p>

\[p\left(\lambda_1, \ldots, \lambda_r\right) = c_r \prod_{i=1}^r e^{-\lambda_i} \prod_{i&lt;j} \left(\lambda_i - \lambda_j\right)^2\]

<p>The eigenvectors are uniform on the unit sphere in \(\mathbb{R}^r\).</p>

<h2 id="reconstruction-objectives">Reconstruction Objectives</h2>

<p>Compressive sensing places the computational load on reconstructing the coefficient sequence \(x\), from the set of compressive samples \(y\). This is in contrast to Nyquist sampling, where the bottleneck is in obtaining the samples themselves—reconstructing the signal is a relatively simple task.</p>

<p>Many recovery algorithms have been proposed, and all are based upon minimizing some functional of the data. This objective is based upon two terms: a data fidelity term, minimizing the discrepancy between the reconstructed and true signal, and a regularization term—biasing the reconstruction towards a class of solutions with desirable properties, for example sparsity. Typically the squared error \(\frac{1}{2} \|y - Ax\|_2^2\) is chosen as the data fidelity term, while several regularization terms have been introduced in the literature.</p>

<p>A particularly important functional is:</p>

<p>\(\arg \min_x \|x\|_1 \text{ s.t. } y = Ax\)
\label{program:bp}</p>

<p>This is known as Basis Pursuit \cite{Chen1998a}, with the following program known as the LASSO \cite{tibshirani1996regression} as a noisy generalization:</p>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_1\)
\label{program:lasso}</p>

<p>The statistical properties of LASSO have been well studied. The program performs both regularization and variable selection: the parameter \(\lambda\) trades off data fidelity and sparsity, with higher values of \(\lambda\) leading to sparser solutions.</p>

<p>The LASSO shares several features with Ridge regression \cite{hoerl1970ridge}:</p>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_2^2\)
\label{program:Ridge-regression}</p>

<p>and the Non-negative Garrote \cite{breiman1995better}, used for best subset regression:</p>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_0\)
\label{program:ell0}</p>

<p>The solutions to these programs can all be related to each other—it can be shown \cite{hastie2005elements} that the solution to \eqref{program:lasso} can be written as:</p>

<p>\(\hat{x} = S_{\lambda}(x^{OLS}) = x^{OLS} \text{ sign}(x_i - \lambda)\)
\label{soln:lasso}</p>

<p>where \(x^{OLS} = (A^T A)^{-1} A^T y\) is the ordinary least squares solution, whereas the solution to Ridge regression can be written as:</p>

<p>\(\hat{x} = \left( 1 + \lambda \right)^{-1} x^{OLS}\)
\label{soln:ridge}</p>

<table>
  <tbody>
    <tr>
      <td>and the solution to the best subset regression \eqref{program:ell0} where $$|x|_0 = {</td>
      <td>i</td>
      <td>: x_i \neq 0 }$$, can be written as:</td>
    </tr>
  </tbody>
</table>

<p>\(\hat{x} = H_{\lambda}(x^{OLS}) = x^{OLS} \mathbb{I}\left( |x^{OLS}| &gt; \lambda \right)\)
\label{soln:l0}</p>

<p>where \(\mathbb{I}\) is the indicator function. From \eqref{soln:l0} and \eqref{soln:ridge}, we can see that the solution to \eqref{program:lasso}, \eqref{soln:lasso}, translates coefficients towards zero by a constant factor, and sets coefficients to zero if they are too small; thus, the LASSO is able to perform both model selection (choosing relevant covariates) and regularization (shrinking model coefficients).</p>

<p><img src="l1l2.jpg" alt="Solutions to the Compressive Sensing optimization problem intersect the $$l_1$$ norm at points where all components (but one) of the vector are zero (i.e., it is sparsity promoting) \cite{Tibshirani1996}." />
\label{fig:l1l2}</p>

<p>Figure \ref{fig:l1l2} provides a graphical demonstration of why the LASSO promotes sparse solutions. \eqref{program:lasso} can also be thought of as the best convex approximation of the \(\ell_0\) problem \eqref{program:ell0}, as the \(\ell_1\)-norm is the convex hull of the points defined by \(\|x\|_p\) for \(p &lt; 1\) as \(p \rightarrow 0\).</p>

<p>Other examples of regularizers are:</p>

<ul>
  <li><strong>Elastic Net</strong>: This estimator is a blend of both \eqref{program:lasso} and \eqref{program:Ridge-regression}, found by minimizing:</li>
</ul>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_2^2 + \mu \|x\|_1\)
\label{program:enat}</p>

<p>The estimate has the benefits of both Ridge and LASSO regression: feature selection from the LASSO, and regularization for numerical stability (useful in the under-determined case we consider here) from Ridge regression. The Elastic Net will outperform the LASSO \cite{zou2005regularization} when there is a high degree of collinearity between coefficients of the true solution.</p>

<ul>
  <li><strong>TV regularization</strong>:</li>
</ul>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|\nabla x\|_1\)
\label{program:tc}</p>

<p>This type of regularization is used when preserving edges while simultaneously denoising a signal is required. It is used extensively in image processing, where signals exhibit large flat patches alongside large discontinuities between groups of pixels.</p>

<ul>
  <li>Candes and Tao in \cite{candes2007dantzig} propose an alternative functional:</li>
</ul>

<p>\(\min_{x \in \mathbb{R}^n} \|x\|_1 \text{ s.t. } \|A^T(Ax - y)\|_{\infty} \leq t\sigma\)
\label{program:danzig}</p>

<p>with \(t = c \sqrt{2 \log{n}}\). Similarly to the LASSO, this functional selects sparse vectors consistent with the data, in the sense that the residual \(r = y - Ax\) is smaller than the maximum amount of noise present. In \cite{candes2007dantzig} it was shown that the \(l_2\) error of the solution is within a factor of \(\log{n}\) of the ideal \(l_2\) error. More recent work by Bikel, Ritov, and Tsybakov \cite{bickel2009simultaneous} has shown that the LASSO enjoys similar properties.</p>

<h2 id="reconstruction-algorithms">Reconstruction Algorithms</h2>
<p>Broadly, reconstruction algorithms fall into three classes: convex-optimisation/linear programming, greedy algorithms, and Bayesian inference. Convex optimisation methods offer better performance, measured in terms of reconstruction accuracy, at the cost of greater computational complexity. Greedy methods are relatively simpler, but don’t have the reconstruction guarantees of convex algorithms. Bayesian methods offer the best reconstruction guarantees, as well as uncertainty estimates about the quality of reconstruction, but come with considerable computational complexity.</p>

<table>
  <thead>
    <tr>
      <th>Algorithm Type</th>
      <th>Accuracy</th>
      <th>Complexity</th>
      <th>Speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Greedy</td>
      <td>Low</td>
      <td>Low</td>
      <td>Fast</td>
    </tr>
    <tr>
      <td>Convex</td>
      <td>Medium</td>
      <td>Medium</td>
      <td>Medium</td>
    </tr>
    <tr>
      <td>Bayesian</td>
      <td>High</td>
      <td>High</td>
      <td>Slow</td>
    </tr>
  </tbody>
</table>

<h3 id="convex-algorithms">Convex Algorithms</h3>
<p>Convex methods cast the optimisation objective either as a linear program with linear constraints or as a second-order cone program with quadratic constraints. Both of these types of programs can be solved with first-order interior point methods. However, their practical application to compressive sensing problems is limited due to their polynomial dependence upon the signal dimension and the number of constraints.</p>

<p>Compressive sensing poses a few difficulties for convex optimisation-based methods. In particular, many of the unconstrained objectives are non-smooth, meaning methods based on descent down a smooth gradient are inapplicable.</p>

<p>To overcome these difficulties, a series of algorithms originally proposed for wavelet-based image de-noising have been applied to CS, known as iterative shrinkage methods. These have the desirable property that they boil down to matrix-vector multiplications and component-wise thresholding.</p>

<p>Iterative shrinkage algorithms replace searching for a minimal facet of a complex polytope with an iteratively denoised gradient descent. The choice of the (component-wise) denoiser is dependent upon the regulariser used in \(\eqref{program:lasso}\). These algorithms have an interpretation as Expectation-Maximisation \cite{figueiredo2003algorithm} — where the E-step is performed as gradient descent, and the M-step is the application of the denoiser.</p>

<h4 id="iterative-soft-thresholding-algorithm-ist">Iterative Soft Thresholding Algorithm (IST)</h4>
<p>```python
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{IST}{$y,A, \mu, \tau, \varepsilon$}
\State $x^0 = 0$
\While{$|x^{t} - x^{t-1}|<em>2^2 \leq \varepsilon$}
\State $x^{t+1} \gets S</em>{\mu\tau}\left(x^t + \tau A^T z^t \right) $
\State $z^t \gets y - A x^t$
\EndWhile
\State \textbf{return} $x^{t+1}$
\EndProcedure
\end{algorithmic}
\caption{The Iterative Soft Thresholding Algorithm \cite{donoho1995noising}}\label{alg:IST}
\end{algorithm}</p>
<h2 id="bayesian-algorithms">Bayesian Algorithms</h2>
<p>Bayesian methods reformulate the optimisation problem into an inference problem. These methods come with a unified theory and standard methods to produce solutions. The theory is able to handle hyper-parameters in an elegant way, provides a flexible modelling framework, and is able to provide desirable statistical quantities such as the uncertainty inherent in the prediction.</p>

<p>Previous sections have discussed how the weights \(x\) may be found through optimisation methods such as basis pursuit or greedy algorithms. Here, an alternative Bayesian model is described.</p>

<p>Equation \(\eqref{CSequation}\) implies that we have a Gaussian likelihood model:</p>

\[p \left(y \mid z, \sigma^2 \right) = (2 \pi \sigma^2)^{-K/2} \exp{\left(- \frac{1}{2 \sigma^2} \|y - Ax|_{2}^{2} \right)}\]

<p>The above has converted the CS problem of inverting sparse weight \(x\) into a linear regression problem with a constraint (prior) that \(x\) is sparse.</p>

<p>To seek the full posterior distribution over \(x\) and \(\sigma^2\), we can choose a sparsity-promoting prior. A popular sparseness prior is the Laplace density function:</p>

\[p\left(x \mid \lambda\right) = \left(\frac{\lambda}{2}\right)^N \exp{-\lambda \sum_{i=1}^{N} |x_i|}\]

<p>Note that the solution to the convex optimisation problem \(\eqref{program:lasso}\) corresponds to a maximum <em>a posteriori</em> estimate for \(x\) using this prior. I.e., this prior is equivalent to using the \(l_1\) norm as an optimisation function (see figure \ref{laplacenormal} \cite{Tibshirani1996}).</p>

<p>\begin{figure<em>}[h]
\centering
\includegraphics[height=7cm]{LaplaceandNormalDensity.png}
\caption{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity-promoting as it penalises solutions away from zero more than the Gaussian density. \cite{Tibshirani1996}}
\label{laplacenormal}
\end{figure</em>}</p>

<p>The full posterior distribution on \(x\) and \(\sigma^2\) may be realised by using a hierarchical prior instead. To do this, define a zero-mean Gaussian prior on each element of \(e\):</p>

\[p\left(e \mid a\right) = \prod_{i=1}^{N} \mathbb{N}\left(n_i \mid 0, \alpha_{i}^{-1}\right)\]

<p>where \(\alpha\) is the precision of the distribution. A gamma prior is then imposed on \(\alpha\):</p>

\[p\left(\alpha \mid a, b \right) = \prod_{i=1}^{N} \Gamma\left( \alpha_i \mid a, b \right)\]

<p>The overall prior is found by marginalising over the hyperparameters:</p>

\[p\left( x \mid a, b \right) = \prod_{i=1}^{N} \int_{0}^{\infty} \mathbb{N}\left(w_i \mid 0, \alpha_{i}^{-1}\right) \Gamma\left( \alpha_i \mid a, b \right)\]

<p>This integral can be done analytically and results in a Student-t distribution. Choosing the parameters \(a\) and \(b\) appropriately, we can make the Student-t distribution peak strongly around \(x_i = 0\), i.e., sparsifying. This process can be repeated for the noise variance \(\sigma^2\). The hierarchical model for this process is shown in figure \ref{fig:bayesiancs}. This model, and other CS models which do not necessarily have closed-form solutions, can be solved via belief-propagation \cite{Baron2010}, or via Monte-Carlo methods.</p>

<p>\begin{figure}[h]
\centering
\includegraphics[height=7cm]{bayesiancs.png}
\caption{The hierarchical model for the Bayesian CS formulation \cite{Ji2008}}
\label{fig:bayesiancs}
\end{figure}</p>

<p>However, as with all methodologies, Bayesian algorithms have their drawbacks. Most notable is the use of the most computationally complex recovery algorithms. In particular, MCMC methods suffer in high-dimensional settings, such as those considered in compressive sensing. There has been an active line of work to address this: most notably, Hamiltonian Monte Carlo (see \cite{neal2011mcmc}) — an MCMC sampling method designed to follow the typical set of the posterior density.</p>

<p>Belief propagation (BP) \cite{Yedidia2011} is a popular iterative algorithm, offering improved reconstruction quality and undersampling performance. However, it is a computationally complex algorithm. It is also difficult to implement. Approximate message passing (AMP) (figure \ref{alg:amp}) solves this issue by blending BP and iterative thresholding (figure \ref{alg:IST}). The algorithm proceeds like iterative thresholding but computes an adjusted residual at each stage. The final term in the update:</p>

\[z^{t+1} = y - Ax^t + \frac{\|x\|_0}{m} z^t\]

<p>comes from a first-order approximation to the messages passed by BP \cite{metzler2014denoising}. This is in contrast to the update from IST (figure \ref{alg:IST}):</p>

\[z^{t+1} = y - Ax^t\]

<p>The choice of prior is key in Bayesian inference, as it encodes all knowledge about the problem. Penalising the least-squares estimate with the \(\ell_1\) norm,</p>

<h2 id="compressive-estimation-labelsecestimation">Compressive Estimation \label{sec:estimation}</h2>
<p>In this section, we develop some intuition into constructing estimators for the signal \(s\) directly from the compressive measurements:</p>

<p>\begin{theorem}</p>

<p>Given a set of measurements of the form:</p>

<p>\(y = As + e\)
<br />
where \(A \in \re^{m \times n}\), \(A_{ij} \sim \mathcal{N}\left(0,1/m\right)\), and \(e \in \re^n\) is AWGN, i.e., \(\sim N\left(0, \sigma^2 I\right)\). We again assume that \(s\) comes from a fixed set of models, parametrised by some set \(\Theta\).</p>

<p>Then, the maximum likelihood estimator of \(s\), for the case where \(s\) can be expanded in an orthonormal basis \(s = \sum_{i=1}^n \alpha_i\phi_i\):</p>

\[\hat{s} = \sum_{i=1}^n m \langle y, A\phi_i \rangle \phi_i\]

<p>\end{theorem}
\begin{proof}
The likelihood for this model is (as \(y\) is a normal random variable):</p>

\[f\left(y \mid s\right) = \left(\frac{1}{\left(2\pi\right)^{n/2}}\right) \exp{\left(- \frac{\left(y-As\right)^T  \left(y-As\right)}{2} \right)}\]

<p>Taking the logarithm and expanding, we find</p>

\[\ln{f\left(y \mid s\right)} = -y^Ty - s^TA^TAs + 2\langle y, As \rangle + c\]

<p>which is equal to:</p>

\[\ln{f} = - \|y\|_2^2 - \|As\|_2^2 + 2\langle y, As \rangle
\label{log-like}\]

<p>(where the constant has been dropped). The first term of \(\eqref{log-like}\) is constant, for the same reasons as in section \(\eqref{sec:estimation}\). The term</p>

\[\|As\|_2^2 = \langle As, As \rangle\]

<p>can be written as</p>

\[\langle A^TAs, s\rangle
\label{ata}\]

<p>We will assume that \(A^TA\) concentrates in the sense of \ref{cond:sub-Gauss concentration} and replace \ref{ata} with its expectation \(\ep{\left( \langle A^TAs, s\rangle \right)}\)</p>

<p>\begin{align<em>}
\ep{\left(\langle A^TAs, s\rangle\right)} &amp;=  \ep{\sum_{i=1}^n (A^TAs)^T_i s_i} <br />
&amp;= \sum_{i=1}^n \ep{(A^TAs)_i s_i} <br />
&amp;= \sum_{i=1}^n \left(\frac{1}{m} e_i s_i\right)^T_i s_i <br />
&amp;= \frac{1}{m} \langle s, s \rangle
\end{align</em>}</p>

<p>because</p>

\[\ep{A^TA} = \frac{1}{m} I\]

<p>as it is a Wishart matrix (see section \ref{sec:prelims}). 
<br />
So we can further approximate \eqref{log-like}:</p>

\[\ln{f\left(y \mid s\right)}  = - \|y\|_2^2 - \frac{1}{m} \|s\|_2^2 + 2 \langle y, As \rangle
\label{approx-log-like}\]

<p>The only non-constant part of \eqref{approx-log-like} is the third term, and so we define the estimator:</p>

<p>\(\hat{s} = \argmax_{\Theta} \langle y , As\left(\Theta\right)\rangle
\label{eq: compressive-estimator}\)
\end{proof}</p>

<p>\begin{corollary}
Consider the case where \(y = As\) (no noise). Then</p>

<p>\begin{align<em>}
y^TA\phi_j &amp;= \sum_i \alpha_i \phi_i^TA^TA\phi_j
\end{align</em>}</p>

<p>So</p>

<p>\begin{align<em>}
y^TA\phi_j &amp;= \sum_i \alpha_i \phi_i^TA^TA\phi_j \sim \frac{\alpha_i}{m} \delta_{ij}
\end{align</em>}</p>

<p>giving</p>

<p>\(\widehat{\alpha_i} = m \left( y^T A \phi_j \right)\)
\end{corollary}</p>

<p>\begin{remark}
The matrix \(M = A^TA\) is the projection onto the row-space of \(A\). It follows that \(\|Ms\|_2^2\) is simply the norm of the component of \(s\) which lies in the row-space of \(A\). This quantity is at most \(\|s\|_2^2\), but can also be \(0\) if \(s\) lies in the null space of \(A\). However, because \(A\) is random, we can expect that \(\|Ms\|_2^2\) will concentrate around \(\sqrt{m/n} \|s\|_2^2\) (this follows from the concentration property of sub-Gaussian random variables \eqref{cond:sub-Gauss concentration}).
\end{remark}</p>

<p>\begin{example}{Example: Single Spike}
We illustrate these ideas with a simple example: estimate which of \(n\) frequencies \(s\) is composed of.</p>

<p>A signal \(s \in \mathbb{R}^{300}\) is composed of a single (random) delta function, with coefficients drawn from a Normal distribution (with mean 100, and variance 1), i.e.,</p>

<p>\(s = \alpha_i \delta_i\)
<br />
with</p>

\[a_i \sim \mathcal{N}\left(100, 1\right)\]

<p>and the index \(i\) chosen uniformly at random from \([1, n]\).
<br />
The signal was measured via a random Gaussian matrix \(A \in \mathbb{R}^{100 \times 300}\), with variance \(\sigma^2 = 1/100\), and the inner product between \(y = As\) and all 300 delta functions projected onto \(\mathbb{R}^{100}\) was calculated:</p>

\[\hat{\alpha}_j = m \langle (A \alpha_i \delta_i), A \delta_j \rangle\]

<p>We plot the \(\hat{\alpha_j}\) below, figure \ref{fig:new_basis_25}, (red circles), with the original signal (in blue, continuous line). Note how the maximum of the \(\hat{\alpha_j}\) coincides with the true signal.</p>

<p>\begin{figure}[h]
\centering
\includegraphics[height=7.3cm]{1spike_legend.jpg}
\label{fig:new_basis_25}
\caption{An example estimation of a single spike using Compressive Inference methods. Note how the maximum of the estimator \(\hat{\alpha}_j\) corresponds to the true signal.}
\end{figure}
\end{example}</p>]]></content><author><name></name></author><category term="Compressive" /><category term="sensing." /><summary type="html"><![CDATA[Introduction \label{sec:csinto}]]></summary></entry><entry><title type="html">Must we adjust p-values if we test multiple hypotheses?</title><link href="http://gregorygundersen.com/blog/2024/05/16/Must-we-adjust-p-values-if-we-test-multiple-hypotheses/" rel="alternate" type="text/html" title="Must we adjust p-values if we test multiple hypotheses?" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/05/16/Must-we-adjust-p-values-if-we-test-multiple-hypotheses</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/05/16/Must-we-adjust-p-values-if-we-test-multiple-hypotheses/"><![CDATA[<p>Jerzy Neyman and Egon Pearson developed their idea of hypothesis testing in 1933. This has since become the dominant paradigm of statistical inference. The goal of Neyman and Pearson’s method were statistical tests which control errors. So scientist’s could adopt claims without being wrong too often. A single number, alpha, controls the long run amount of wrong decisions. It is set before a scientist conducts any experiment. One corollary of this is that the scientific literature contains errors. Be careful about trusting scientific claims.</p>

<p>If you want to test more than one hypothesis, then you can’t use the overall alpha for all tests. Using the same alpha for all tests inflates the false positive error rate. A false positive is when a statistical test claims that an effect is present, but no effect is present. When we add an endpoint to our experiment we are not increasing the number of endpoints by one. We increase the number of endpoints by an order of magnitude. With one dependent variable, you have two endpoints for the experiment: either true (T) or false (F). Adding a second dependent variable, the experiment table: FF, FT, TF, TT. As you add more hypotheses it becomes more likely that any single one will be a false positive. To prevent this, you need to control the error rate at the level of the claim. Statisticians recommend that researchers use some form of multiplicity correction in these situations.</p>

<p>Adding an endpoint to your analysis is legitimate. For example, testing whether a drug is effective for depression and anxiety. But leaving your significance threshold uncorrected is not legitimate. Some scientific disciplines question whether they need multiplicity corrections. Any paper which doesn’t advocate using multiplicity correction is wrong. The need for adjustments is a straightforward consequence of the Neyman-Pearson approach.
There are two cases which require multiplicity corrections:</p>
<ul>
  <li>When your experiment has multiple endpoints.</li>
  <li>When your experiment has multiple levels of an intervention.</li>
</ul>

<p>The simplest approach to multiple comparisons is by controlling the familywise error rate (FWER). The FWER is the probability of making at least one error across a family of tests. It ensures that the total rate of false positives results is always below a strict threshold. Keeping the FWER at an acceptable level is challenging, so it is a stringent rule. The best-known method to control the FWER is the Bonferroni correction. This correction divides the significance level by the number of tests. This controls the alpha of the overall family of tests. Researchers use Bonferroni corrections as they are simple. The resulting significance threshold can be known ahead of time. The drawback of the Bonferroni correction is that it applies the harshest correction. It is so conservative that it can miss true effects.</p>

<p>What a family of tests is, requires some thought. Error control does not aim to constrain the number of erroneous statistical inferences. It also aims to constrain the number of erroneous theoretical inferences. So we need to understand which tests relate to a single theoretic inference. Researchers don’t correct for multiple comparisons becuase they don’t understand their theoretical inference.</p>

<p>We can control error rates for all tests in an experiment. This is the experimentwise error rate. Or we can contorl error rates for a specific group of tests the familywise error rate. Broad questions have many possible answers. As an example consider running a 2x2x2 ANOVA. We test for three main effects, three two-way interactions, and one three-way interaction. This makes seven tests in total. We want to know if there is ‘an effect’ out of any of the interactions we can test for. Rejecting the null-hypothesis in any test would suggest the answer to our question is ‘yes’. Here the experimentwise error rate is what we need to correct for. This is the probability of deciding there is any effect, when all null hypotheses are true. A 5% alpha level for every test in the ANOVA would suggest that there is an effect 30% of the time. This is even when all null hypotheses are true.</p>

<p>Researchers are often vague about their claims. They do not specify their hypotheses clearly enough. So the issue of multiple comparisons remains.. Suppose we want to compare predictions from two competing theories. So we design an experiment to test them. Theory A predicts an interaction in a 2x2 ANOVA. Theory B predicts no interaction, but at least one significant main effect.</p>

<p>The researcher will perform three statistical tests. But don’t assume that we need to control the error rate across all three by using a/3! A ‘family’ depends on a set of related tests. In this case, where we test two theories, there are two families of tests. The first family consists of a single interaction effect. The second family of two main effects. For both families the overall alpha level is 5%. We will decide to accept Theory A when p &lt; α for the interaction, and when p &gt; α we will decide not to accept Theory A . If the null is true, at most 5% of these decisions we make in the long run will be incorrect. So we control the percentage of decision errors. For the second theory, we need to apply a Bonferroni correction. We will decide to accept Theory B when p &lt; α/2 for either of the two main effects, and not accept theory B when p &gt; α/2. When the null hypothesis is true, we will decide to accept Theory B when it is not true at most 5% of the time. We could still accept neither theory, or even both, if the experiment was not the decisive test.</p>

<p>Here’s another example. Let’s assume we collect data from 100 participants in a control and treatment condition. We collect 3 dependent variables (dv1, dv2, and dv3). The true effect size is 0). We will analyse the three dv’s in independent t-tests. We must specify our alpha level, and decide if we need to correct for multiplicity. How we control error rates depends on the claim we want to make. One claim is that the treatment works if there is an effect on any of the three variables. This means we corroborate the claim when the p-value of the any t-test is smaller than alpha level. We could also want to make three different predictions. Here we have three different hypotheses, and predict there will be an effect on dv1, dv2, and dv3. The criterion for each t-test is the same, but we now have three hypotheses to check (H1, H2, and H3). Each of these claims can be corroborated, or not.</p>

<p>You don’t always need to control the FWER. Limiting the number of false positives among all discoveries is good enough most of the time. There is a name for this rate, and it’s called the false discovery rate. In maths you can often define yourself out of a problem, and the false discovery rate is one such move. The false discovery rate (FDR) is the average number of false positives in all positive tests. Any adjustment scheme for the FDR is more forgiving than a scheme for the FWER. This comes at the cost of more false positives overall.</p>

<p>The Benjamini-Hochberg procedure is a common method used to control the FDR. It calculates a sequence of increasing adjusted p-values for each different hypothesis. Unlike the Bonferroni correction it is data-dependent. So the outcome cannot be known in advance.</p>]]></content><author><name></name></author><category term="GAMs" /><summary type="html"><![CDATA[Jerzy Neyman and Egon Pearson developed their idea of hypothesis testing in 1933. This has since become the dominant paradigm of statistical inference. The goal of Neyman and Pearson’s method were statistical tests which control errors. So scientist’s could adopt claims without being wrong too often. A single number, alpha, controls the long run amount of wrong decisions. It is set before a scientist conducts any experiment. One corollary of this is that the scientific literature contains errors. Be careful about trusting scientific claims.]]></summary></entry><entry><title type="html">Bayesian Sequential Hypothesis Testing</title><link href="http://gregorygundersen.com/blog/2024/02/04/bayesian-sequential-hypothesis-testing/" rel="alternate" type="text/html" title="Bayesian Sequential Hypothesis Testing" /><published>2024-02-04T00:00:00+00:00</published><updated>2024-02-04T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/02/04/bayesian-sequential-hypothesis-testing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/02/04/bayesian-sequential-hypothesis-testing/"><![CDATA[<p>We show how mSPRT can be thought of as a Bayesian Algorithm.</p>

<h3 id="a-bayesian-model">A Bayesian Model</h3>
<p>Consider the following Bayesian model for the data:
\(Y_1|\alpha, \delta, M_1 \sim \mathcal{N}\left(\frac{1}{n_1} \alpha + \frac{1}{2n_1} \delta, \sigma^2 I_{n_1}\right)\)</p>

\[Y_2|\alpha, \delta, M_1 \sim \mathcal{N}\left(\frac{1}{n_2} \alpha - \frac{1}{2n_2} \delta, \sigma^2 I_{n_2}\right)\]

\[\delta|M_1 \sim \mathcal{N} (0, \tau^2)\]

\[p(\alpha|M_1) \propto 1\]

<p>Note that the prior distribution for the lift \(\delta\) is equal to the mixing distribution in the mSPRT. If you are not familiar with this vectorized notation, an alternative notation is</p>

<table>
  <tbody>
    <tr>
      <td>$$y_{1i}</td>
      <td>\alpha, \delta, M_1 \sim \mathcal{N} (\alpha + \frac{\delta}{2}, \sigma^2) \text{ } \forall i = 1, \ldots, n_1$$</td>
    </tr>
    <tr>
      <td>$$y_{2j}</td>
      <td>\alpha, \delta, M_1 \sim \mathcal{N} (\alpha - \frac{\delta}{2}, \sigma^2) \text{ } \forall j = 1, \ldots, n_2$$</td>
    </tr>
  </tbody>
</table>

<p>In this model for the “alternative” we have a grand mean \(\alpha\) and a lift parameter \(\delta\).</p>

<p>Following a Bayesian approach, we assign \(\alpha\) a uniform prior, and \(\delta\) a Normal prior. Under this model, the interpretation of \(\delta\) is still unchanged as \(E[y_1]-E[y_2]\) i.e., the expected difference in means. Moreover, it also follows under this model that \(\bar{Y}_1 - \bar{Y}_2 \sim \mathcal{N} (\delta, \sigma^2(\frac{1}{n_1} + \frac{1}{n_2}))\). Notice, however, that we have conditioned on this model being the alternative model \(M_1\). The “null” model can be expressed as</p>

<table>
  <tbody>
    <tr>
      <td>$$Y_1</td>
      <td>\alpha, M_0 \sim \mathcal{N} (\frac{1}{n_1} \alpha, \sigma^2 I_{n_1})$$</td>
    </tr>
    <tr>
      <td>$$Y_2</td>
      <td>\alpha, M_0 \sim \mathcal{N} (\frac{1}{n_2} \alpha, \sigma^2 I_{n_2})$$</td>
    </tr>
    <tr>
      <td>$$p(\alpha</td>
      <td>M_0) \propto 1$</td>
    </tr>
  </tbody>
</table>

<p>in which there is no \(\delta\) i.e., it is the same as the alternative model except that our prior on \(\delta\) concentrates all of its mass at zero. A Bayesian would test the hypothesis that \(\delta \neq 0\) by looking at the Bayes Factor between models \(M_1\) and \(M_0\)</p>

\[\frac{p(Y_1, Y_2|M_1)}{p(Y_1, Y_2|M_0)} = \frac{\int p(Y_1, Y_2|\alpha, \delta)d\alpha d\delta}{\int p(Y_1, Y_2|\alpha)d\alpha},\]

<p>note that we are explicitly integrating out the nuisance intercept parameter \(\alpha\) w.r.t. a uniform prior. Let’s go ahead and compute these integrals!</p>

<p>To make the mathematics easier, it is helpful to stack the observations \(Y_1, Y_2\) into a larger multivariate normal:</p>

\[Y = [Y_1', Y_2']' \sim \mathcal{N} (1_n\alpha + X\delta, \sigma^2I_n)\]

<p>where \(n = n_1 + n_2\), \(1_n\) is a vector of 1’s, \(I_n\) is an identity matrix and \(X' = \frac{1}{2}[1_{n_1}', -1_{n_2}']\).</p>

<p>The first trick is to marginalize out the intercept parameter by computing the component of \(Y\) that is orthogonal to the column space of \(1_n\). Dropping the \(n\)’s from now on, let \(P_1 = \frac{1}{n}1(1'1)^{-1}1' = \frac{1}{n} 11'\) be the projection operator onto the column space of \(1\). This neatly isolates the component of \(\alpha\) in the quadratic form i.e.</p>

\[\begin{aligned}
\|Y - 1\alpha - X\delta\|^2_2 &amp;= (Y - 1\alpha - X\delta)'(Y - 1\alpha - X\delta) \\
&amp;= (Y - 1\alpha - X\delta)'(P_1 + I - P_1)(Y - 1\alpha - X\delta) \\
&amp;= \|P_1(Y - 1\alpha - X\delta)\|^2_2 + \|(I - P_1)(Y - 1\alpha - X\delta)\|^2_2 \\
&amp;= n(\alpha - \bar{Y} - \bar{X}\delta)^2 + \|Y_c - X_c\delta\|^2_2
\end{aligned}\]

<p>where \(Y_c\) and \(X_c\) are the centered observations and design matrix</p>

\[Y_c = (I - P_1)Y = Y - 1 \bar{Y}\]

\[X_c = (I - P_1)X = \frac{1}{2}
\begin{bmatrix}
1_{n_1} \\
-1_{n_2}
\end{bmatrix}
- \frac{1}{2}
\frac{n_1 - n_2}{n_1 + n_2}
\begin{bmatrix}
1_{n_1} \\
1_{n_2}
\end{bmatrix}
= \frac{1}{n_1 + n_2}
\begin{bmatrix}
n_2 1_{n_1} \\
-n_1 1_{n_2}
\end{bmatrix}.\]

<p>As a foreshadowing of a result later it is also suggestive to note at this point that</p>

\[X_c'X_c = \frac{n_1n_2}{n_1 + n_2} = \frac{1}{\frac{1}{n_1} + \frac{1}{n_2}}.\]

<p>This has rearranged the quadratic form in the likelihood into two terms, one a quadratic in \(\alpha\), which makes the integration step easy.</p>

\[p(Y_1, Y_2|\delta, M_1) = \frac{1}{n} \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n-1}{2}} \exp\left(-\frac{1}{2\sigma^2} \|Y_c - X_c\delta\|^2\right)\]

\[p(Y_1, Y_2|M_0) = \frac{1}{n} \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n-1}{2}} \exp\left(-\frac{1}{2\sigma^2} \|Y_c\|^2\right)\]

<p>Integrating out the intercept from the model had the effect of losing a degree of freedom and working with the centered observations and design matrix. The final integration is the marginalization step of \(\delta\) w.r.t. the prior \(N (0, \tau^2)\):</p>

\[p(Y_1, Y_2|M_1) = \frac{1}{n} \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n-1}{2}} \exp\left(-\frac{1}{2\sigma^2} \|Y_c\|^2\right) \left(\frac{1}{2\pi\tau^2}\right)^{\frac{1}{2}} \exp\left(\frac{1}{2} \left(X_c'X_c\sigma^2 + 1\tau^2\right)^{-1} (X_c'Y_c\sigma^2)^2\right)\]

<p>When forming the Bayes factor, most of these terms cancel:</p>

\[\frac{p(Y_1, Y_2|M_1)}{p(Y_1, Y_2|M_0)} = \left(\frac{1}{\tau^2} \frac{1}{\tau^2 + X_c'X_c\sigma^2}\right)^{\frac{1}{2}} \exp\left(\frac{1}{2} \left(X_c'X_c\sigma^2 + 1\tau^2\right)^{-1} (X_c'Y_c\sigma^2)^2\right),\]

<p>yet \(X_c'X_c/\sigma^2 = 1/(\sigma^2(\frac{1}{n_1} + \frac{1}{n_2}))\) which is \(1/\rho^2\) under our earlier definition of \(\rho^2\) and</p>

\[X_c'Y_c =X_c'Y_c + X_c'1 \bar{Y} = X_c'Y = \frac{n_2n_1}{n_1 + n_2} (\bar{Y}_1 - \bar{Y}_2) = \frac{\bar{Y}_1 - \bar{Y}_2}{\frac{1}{n_1} + \frac{1}{n_2}}.\]

<p>The Bayes factor then simplifies to
\(\frac{p(Y_1, Y_2|M_1)}{p(Y_1, Y_2|M_0)} = \left(\frac{\rho^2}{\rho^2 + \tau^2}\right)^{\frac{1}{2}} \exp\left(\frac{1}{2} \frac{\tau^2}{\rho^2 + \tau^2} \frac{(\bar{Y}_1 - \bar{Y}_2)^2}{\rho^2}\right)
=\Lambda\)</p>]]></content><author><name></name></author><category term="hypothesis-testing" /><summary type="html"><![CDATA[We show how mSPRT can be thought of as a Bayesian Algorithm.]]></summary></entry><entry><title type="html">Sequential Hypothesis Testing</title><link href="http://gregorygundersen.com/blog/2024/02/02/sequential-hypothesis-testing/" rel="alternate" type="text/html" title="Sequential Hypothesis Testing" /><published>2024-02-02T00:00:00+00:00</published><updated>2024-02-02T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/02/02/sequential-hypothesis-testing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/02/02/sequential-hypothesis-testing/"><![CDATA[<p>We introduce the mSPRT and give a derivation from a Bayesian point of view.</p>

<h1 id="sequential-testing">Sequential Testing</h1>

<p>Normally when you go about hypothesis testing, after a random sample is observed one of two possible actions are taken: accept the null hypothesis \(H_0\), or accepct the tive hypothesis \(H_1\). In some cases the evidence may strongly support one of the hypotheses, whilst in other cases the evidence may be less convincing. Nevertheless, a decision must he made. All this assumes that all the data has been collected, and no more is available. It doesn’t have to be this way. There is a class of hypothesis tests where you can safely collect more data when the evidence is ambiguous. Such a test typically continues until the evidence strongly favors one of the two hypotheses.</p>

<p>In case of simple hypotheses the strength of the evidence for \(H\) is given by the ratio of the probability of the data under \(H_0\), to the probability of the data under \(H_1\). We denote this likelihood ratio by \(\Lambda\). The Neyman-Pearson lemma implies that for a given amount of information the likelihood ratio test is the most powerful test. Such a rule decides to accept \(H_1\) if \(\Lambda\) is big enough, and decides to accept \(H_0\) otherwise. How big \(\Lambda\) must get to lead to the decicion \(H_1\), danends on, amond other things, its sampling distribution under \(H_0\) and \(H_1\). It is not unusual for “big enough” to mean \(\Lambda \geq 1\). Such tests could easily decide \(H_0\) or \(H_1\) when the actual evidence is neutral.</p>

<h2 id="sprt">SPRT</h2>

<p>In 1943 Wald proposed the <em>sequential probability ratio test</em> (SPRT). Suppose that \(Y\) is a random variable with unknown distribution \(f\). We want to test the following hypotheses:</p>

<ul>
  <li>
\[H_0: f=f_0\]
  </li>
  <li>
\[H_1: f=f_1\]
  </li>
</ul>

<p>where \(f_0\) and \(f_1\) are specified. We observe values of \(Y\) successively: \(y_1, y_2, \ldots\) the random variables \(Y_i\) corresponding to the \(y_i\) are i.i.d with common distribution \(f\). Let</p>

\[\Lambda = \prod_{i=1}^n \frac{f_1\left(x_i\right)}{f_0\left(x_i\right)}\]

<p>be the likelihood ratio at stage \(n\). We choose two decision boundaries \(A\) and \(B\) such that \(0 &lt; B &lt; A &lt; \infty\),  we accept \(H_0\) if \(\Lambda \leq B\) and \(H_1\) if \(\Lambda \geq A\), and we continue if \(B \leq \Lambda \leq A\). The constants \(A\) and \(B\) are determined by the desired false positive and false negative rates of the experimenter. In general it can  be shown that the boundaries A and B can be calculated as with very good approximation as</p>

\[A=\log\left(\frac{\beta}{1=\alpha}\right)\]

\[B=\log\left(\frac{1-\beta}{\alpha}\right)\]

<p>so the SPRT is really very simple to apply in practice.</p>

<h3 id="example-iid-case">Example: IID case</h3>

<p>Assume that \(X_1 , X_2 \ldots\) are independent and identically distributed with distributions \(P\), and \(Q\), under \(H_0\) and \(H_1\) respectively. Then \(L_n =\Lambda\left(X_n\right)\). Let \(Z_i=\log{\left(\Lambda\left(X_n\right)\right)}\). Since \(\log{L_n} = \sum_i^n Z_i\) the SPRT can be viewed as z random walk (or more properly a family of random walks), with steps \(Z_i\) which proceeds until it crosses \(\log{B}\) or \(\log{A}\).</p>

<p>We now focus on the more general case where \(P\) and \(Q\), have either densities or probability mass functions of the form:</p>

\[f\left(x; \theta\right) = h\left(x\right)\exp{C\left(\theta\right)x - D\left(\theta\right)}\]

<p>where \(\theta\) is a real valued parameter. The <em>exponential families</em> are to work with and include many common distributions. Let \(P\), be determined by \(f\left(x; \theta_0\right)\), and let \(Q\), be determined by \(f\left(x; \theta_1\right)\). Then 
\(Z_i = \left[C\left(\theta_1\right) - C\left(\theta_0\right) \right]X_i - \left[D\left(\theta_1\right)-D\left(\theta_0\right)\right]\).</p>

<p>In terms of the random walk with steps \(Z_i\) the SPRT continues until fixed boundaries are crossed.</p>

<p>It is somtimes easier to perform the test using the sums of the \(X_i\)’s. Let \(S_n = \sum X_i\). Assuming that \(C\left(\theta_1\right) &gt; C\left(\theta_0\right)\) the test continues until</p>

\[S_n \geq \frac{ \log{A} }{ C\left(\theta_1\right) - C\left(\theta_0\right) } + n\frac{ D\left(\theta_1\right)-D\left(\theta_0\right) }{ C\left(\theta_1\right) - C\left(\theta_0\right) }\]

<p>Or</p>

\[S_n \leq \frac{ \log{A} }{ C\left(\theta_1\right) - C\left(\theta_0\right) } + n\frac{ D\left(\theta_1\right)-D\left(\theta_0\right) }{ C\left(\theta_1\right) - C\left(\theta_0\right) }\]

<p>The following tableshow the SPRT for some common distributions:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Distribution</th>
      <th style="text-align: right">\(f\left(x; \theta\right)\)</th>
      <th style="text-align: right">\(C\left(\theta\right)\)</th>
      <th style="text-align: right">\(D\left(\theta\right)\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Normal</td>
      <td style="text-align: right">\(\frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta\right)^2/2\right)\)</td>
      <td style="text-align: right">\(\theta\)</td>
      <td style="text-align: right">\(\frac{\theta^2}{2}\)</td>
    </tr>
    <tr>
      <td style="text-align: left">Bernoulli</td>
      <td style="text-align: right">\(\theta^{x}\left(1-\theta\right)^{(1-x)}\)</td>
      <td style="text-align: right">\(\log{\left(\frac{\theta}{1-\theta}\right)}\)</td>
      <td style="text-align: right">\(-\log{\left(1-\theta\right)}\)</td>
    </tr>
    <tr>
      <td style="text-align: left">Exponential</td>
      <td style="text-align: right">\(\frac{1}{\theta}e^-x/\theta\)</td>
      <td style="text-align: right">\(\log{\theta}\)</td>
      <td style="text-align: right">\(\theta\)</td>
    </tr>
    <tr>
      <td style="text-align: left">Poisson</td>
      <td style="text-align: right">\(\frac{e^-\theta \theta^x}{x!}\)</td>
      <td style="text-align: right">\(-\frac{1}{\theta}\)</td>
      <td style="text-align: right">\(\log{\theta}\)</td>
    </tr>
  </tbody>
</table>

<h3 id="example-exponential-distribution">Example: Exponential Distribution</h3>

<p>A textbook example is parameter estimation of a probability distribution function. Consider the exponential distribution:</p>

\[f\left(x; \theta\right) = \frac{1}{\theta}e^-x/\theta\]

<p>The hypotheses are</p>

<ul>
  <li>
\[H_0: \theta = \theta_0\]
  </li>
  <li>
\[H_1: \theta = \theta_1\]
  </li>
</ul>

<p>Where \(\theta_1 &gt; \theta_1\).</p>

<p>Then the log-likelihood function (LLF) for one sample is</p>

\[\begin{aligned}
\log{\Lambda\left(x\right)} &amp;= \log{ \frac{1}{\theta_1}e^-x/\theta_1  }{ \frac{1}{\theta_0}e^-x/\theta_0  } 
&amp;= -\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}x
\end{aligned}\]

<p>The cumulative sum of the LLFs for all x is</p>

\[S_n = \sum_n \log{\Lambda\left(x_i\right)} = -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i\]

<p>Accordingly, the stopping rule is:</p>

\[a &lt;  -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b\]

<p>After re-arranging we finally find</p>

\[a + n\log{\frac{\theta_1}{\theta_0}} &lt; \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b + n\log{\frac{\theta_1}{\theta_0}}\]

<p>The thresholds are simply two parallel lines with slope \(\log{\frac{\theta_1}{\theta_0}}\). Sampling should stop when the sum of the samples makes an excursion outside the continue-sampling region.</p>

<h3 id="example-binomial-distribution">Example: Binomial Distribution</h3>

<h3 id="1-likelihood-ratio-for-bernoulli-distribution">1. Likelihood Ratio for Bernoulli Distribution</h3>

<p>For a Bernoulli trial, \(X \in \{0, 1\}\) with probability \(\theta\) for success (\(\theta = 1\)) and \(1 - \theta\) for failure (\(\theta = 0\)).</p>

<ul>
  <li>
    <p>Under hypothesis \(H_1\) with parameter \(\theta_1\):<br />
\(P(X = x | \theta_1) = \theta_1^x (1 - \theta_1)^{1 - x}\)</p>
  </li>
  <li>
    <p>Under hypothesis \(H_0\) with parameter \(\theta_0\):<br />
\(P(X = x | \theta_0) = \theta_0^x (1 - \theta_0)^{1 - x}\)</p>
  </li>
</ul>

<p>The likelihood ratio is:
\(\Lambda(x) = \frac{P(X = x | \theta_1)}{P(X = x | \theta_0)} = \frac{\theta_1^x (1 - \theta_1)^{1 - x}}{\theta_0^x (1 - \theta_0)^{1 - x}}\)</p>

<h3 id="2-log-likelihood-ratio">2. Log-Likelihood Ratio</h3>

<p>The log-likelihood ratio is:
\(\log \Lambda(x) = \log \left( \frac{\theta_1^x (1 - \theta_1)^{1 - x}}{\theta_0^x (1 - \theta_0)^{1 - x}} \right)\)</p>

<p>Breaking this into cases for \(x = 0\) and \(x = 1\):</p>

<p>If \(x = 1\):</p>

\[\log \Lambda(1) = \log \frac{\theta_1}{\theta_0}\]

<p>If \(x = 0\):</p>

\[\log \Lambda(0) = \log \frac{1 - \theta_1}{1 - \theta_0}\]

<p>Thus, the general form for \(x\) is:</p>

\[\log \Lambda(x) = x \log \frac{\theta_1}{\theta_0} + (1 - x) \log \frac{1 - \theta_1}{1 - \theta_0}\]

<h3 id="3-cumulative-log-likelihood-for-n-observations">3. Cumulative Log-Likelihood for \(n\) Observations</h3>

<p>Now we calculate the cumulative log-likelihood ratio for ( n ) independent observations ( x_1, x_2, \ldots, x_n ):
\(S_n = \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i + \log \frac{1 - \theta_1}{1 - \theta_0} \sum_{i=1}^{n} (1 - x_i)\)</p>

<p>This simplifies to:
[
S_n = \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i + \log \frac{1 - \theta_1}{1 - \theta_0} \sum_{i=1}^{n} (1 - x_i)
]</p>

<h3 id="4-stopping-rule">4. Stopping Rule</h3>

<p>The stopping rule in this context is when the cumulative sum ( S_n ) crosses specific thresholds. The inequality becomes:
\(a &lt; \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i + \log \frac{1 - \theta_1}{1 - \theta_0} \sum_{i=1}^{n} (1 - x_i) &lt; b\)</p>

<p>Rearranging this gives:
\(a - n \log \frac{1 - \theta_1}{1 - \theta_0} &lt; \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i &lt; b - n \log \frac{1 - \theta_1}{1 - \theta_0}\)</p>

<h3 id="5-interpretation">5. Interpretation</h3>

<p>In this case, the thresholds are two parallel lines with slopes based on the log-ratios \(\log \frac{\theta_1}{\theta_0}\) and \(\log \frac{1 - \theta_1}{1 - \theta_0}\). The sampling process stops when the sum of the observed successes \(\sum x_i\) exits the continuation region bounded by these two lines.</p>

<p>We can summarise this in the following algorithm:</p>

<p>Set \(LR \leftarrow 1\) and \(j \leftarrow 0\)</p>

<ol>
  <li>Increment \(j\)</li>
  <li>If \(𝑋_j=1\) then set \(LR \leftarrow LR \frac{\theta_1}{\theta_0}\)</li>
  <li>If \(𝑋_j=0\) then set \(LR \leftarrow LR \frac{(1−\theta_1)}{(1−\theta_0)}\)</li>
</ol>

<p>What’s \(LR\) at stage 𝑚? Let \(T_m≡ \sum_{j=1}^m 𝑋_j\) then:</p>

\[\frac{\theta_{1𝑚}}{\theta_{0m}} = \frac{\theta_1^{𝑇_𝑚}(1−\theta_1)^{𝑚−𝑇_𝑚}}{\theta_0^{𝑇_𝑚}(1−\theta_0)^{𝑚−𝑇_𝑚}}\]

<p>This is the ratio of binomial probability when \(\theta=\theta_1\) to binomial probability when \(\theta=\theta_0\) (the binomial coefficients in the numerator and denominator cancel). It simplifies further to</p>

\[\frac{\theta_{1𝑚}}{\theta_{0𝑚}}=\left(\frac{(\theta_0}{\theta_1}\right)^{𝑇_m}\left(\frac{1−\theta_0}/{1−\theta_1})\right)^{𝑚−𝑇_𝑚}\]

<p>We conclude \(\theta &gt; \theta_0\) if</p>

\[\frac{\theta_1m}{\theta_0m} \geq \frac{1-\beta}{\alpha}\]

<p>And we conclude \(\theta &lt; \theta_0\) if</p>

\[\frac{\theta_1m}{\theta_0m} \geq \frac{\beta}{1-\alpha}\]

<p>Otherwise, we draw again.</p>

<p>The SPRT approximately minimizes the expected sample size when \(\theta &gt; \theta_0\) or \(\theta &gt; \theta_1\). For values in \(\left(\theta_1,\theta_0\right)\), it can have larger sample sizes than fixed-sample-size tests.</p>

<h3 id="example-normal-distribution">Example: Normal Distribution</h3>

<p>Consider the Normal distribution:</p>

\[f\left(x; \theta\right) = \frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta\right)^2/2\right)\]

<p>The hypotheses are</p>

<ul>
  <li>
\[H_0: \theta = \theta_0\]
  </li>
  <li>
\[H_1: \theta = \theta_1\]
  </li>
</ul>

<p>Where \(\theta_1 &gt; \theta_1\).</p>

<p>Then the log-likelihood function (LLF) for one sample is</p>

\[\begin{aligned}
\log{\Lambda\left(x\right)} &amp;= \log{ \frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta_1\right)^2/2\right)  }{ \\frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta_0\right)^2/2\right)  } 
&amp;= -\frac{1}{2}\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{2\theta_1\theta_0}x_i^2 - \frac{\theta_1 - \theta_0}{2}
\end{aligned}\]

<p>The cumulative sum of the LLFs for all x is</p>

\[S_n = \sum_n \log{\Lambda\left(x_i\right)} = -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i\]

<p>Accordingly, the stopping rule is:</p>

\[a &lt;  -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b\]

<p>After re-arranging we finally find</p>

\[a + n\log{\frac{\theta_1}{\theta_0}} &lt; \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b + n\log{\frac{\theta_1}{\theta_0}}\]

<p>The thresholds are simply two parallel lines with slope \(\log{\frac{\theta_1}{\theta_0}}\). Sampling should stop when the sum of the samples makes an excursion outside the continue-sampling region.</p>

<h2 id="the-mixture-sequential-probability-ratio-test-msprt">The Mixture Sequential Probability Ratio Test (mSPRT)</h2>

<p>The main limitation of the Sequential Probability Ratio Test (SPRT) is its requirement for specifying an explicit alternative hypothesis, which might not always align with the goal of merely rejecting a null hypothesis. In contrast, the modified Sequential Probability Ratio Test (mSPRT) offers flexibility in determining sample sizes at the start of an experiment, unlike the traditional SPRT, where sample size calculations are not feasible. Consequently, parameters such as \(N\), \(\alpha\), and \(\beta\) are fixed at the outset of an experiment.</p>

<p>The mixture Sequential Probability Ratio Test (mSPRT) amends these limitations. The test is defined by a “mixing” distribution \(H\) over a parameter space \(\Theta\), where \(H\) is assumed to have a density \(h\) that is positive everywhere. Utilizing \(H\), we first compute the following mixture of likelihood ratios against the null hypothesis \(\theta = \theta_0\):</p>

\[\Lambda_n(s_n) = \int_{\Theta} \left(\frac{f_{\theta}(s_n)}{f_{\theta_0}(s_n)}\right)^n dH(\theta).\]

<p>The mSPRT is parameterized by a mixing distribution \(H\) over \(\Theta\), which is restricted to have an everywhere continuous and positive derivative. Given an observed sample average \(s_n\) up to time \(n\), the likelihood ratio of \(\theta\) against \(\theta_0\) is \(\left(\frac{f_{\theta}(s_n)}{f_{\theta_0}(s_n)}\right)^n\). Thus, we define the mixture likelihood ratio with respect to \(H\) as:</p>

\[\Lambda_n(s_n) = \int_{\Theta} \left(\frac{f_{\theta}(s_n)}{f_{\theta_0}(s_n)}\right)^n dH(\theta).\]

<h3 id="application-to-normal-data">Application to Normal Data</h3>

<p>Considering normal data, for any \(\mu_A\) and \(\mu_B\), the difference \(Z_n = Y_n - X_n\) follows a normal distribution \(\sim N(\theta, 2\sigma^2)\). We can apply the one-variation mSPRT to the sequence \(\{Z_n\}\), leading to the following definition:</p>

\[\sqrt{\frac{2\sigma^2}{2\sigma^2 + n\tau^2}} \exp{\left(\frac{n^2\tau^2\left(X_n - Y_n - \theta_0\right)^2}{4\sigma^2\left(2\sigma^2 + n\tau^2\right)}\right)}.\]

<p>Here’s a small simulation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_mSPRT</span><span class="p">(</span><span class="n">y_data</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">sigma_squared</span><span class="p">,</span> <span class="n">tau_squared</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
    <span class="n">lambda_n</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tau_squared</span><span class="p">))</span> <span class="o">*</span> \
               <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tau_squared</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_mean</span> <span class="o">-</span> <span class="n">x_mean</span> <span class="o">-</span> <span class="n">theta_0</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tau_squared</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">lambda_n</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Parameters for synthetic data generation
</span><span class="n">mu_X</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mu_Y</span> <span class="o">=</span> <span class="mi">3</span>  
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>  
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>  

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_X</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sigma_squared</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">tau_squared</span> <span class="o">=</span> <span class="mi">1</span>  
<span class="n">theta_0</span> <span class="o">=</span> <span class="mi">0</span>  

<span class="n">msprt_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">lambda_n</span> <span class="o">=</span> <span class="n">compute_mSPRT</span><span class="p">(</span><span class="n">y_data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">sigma_squared</span><span class="p">,</span> <span class="n">tau_squared</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">)</span>
    <span class="n">msprt_values</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambda_n</span><span class="p">)</span>

<span class="n">p_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lambda_val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">msprt_values</span><span class="p">):</span>
    <span class="n">p_values</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">p_values</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_val</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Always Valid p-values'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of Observations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'p-value'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Always Valid p-values Over Time'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">msprt_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'likelihood'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of Observations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'likelihood'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Likelihood Over Time'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="hypothesis-testing" /><summary type="html"><![CDATA[We introduce the mSPRT and give a derivation from a Bayesian point of view.]]></summary></entry><entry><title type="html">What p-values really mean</title><link href="http://gregorygundersen.com/blog/2024/01/28/what-p-values-mean/" rel="alternate" type="text/html" title="What p-values really mean" /><published>2024-01-28T00:00:00+00:00</published><updated>2024-01-28T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/01/28/what-p-values-mean</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/01/28/what-p-values-mean/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In the branch of statistics known as hypothesis testing,  a statistical hypothesis is a claim about the value of a parameter in a statistical model: for example  the mean difference in revenue from two random groups of customers visiting a website is exactly zero. In statistics we call the hypothesis you want to reject the null hypothesis. A p-value is a summary of your data; it tells you how likely you are to see data at least as extreme as the data you have collected when your null hypothesis is correct.</p>

<p>A statistical hypothesis test is an algorithm used to calculate a test statistic and a p-value. A large p-value just means that the data was likely to occur under your hypothesis. Whereas a small value means the data was unlikely to have occurred under your hypothesis. This means that p-values are better at rejecting bad hypotheses than confirming good ones.</p>

<p>The above discussion is mostly concerned with how well the data fits the model, and not whether the hypotheses in question are true. Statistics alone cannot answer such questions. Because the data you collect can be consistent with many similar hypotheses, statistics can only tell you which hypothesis could best have generated the data. Not whether the hypotheses were true. Data analysis alone cannot tell you whether the government program was a success, whether the drug should be prescribed to patients, or whether the website really is better with this new design. Statistics can tell you how many people were saved from poverty, how effective the drug is, or whether more people buy your products after a website redesign.</p>

<p>This might be why p-values are the most misunderstood, misinterpreted, and occasionally miscalculated of statistical quantities. No matter how hard statisticians try, what the p-value really means hasn’t broken through to the wider scientific consciousness. In fact, p-values are so difficult to interpret that when the Journal of the American Medical Association surveyed its members in 2007 about how to interpret p-values, none of the available options were correct <a class="citation" href="#windish2007medicine">(Windish et al., 2007)</a>.</p>

<p>Applied researchers are certainly not to blame for misinterpreting p-values: p-values are, after all,  the problem child of statistics—a quantity straightforward to calculate but philosophically and practically difficult to interpret. That being said, we still conduct experiments so we still have to interpret their outcomes—that means we have to interpret p-values.</p>

<h2 id="how-to-interpret-experiment-results-this-example-was-supplied-by-httpswwwscribbrcomstatisticsp-value">How to Interpret Experiment Results (‘This example was supplied by: https://www.scribbr.com/statistics/p-value/’)</h2>

<p>Let’s say you want to know whether there’s a difference in longevity between two groups of mice fed on different diets, diet A and diet B. You can statistically test the difference between these two diets using a two-tailed t-test. We consider the following hypotheses:</p>

<ul>
  <li><strong>Null hypothesis (H0)</strong>: there is no difference in longevity between the two groups.</li>
  <li><strong>Alternative hypothesis (H1)</strong>: there is a difference in longevity between the two groups.</li>
</ul>

<p>If the mice live equally long on either diet, then your test statistic will closely match the test statistic from the null hypothesis (that there is no difference between groups). The resulting p-value could be anything between 0 and 1. However, if there is an average difference in longevity between the two groups, then your test statistic will move further away from the values predicted by the null hypothesis, and the p-value will get smaller. The p-value will never reach zero, because there’s always a possibility that the null hypothesis could generate the data you have seen.</p>

<p>You run the experiment: you randomise the mice into different groups and they receive either diet A or diet B. You find that the lifespan on diet A (M = 2.1 years; SD = 0.12) was shorter than the lifespan on diet B (M = 2.6 years; SD = 0.1), with an average difference of 6 months (t(80) = -12.75; p &lt; 0.01). Your comparison of the two diets results in a p-value of less than 0.01 below your alpha value of 0.05; therefore, you determine that there’s a statistically significant difference between the two diets.</p>

<p>But we should be cautious here. The reported p-value means that there is a 1% chance that these data would occur under the null hypothesis. This is not the same thing as saying that the null hypothesis is not true. It just means that you can reject the hypothesis that both diets produce similar outcomes. Building up a fuller picture would require knowledge about the specific content of the diets. So we can’t rely on the p-value alone to tell us much about how diet affects lifespan.</p>

<p>One of the biggest mistakes in statistics is to over-interpret your p-values. Deciding a policy based on the criteria that p &lt; 0.05, thereby rejecting the null hypothesis , is tantamount to saying that the effect is irrelevant. It puts the cart before the horse. Remember you did the experiment in the first place to find out something else: to measure the magnitude and direction of the effect size.</p>

<p>As an expert you will need to combine the formal results of the experiment with your domain knowledge and come to a conclusion. Sometimes the conclusion might be that the null hypothesis holds true, despite a significant result; and sometimes the treatment works despite the test statistic disagreeing. Beliefs and actions cannot come directly from statistical results, especially results from a single experiment.</p>

<p>Hypothesis tests aren’t the only form of evidence out there, and you will have to gather many different strands to make a decision. Deciding upfront that if your p-value is  &lt; 0.05 then your research program is a success, and a failure otherwise, is unhealthy. It leads researchers to focus their actions on the outcomes at the expense of the process. Only healthy processes will create good results.</p>

<h1 id="references">References</h1>
<ol class="bibliography"><li><span id="windish2007medicine">Windish, D. M., Huot, S. J., &amp; Green, M. L. (2007). Medicine residents’ understanding of the biostatistics and results in the medical literature. <i>Jama</i>, <i>298</i>(9), 1010–1022.</span></li></ol>

<p>Bibliography:</p>

<p>‘Windish DM, Huot SJ, Green ML: Medicine residents’ understanding of the biostatistics and results in the medical literature. JAMA 298:1010- 1022, 2007’</p>]]></content><author><name></name></author><category term="statistics," /><category term="hypothesis" /><category term="testing" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Running Conda environments from Jupyter Notebook.</title><link href="http://gregorygundersen.com/blog/2022/07/03/conda-and-jupyter-notebook/" rel="alternate" type="text/html" title="Running Conda environments from Jupyter Notebook." /><published>2022-07-03T00:00:00+00:00</published><updated>2022-07-03T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2022/07/03/conda-and-jupyter-notebook</id><content type="html" xml:base="http://gregorygundersen.com/blog/2022/07/03/conda-and-jupyter-notebook/"><![CDATA[<p>This post is mostly take from <a href="https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084">here</a> as an aide-memoir and to prevent</p>]]></content><author><name></name></author><category term="optimisation" /><summary type="html"><![CDATA[This post is mostly take from here as an aide-memoir and to prevent]]></summary></entry><entry><title type="html">An introduction to Gradient Descent</title><link href="http://gregorygundersen.com/blog/2021/10/10/gradient-descent/" rel="alternate" type="text/html" title="An introduction to Gradient Descent" /><published>2021-10-10T00:00:00+00:00</published><updated>2021-10-10T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2021/10/10/gradient-descent</id><content type="html" xml:base="http://gregorygundersen.com/blog/2021/10/10/gradient-descent/"><![CDATA[<p>Gradient Descent is the simplest learning algorithm. It is very easy to implement, is robust to complex cost functions, and is the foundation of an enormous zoo of variations.</p>

<h2 id="introduction">Introduction</h2>

<p>Some ideas are so ubiquitous and all-encompassing that it’s difficult to remember that they were once invented. Other ideas are so simple and apparently obvious that they seem less invented and more handed down by God. Gradient descent is one such idea. It there is hardly a branch of applied mathematics which doesn’t use it, or not it seems to have appeared fully formed as an algorithmic idea for both the primordial mathematical time.</p>

<p>It’s worth noting how Agoras trusted an idea, gradient descent is is it allows the researcher to numerically compute an optimal answer even when they can’t analytically describe the function over which they are optimizing. Seen further, all that is required is some measurement of how our solution does as we form the optimal solution, and that the function we are optimizing over be smooth over our domain of interest.</p>

<p>It’s seems too good to be true! Yet the core issue in many applications such as signal processing, operations research, economics, and statistics boil down to the following minimization problem</p>

\[\mathrm{min}_{\theta \in \mathcal{R^m}} J\left(\theta\right)\]

<p>where \(J\left(\theta\right)\) is what is called the <em>cost function,</em> which measures how well the model parameters \(\theta\) fit to a given dataset. A few examples would be</p>

<ul>
  <li>Logistic regression</li>
</ul>

\[J\left(\theta\right) = \sum_{n} \log{\left(1 + \exp{y_i X_{i}^T\theta}\right)}\]

<ul>
  <li>Linear regression</li>
</ul>

\[J\left(\theta\right) = \frac{1}{2}\lvert\lvert X\theta - y \rvert\rvert^2\]

<ul>
  <li>Composite</li>
</ul>

\[J\left(\theta\right) = f\left(\theta\right) + g\left(\theta\right)\]

<p>with $ f $ being ‘well behaved’ and $ g $ causing us some trouble.</p>

<p>We assume that \(J\left(\theta\right)\) is bounded from below (i.e. the minimum is not \(-\infty\)). This just guarantees the existence of a solution. If $ J $ is convex this is no problem, but if $ J $ is not (and also not smooth) then it’s much more difficult. Always check that a solution exists before wasting time minimising!</p>

<p>There are a couple of ways you could find such an $ \theta $, given a cost function. The most straigtforward is to start with some initital value, and then move in the direction of the negative gradient of the cost funtion:</p>

\[\theta_{k+1} = \theta_{k} - \eta\nabla_{\theta} J\left(\theta\right)\]

<p>with $ \Theta_0 = 0 $. Here. $\eta$ is the learning rate - a tuneable parameter. We terminate the algorithm once</p>

\[\lvert \theta_{k+1} -  \theta_{k} \rvert \leq \varepsilon\]

<p>Intuitively, \(J\left(\theta\right)\) defines a surface over \(\theta\\). We, the end-users, want to find the lowest point on that surface (or the lowest point subject to some intersection constraint). Gradient descent finds that lowest point by constructing a sequence of approximations to \(\theta^*\) (the optimal point), with eath \(\theta_{k}\) (the interations of the algorithm) always in the direction of steepest descent from \(\theta_{k-1}\). We will formalise this intuition and provide performance guarantees later.</p>

<h2 id="linear-regression">Linear Regression</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span> 
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s">'retina'</span>
<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">([</span><span class="s">'seaborn-colorblind'</span><span class="p">,</span> <span class="s">'seaborn-darkgrid'</span><span class="p">])</span>
</code></pre></div></div>

<p>We’ll generate some data that we’ll use for the rest of this post:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">2.25</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">6.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.5</span><span class="p">):</span>
    <span class="s">"""Generate n data points approximating given line.
    m, b: line slope and intercept.
    stddev: standard deviation of added error.
    Returns pair x, y: arrays of length n.
    """</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Suppose you have a set of observations of some process you wanted to model, for example the size of a house labelled as \(x_i \in \mathrm{R}^n\), and the house price labeleld as \(y_i \in \mathrm{R}\), \(i = 1 \ldots m\) (i.e. you have \(m\) examples). One good choice for a model is linear:</p>

\[\hat{y}\left(x\right) = Ax + b\]

<p>The goal is to find some suitable \(A \in \mathcal{R}^n\) and \(b \in \mathcal{R}\), to model this process corectly.</p>

<p>For convenience, let’s make some new definitions:</p>

<p>\(\Theta = [A: b]\)
\(X = [x_1: \ldots : x_m :1]\)
\(Y = [y_1: \ldots : y_m :1]\)</p>

<p>What we’ve done her is to stack our parameters into \(\Theta\) so that it’s a \(\mathrm{R}^{\left(n+1\right)\times 1}\) vector, we have stacked our examples into an \(\mathrm{R}^{\left(n+1\right) \times m}\) matrix, and we have stacked all the outputs into a \(\mathrm{R}^{\left(n+1\right) \times 1}\) vector. Now our hypothesis can be written as</p>

<p>\(\hat{Y} = \Theta^T X\).</p>

<p>Say you also had good reason to believe that the best reconstruction of $$ x $ you could possilby hope to achieve was to minimise the following (mean squared) error measure:</p>

\[J\left(\Theta\right) = \frac{1}{n} \lVert \hat{Y} - Y\rVert_2^2\]

<p>A function to compute the cost is below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MSE</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
    <span class="s">"""Function to compute MSE between true values and estimate
    
    y: true values
    yhat: estimate
    """</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>There are a couple of ways you could find such an $4 \hat{Y} $$, given a cost function. The most straigtforward is to start with some initital value, and then move in the direction of the negative gradient of the cost funtion:</p>

\[\Theta_{k+1} = \Theta_{k} - \alpha\nabla_{\Theta} J\left(\Theta\right)\]

<p>with \(\Theta_0 = 0\). Here. \(\alpha\) is the learning rate - a tuneable parameter. We’ll have more to say about that later.</p>

<p>The following function does exactly this,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">250</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">theta</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
    <span class="k">yield</span> <span class="n">theta</span><span class="p">,</span> <span class="n">cost</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">yhat</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">theta</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
        <span class="n">yhatt</span> <span class="o">=</span> <span class="n">yhat</span><span class="p">.</span><span class="n">T</span>
        <span class="n">nabla</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">yhatt</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">nabla</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">theta</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">+=</span>  <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">nabla</span>
        <span class="k">yield</span> <span class="n">theta</span><span class="p">,</span> <span class="n">cost</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">yhat</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<p>Some notes on the algorithm: it may be mysterious as to why we divide the learning rate y the number of samples \(m\). But this is a way to average the gradient across all samples. This approach has several important implications:</p>

<p>Scaling of the Gradient: When you compute the gradient of the cost function, you’re essentially summing up the gradients calculated for each individual sample. If you don’t average this sum by dividing by \(m\), the magnitude of the gradient could become very large, especially with a large number of samples. This large gradient could lead to very large steps when updating your parameters, potentially causing overshooting and failure to converge to the minimum of the cost function.</p>

<p>Consistent Learning Rate By dividing by \(m\): you ensure that the learning rate behaves consistently regardless of the number of samples. This means that the learning rate you choose is less dependent on the size of your dataset. Without this scaling, you might need to adjust your learning rate based on the size of your dataset, which is not ideal.</p>

<p>Numerical Stability: Scaling by \(m\) also contributes to numerical stability. It prevents the gradient values from becoming too large, which can cause numerical issues in computation (like overflow or underflow problems).</p>

<p>Interpretability: It makes the learning rate parameter more interpretable and independent of the sample size. This is beneficial when you want to use the same learning rate for datasets of different sizes or when comparing the performance of the algorithm on different datasets.</p>

<p>Now we check graphically if the fitted parameters make sense:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">ones</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">MSE</span><span class="p">)</span>
<span class="n">final</span> <span class="o">=</span> <span class="p">[(</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">]</span>
<span class="n">costs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final</span><span class="p">]</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">final</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="costs-figure-goes-here">Costs figure goes here</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span>
    <span class="n">array</span><span class="p">([[</span> <span class="mf">2.27769915</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">5.90213934</span><span class="p">]])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s">"r-"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="fit-goes-here">Fit goes here.</h2>

<p>We could also seek to minimise the least absolute deviations of our predictions from the data:</p>

\[J\left(\Theta\right)  = \frac{1}{n} \lVert \hat{Y} - Y\rVert_1\]

<p>a function to do this is included below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MAE</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
    <span class="s">"""Function to compute LAE between true values and estimate
    
    y: true values
    yhat: estimate
    """</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>Finally, if you are minimising the MSE you can compute it analytically via the normal equations (left as an exercise to the reader):</p>

\[\hat{\theta} = (X^T X)^{-1} X^Ty\]

<p>We can do this with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normal_equation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ones</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">"""
    linear regression using the normal equation.
    X: Feature matrix
    y: Target variable array
    ones: Add a col of ones to Feature matrix
    Returns the calculated theta values.
    """</span>
    <span class="c1"># Adding a column of ones to X for the intercept term
</span>    <span class="k">if</span> <span class="n">ones</span><span class="p">:</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">ones</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute the coefficients
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">theta</span>
  
<span class="n">theta_analytic</span> <span class="o">=</span> <span class="n">normal_equation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">theta</span><span class="p">,</span> <span class="n">theta_analytic</span> 
</code></pre></div></div>

<p>You can derive this equation as follows:</p>

<ol>
  <li>Take the (vector) derivative of the cost function to find:</li>
</ol>

\[\nabla_{\theta} J\left(\theta\right) = X\left(X^T\theta - y\right)\]

<ol>
  <li>Set the derivative to zero and rearrange:</li>
</ol>

\[X\left(X^T\theta - y\right) = 0\]

\[X^TX \theta = X^T y\]

\[\theta = \left( X^TX  \right)^{-1} X^T y\]

<p>Intuitivelyy the product \(X^Ty\) is the projection of \(y\) onto the space spanned by the columns of \(X\). This operation translates the target values into the “language” of our features.</p>

<p>The matrix \(X^TX\) is a feature-feature correlation matrix. It’s symmetrical and captures how each feature relates to every other feature. Taking the inverse of this matrix is akin to understanding how to uniquely weigh each feature to best describe the target \(y\). The inverse undoes the mixing of feature influences, allowing us to isolate the effect of each feature.</p>

<p>When you multiply the inverse of \(X^TX\) with \(X^Ty\), you apply the unique weighting to the projection of \(y\) in feature space. This results in the coefficients \(\Theta\) that best map your features to your target in a least-squares sense.</p>

<p>Note that even though this has a nice interpretation calculating \(\left( X^TX  \right)^{-1}\) is prohibitively expensive (it’s on \(\mathcal{O}\left(n^3\right)\) operation). In practice gradient descent is always used.</p>

<h2 id="logistic-regression">Logistic Regression</h2>

<p>Logistic regression is used when we need our outputs to be strictly within the interval \([0, 1]\). It’s usually taught as a classification algorithm, but it’s best thought of as a regression which predictst <strong>probabilities</strong>. Logistic regression minimises the following loss function:</p>

\[J\left(w\right) = y * p\left( y_i | x_i ; w \right) + (1-y) * (1 - p\left( y_i | x_i ; w \right))\]

<p>There is a mathematical justification for why this is the right loss to use, but heuristically, this loss minimises the probability error between the predicition classes and the true classes.</p>

<p>The other major difference is that we choose the sigmoid function. $ \sigma\left(X\theta\right) $ with</p>

\[\sigma\left(z\right) = \frac{1}{1+e^{-z}}\]

<p>instead of \(X^t\theta\) as the hypothesis function. The sigmoid function maps any real-valued number into the range \((0, 1)\), making it useful for a probability estimate. Essentially what we are doing here with logistic regression is doing a linear regression but then transforming the outputs to probability space. We’ll predict the class of each point using softmax (multinomial logistic) regression. The model has a matrix \(W\) of weights, which measures for each feature how likely that feature is to be in a particular. It is of size \(\mathrm{n_{features}} \times \mathrm{n_{classes}}\). The goal of softmax regression is to learn such a matrix. Given a matrix of weights, \(W\), and matrix of points, \(X\), it predicts the probability od each class given the samples.</p>

<p>In code we can do it like this:</p>

<p>```python
	def sigmoid(z):
    return 1 / (1 + jnp.exp(-z))</p>

<p>def logistic_cost(theta, X, y):
    m = len(y)
    h = sigmoid(X @ theta)
    cost = -(1/m) * jnp.sum(y * jnp.log(h) + (1 - y) * jnp.log(1 - h))
    return cost</p>

<p>def logistic_regression(X, y, cost_fn, learning_rate=0.01, num_iters=1000):
    m, n = X.shape
    theta = np.zeros((n, 1))
    y = y[:, 0]
    y = y.reshape(-1, 1)
    cost_history = []</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i in range(num_iters):
    z = X @ theta
    h = sigmoid(z)
    gradient = (1/m) * X.T @ (h - y)
    theta -= learning_rate * gradient
    
    cost = cost_fn(theta, X, y)
    cost_history.append(cost)

return theta, cost_history
</code></pre></div></div>

<p>def make_blobs(num_samples=1000, num_features=2, num_classes=2):
    mu = np.random.rand(num_classes, num_features)
    sigma = np.ones((num_classes, num_features)) * 0.1
    samples_per_class = num_samples // num_classes
    x = np.zeros((num_samples, num_features))
    y = np.zeros((num_samples, num_classes))
    for i in range(num_classes):
        class_samples = np.random.normal(mu[i, :], sigma[i, :], (samples_per_class, num_features))
        x[i * samples_per_class:(i+1) * samples_per_class] = class_samples
        y[i * samples_per_class:(i+1) * samples_per_class, i] = 1
    return x, y</p>

<p>def plot_clusters(x, y, num_classes=2):
    temp = np.argmax(y, axis=1)
    colours = [‘r’, ‘g’, ‘b’]
    for i in range(num_classes):
        x_class = x[temp == i]
        plt.scatter(x_class[:, 0], x_class[:, 1], color=colours[i], s=1)
    plt.show()</p>

<p>NUM_FEATURES=50
NUM_CLASSES=2
NUM_SAMPLES=1000</p>

<p>X, y, = make_blobs(num_samples=NUM_SAMPLES, num_features=NUM_FEATURES, num_classes=NUM_CLASSES)
plot_clusters(X, y, num_classes=NUM_CLASSES)
``</p>

<h2 id="gradient-descent-without-gradients">Gradient Descent without Gradients</h2>

<p>We can see the outline of what we need: a function which represents the cost of our operation, and something to compute the gradients. Fortunately <code class="language-plaintext highlighter-rouge">autograd</code> does exactly this!</p>

<p>According to the webiste, autograd</p>

<blockquote>
  <p>Autograd can automatically differentiate native Python and Numpy code. It can handle a large subset of Python’s features, including loops, ifs, recursion and closures, and it can even take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation), which means it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments, as well as forward-mode differentiation, and the two can be composed arbitrarily. The main intended application of Autograd is gradient-based optimization.</p>
</blockquote>]]></content><author><name></name></author><category term="optimisation" /><summary type="html"><![CDATA[Gradient Descent is the simplest learning algorithm. It is very easy to implement, is robust to complex cost functions, and is the foundation of an enormous zoo of variations.]]></summary></entry></feed>