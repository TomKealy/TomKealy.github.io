<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://gregorygundersen.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://gregorygundersen.com/" rel="alternate" type="text/html" /><updated>2025-01-06T15:37:37+00:00</updated><id>http://gregorygundersen.com/feed.xml</id><entry><title type="html">An introduction to Compressive Sensing.</title><link href="http://gregorygundersen.com/blog/2024/10/13/compressive-sensing/" rel="alternate" type="text/html" title="An introduction to Compressive Sensing." /><published>2024-10-13T00:00:00+00:00</published><updated>2024-10-13T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/10/13/compressive-sensing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/10/13/compressive-sensing/"><![CDATA[<h1 id="introduction-labelseccsinto">Introduction \label{sec:csinto}</h1>

<p>This post discusses Compressive Sensing \gls{cs}: an alternative signal acquisition method to Nyquist sampling, which is capable of accurate sensing at rates well below those predicted by the Nyquist theorem. This strategy hinges on using the structure of a signal, and the fact that many signals we are interested in can be compressed successfully. Thus, Compressive Sensing acquires the most informative parts of a signal directly.</p>

<p>The first section surveys the mathematical foundation of CS. It covers the Restricted Isometry Property and Stable Embeddings - necessary and sufficient conditions on which a signal can be successfully acquired. Informally, these conditions suggest that the sensing operator preserves pairwise distances between points when projected to a (relatively) low dimensional space from a high dimensional space. We then discuss which operators satisfy this condition, and why. In particular, random ensembles such as the Bernoulli/Gaussian ensembles satisfy this property - we discuss how this can be applied to the problem of wideband spectrum sensing. We also survey a small amount of the theory of Wishart matrices.</p>

<p>The section concludes with an overview of reconstruction algorithms for CS - methods for unpacking the original signal from its compressed representation. We give insight into the minimum number of samples for reconstruction. We survey convex, greedy, and Bayesian approaches; as well as algorithms which are blends of all three. The choice of algorithm is affected by the amount of undersampling required for system performance, the complexity of the algorithm itself, and the desired reconstruction accuracy. In general: the more complex the algorithm, the better the reconstruction accuracy. The allowable undersampling depends upon the signal itself, and the prior knowledge available to the algorithm. Greedy algorithms are the simplest class, making locally optimal updates within a predefined dictionary of signal atoms. Greedy algorithms have relatively poor performance, yet are the fastest algorithms. Convex algorithms are the next most complex, based upon minimising a global functional of the signal. This class is based upon generalised gradient descent, and has no fixed number of steps. Finally, Bayesian algorithms are the slowest and most complex, but offer the best performance - both in terms of undersampling (as these algorithms incorporate prior knowledge in an elegant way), and in terms of reconstruction accuracy.</p>

<p>We survey some distributed approaches to compressive sensing, in particular some models of joint sparsity, and joint sparsity with innovations.</p>

<p>Finally, we survey some of the approaches to wideband spectrum sensing based upon compressive sensing. In particular we survey the Random Demodulator and the Modulated Wideband converter. Both of these systems make use of low-frequency chipping sequences (also used in spread spectrum communication systems). These low-frequency sequences provide the basis for CS - several different sequences each convoluted with the signal are sufficient to accurately sense the signal.</p>

<h1 id="preliminaries-labelsecprelims">Preliminaries \label{sec:prelims}</h1>

<p>Compressive sensing is a modern signal acquisition technique in which randomness is used as an effective sampling strategy. This is in contrast to traditional, or Nyquist, sampling, which requires that a signal is sampled at regular intervals. The motivation for this new method comes from two disparate sources: data compression and sampling hardware design.</p>

<p>The work of Shannon, Nyquist, and Whittaker \cite{unser2000sampling,} has been an extraordinary success - digital signal processing enables the creation of sensing systems which are cheaper, more flexible, and which offer superior performance to their analogue counterparts. For example, radio dongles, such as those which support the RTLSDR standard, which can process millions of samples per second, can now be bought for as little as £12. However, the sampling rates underpinning these advances have a doubling time of roughly 6 years - this is due to physical limitations in Analogue to Digital conversion hardware. Specifically, these devices will always be limited in bandwidth and dynamic range (number of bits), whilst applications are creating a deluge of data to be processed downstream.</p>

<p>Data compression means that in practice many signals encountered ‘in the wild’ can be fully specified by much fewer bits than required by the Nyquist sampling theorem. This is either a natural property of the signals, for example, images have large areas of similar pixels, or as a conscious design choice, as with training sequences in communication transmissions. These signals are not statistically white, and so these signals may be compressed (to save on storage). For example, lossy image compression algorithms can reduce the size of a stored image to about 1% of the size required by Nyquist sampling. In fact, the JPEG standard uses Wavelets to exploit the inter-pixel redundancy of images.</p>

<p>Whilst this vein of research has been extraordinarily successful, it poses the question: if the reconstruction algorithm is able to reconstruct the signal from this compressed representation, why collect all the data in the first place when most of the information can be thrown away?</p>

<p>Compressed Sensing answers these questions by way of providing an alternative signal acquisition method to the Nyquist theorem. Specifically, situations are considered where fewer samples are collected than traditional sensing schemes. That is, in contrast to Nyquist sampling, Compressive Sensing is a method of measuring the informative parts of a signal directly without acquiring unessential information at the same time.</p>

<p>These ideas have not come out of the ether recently however. Prony, in 1795, \cite{prony1795essai}, proposed a method for estimating the parameters of a number of exponentials corrupted by noise. This work was extended by Caratheodory in 1907 \cite{Caratheodory1907}, who proposed in the 1900s a method for estimating a linear combination of \(k\) sinusoids for the state at time \(0\) and any other \(2k\) points. In the 1970s, Geophysicists proposed minimising the \(\ell_1\)-norm to reconstruct the structure of the earth from seismic measurements. Clarebuot and Muir proposed in 1973, \cite{claerbout1973robust}, using the \(\ell_1\)-norm as an alternative to Least squares. Whilst Taylor, Banks, and McCoy showed in \cite{taylor1979deconvolution} how to use the \(\ell_1\)-norm to deconvolve spike trains (used for reconstructing layers in the earth). Santosa and Symes in \cite{Santosa1986} introduced the constrained \(\ell_1\)-program to perform the inversion of band-limited reflection seismograms. The innovation of \gls{cs} is to tell us under which circumstances these problems are tractable.</p>

<p>The key insight in CS is that for signals which are sparse or compressible - signals which are non-zero at only a fraction of the indices over which they are supported, or signals which can be described by relatively fewer bits than the representation they are traditionally captured in - may be measured in a non-adaptive way through a measurement system which is orthogonal to the signal’s domain.</p>

<p>Examples of sparse signals are:</p>

<ol>
  <li>A sine wave at frequency \(\omega\) is defined as a single spike in the frequency domain yet has an infinite support in the time domain.</li>
  <li>An image will have values for every pixel, yet the wavelet decomposition of the image will typically only have a few non-zero coefficients.</li>
</ol>

<p>Informally, CS posits that for \(s\)-sparse signals \(\alpha \in \mathbb{R}^{n}\) (signals with \(s\) non-zero amplitudes at unknown locations) - \(\mathcal{O}(s \log{n})\) measurements are sufficient to exactly reconstruct the signal.</p>

<p>In practice, this can be far fewer samples than conventional sampling schemes. For example, a megapixel image requires 1,000,000 Nyquist samples but can be perfectly recovered from 96,000 compressive samples in the wavelet domain \cite{candes2008introduction}.</p>

<p>The measurements are acquired linearly, by forming inner products between the signal and some arbitrary sensing vector:</p>

<p>\(y_i = \langle \alpha, \psi_i \rangle\)
\label{inner-product-repr}</p>

<p>or</p>

<p>\(y = \Psi \alpha\)
\label{vector-repr}</p>

<p>where \(y_i\) is the \(i^{th}\) measurement, \(\alpha \in \mathbb{R}^n\) is the signal, and \(\psi_i\) is the \(i^{th}\) sensing vector. We pass to \eqref{vector-repr} from \eqref{inner-product-repr}, by concatenating all the \(y_i\) into a single vector. Thus the matrix \(\Psi\) has the vectors \(\psi_i\) as columns.</p>

<p>If \(\alpha\) is not \(s\)-sparse in the natural basis of \(y\), then we can always transform \(\alpha\) to make it sparse in some other basis:</p>

<p>\(x = \Phi \alpha\)
\label{vector-repr-2}</p>

<p>Note that the measurements may be corrupted by noise, in which case our model is:</p>

<p>\(y = Ax + e\)
\label{CSequation}</p>

<p>where \(e \in \mathbb{R}^m\), and each component is sampled from a \(\mathcal{N}(0, 1/n)\) distribution. Here we have \(A = \Psi \Phi \alpha \in \mathbb{R}^{n \times m}\), i.e., a basis in which the signal \(x \in \mathbb{R}^n\) will be sparse.</p>

<p>We require that sensing vectors satisfy two technical conditions (described in detail below): an Isotropy property, which means that components of the sensing vectors have unit variance and are uncorrelated, and an Incoherence property, which means that sensing vectors are almost orthogonal. Once the set of measurements have been taken, the signal may be reconstructed from a simple linear program. We describe these conditions in detail in the next section.</p>

<h2 id="rip-and-stable-embeddings">RIP and Stable Embeddings</h2>

<p>We begin with a formal definition of sparsity:</p>

<p>\begin{definition}[Sparsity]</p>

<p>A high-dimensional signal is said to be \(s\)-sparse, if at most \(s\) coefficients \(x_i\) in the linear expansion</p>

<p>\(\alpha = \sum_{i=1}^{n} \phi_i x_i\)
\label{sparse-basis-expansion}</p>

<p>are non-zero, where \(x \in \mathbb{R}\), \(\alpha \in \mathbb{R}\), and \(\phi_i\) are a set of basis functions of \(\mathbb{R}^n\).</p>

<p>We can write \eqref{sparse-basis-expansion} as:</p>

<p>\(\alpha = \Phi x\)
\label{def:alpha}</p>

<p>We can make the notion of sparsity precise by defining \(\Sigma_s\) as the set of \(s\)-sparse signals in \(\mathbb{R}^n\):</p>

\[\Sigma_s = \{ x \in \mathbb{R}^n : |\mathrm{supp}(x)| \leq s \}\]

<p>where \(\mathrm{supp}(x)\) is the set of indices on which \(x\) is non-zero.</p>

<p>\end{definition}</p>

<p>\begin{figure<em>}[h]
\centering
\includegraphics[height=7cm, width=\textwidth]{compressive_sensing_example.jpg}
\caption{A visualisation of the Compressive Sensing problem as an under-determined system. Image from \cite{cstutwatkin}}
\label{l1l2}
\end{figure</em>}</p>

<p>We may not be able to directly obtain these coefficients \(x\), as we may not possess an appropriate measuring device, or one may not exist, or there is considerable uncertainty about where the non-zero coefficients are.</p>

<p>Given a signal \(\alpha \in \mathbb{R}^n\), a matrix \(A \in \mathbb{R}^{m \times n}\), with \(m \ll n\), we can acquire the signal via the set of linear measurements:</p>

<p>\(y = \Psi \alpha = \Psi\Phi x = Ax\)
\label{cs-model}</p>

<p>where we have combined \eqref{vector-repr} and \eqref{def:alpha}. In this case, \(A\) represents the sampling system (i.e., each column of \(A\) is the product of \(\Phi\) with the columns of \(\Psi\)). We can work with the abstract model \eqref{cs-model}, bearing in mind that \(x\) may be the coefficient sequence of the object in the proper basis.</p>

<p>In contrast to classical sensing, which requires that \(m = n\) for there to be no loss of information, it is possible to reconstruct \(x\) from an under-determined set of measurements as long as \(x\) is sparse in some basis.</p>

<p>There are two conditions the matrix \(A\) needs to satisfy for recovery below Nyquist rates:</p>

<ol>
  <li>Restricted Isometry Property.</li>
  <li>Incoherence between sensing and signal bases.</li>
</ol>

<p>\begin{definition}[RIP]\label{def:RIP}
We say that a matrix \(A\) satisfies the RIP of order \(s\) if there exists a \(\delta \in (0, 1)\) such that for all \(x \in \Sigma_s\):</p>

\[(1 - \delta) \|x\|_2^2 \leq \|Ax\|_2^2 \leq (1 + \delta) \|x\|_2^2\]

<p>i.e., \(A\) approximately preserves the lengths of all \(s\)-sparse vectors in \(\mathbb{R}^n\).</p>

<p>\label{def:RIP}
\end{definition}</p>

<p>\begin{remark}
Although the matrix \(A\) is not square, the RIP (\ref{def:RIP}) ensures that \(A^TA\) is close to the identity, and so \(A\) behaves approximately as if it were orthogonal. This is formalised in the following lemma from \cite{shalev2014understanding}:</p>

<p>\begin{lemma}[Identity Closeness \cite{shalev2014understanding}]
Let \(A\) be a matrix that satisfies the RIP of order \(2s\) with RIP constant \(\delta\). Then for two disjoint subsets \(I, J \subset [n]\), each of size at most \(s\), and for any vector \(u \in \mathbb{R}^n\):</p>

\[\langle Au_I, Au_J \rangle \leq \delta \|u_I\|_2 \|u_J\|_2\]

<p>where \(u_I\) is the vector with component \(u_i\) if \(i \in I\) and zero elsewhere.</p>

<p>\end{lemma}</p>

<p>\end{remark}</p>

<p>\begin{remark} \label{rem:rip-delta-comment}
The restricted isometry property is equivalent to stating that all eigenvalues of the matrix \(A^TA\) are in the interval \([1 - \delta, 1 + \delta]\). Thus, the meaning of the constant \(\delta\) in (\ref{def:RIP}) is now apparent. \(\delta\) is called the \textit{restricted isometry constant} in the literature.</p>

<p>The constant \(\delta\) in \eqref{def:RIP} measures how close to an isometry the action of the matrix \(A\) is on vectors with a few non-zero entries (as measured in \(\ell_2\) norm). For random matrices \(A\) where the components are drawn from a \(\mathcal{N}(0, 1/n)\) distribution, \(\delta &lt; \sqrt{2} - 1\) \cite{candes2008restricted}.
\end{remark}</p>

<p>\begin{remark} [Information Preservation \cite{davenport2010signal}]
A necessary condition to recover all \(s\)-sparse vectors from the measurements \(Ax\) is that \(Ax_1 \neq Ax_2\) for any pair \(x_1 \neq x_2\), \(x_1, x_2 \in \Sigma_s\), which is equivalent to:</p>

\[\|A(x_1 - x_2)\|_2^2 &gt; 0\]

<p>This is guaranteed as long as \(A\) satisfies the RIP of order \(2s\) with constant \(\delta\). The vector \(x_1 - x_2\) will have at most \(2s\) non-zero entries, and so will be distinguishable after multiplication with \(A\). To complete the argument, take \(x = x_1 - x_2\) in definition \ref{def:RIP}, guaranteeing:</p>

\[\|A(x_1 - x_2)\|_2^2 &gt; 0\]

<p>and requiring the RIP order of \(A\) to be \(2s\).
\end{remark}</p>

<p>\begin{remark} [Stability \cite{davenport2010signal}]
We also require that the dimensionality reduction of compressed sensing preserves relative distances: that is, if \(x_1\) and \(x_2\) are far apart in \(\mathbb{R}^n\), then their projections \(Ax_1\) and \(Ax_2\) are far apart in \(\mathbb{R}^m\). This will guarantee that the dimensionality reduction is robust to noise.
\end{remark}</p>

<p>A requirement on the matrix \(A\) that satisfies both of these conditions is the following:</p>

<p>\begin{definition}[\(\delta\)-stable embedding \cite{davenport2010signal}]
We say that a mapping is a \(\delta\)-stable embedding of \(U,V \subset \mathbb{R}^n\) if</p>

\[(1 - \delta) \|u - v\|_2^2 \leq \|Au - Av\|_2^2 \leq (1 + \delta) \|u - v\|_2^2\]

<p>for all \(u \in U\) and \(v \in V\).</p>

<p>\label{def:d-stable}
\end{definition}</p>

<p>\begin{remark}[\cite{davenport2010signal}]
Note that a matrix \(A\), satisfying the RIP of order \(2s\), is a \(\delta\)-stable embedding of \(\Sigma_s, \Sigma_s\). 
\end{remark}</p>

<p>\begin{remark}[\cite{davenport2010signal}]
Definition \ref{def:d-stable} has a simple interpretation: the matrix \(A\) must approximately preserve Euclidean distances between all points in the signal model \(\Sigma_s\).
\end{remark}</p>

<h1 id="incoherence">Incoherence</h1>

<p>Given that we know a basis in which our signal is sparse, \(\phi\), how do we choose \(\psi\) so that we can accomplish this sensing task? In classical sensing, we choose \(\psi_k\) to be the set of \(T_s\)-spaced delta functions (or equivalently the set of \(1/T_s\) spaced delta functions in the frequency domain). A simple set of \(\psi_k\) would be to choose a (random) subset of the delta functions above.</p>

<p>In general, we seek waveforms in which the signals’ representation would be dense.</p>

<p>\begin{definition}[Incoherence]
A pair of bases is said to be incoherent if the largest projection of two elements between the sensing (\(\psi\)) and representation (\(\phi\)) basis is in the set \([1, \sqrt{n}]\), where \(n\) is the dimension of the signal. The coherence of a set of bases is denoted by \(\mu\).
\end{definition}</p>

<p>Examples of pairs of incoherent bases are:</p>

<ul>
  <li>Time and Fourier bases: Let \(\Phi = \mathbf{I}_n\) be the canonical basis and \(\Psi = \mathbf{F}\) with \(\psi_i = n^{-\frac{1}{2}} e^{i \omega k}\) be the Fourier basis, then \(\mu(\phi, \psi) = 1\). This corresponds to the classical sampling strategy in time or space.</li>
  <li>Consider the basis \(\Phi\) to have only entries in a single row, then the coherence between \(\Phi\) and any fixed basis \(\Psi\) will be \(\sqrt{n}\).</li>
  <li>Random matrices are incoherent with any fixed basis \(\Psi\). We can choose \(\Phi\) by creating \(n\) orthonormal vectors from \(n\) vectors sampled independently and uniformly on the unit sphere. With high probability \(\mu = \sqrt{n \log n}\). This extends to matrices whose rows are created by sampling independent Gaussian or Bernoulli random vectors.</li>
</ul>

<p>This implies that sensing with incoherent systems is good (in the sine wave example above, it would be better to sample randomly in the time domain as opposed to the frequency domain), and efficient mechanisms ought to acquire correlations with random waveforms (e.g., white noise).</p>

<p>\begin{theorem}[Reconstruction from Compressive measurements \cite{Candes2006}]
Fix a signal \(f \in \mathbb{R}^n\) with a sparse coefficient basis, \(x_i\) in \(\phi\). Then a reconstruction from \(m\) random measurements in \(\psi\) is possible with probability \(1 - \delta\) if:</p>

<p>\(m \geq C \mu^2(\phi, \psi) S \log \left( \frac{n}{\delta} \right)\)
\label{minsamples}</p>

<p>where \(\mu(\phi, \psi)\) is the coherence of the two bases, and \(S\) is the number of non-zero entries on the support of the signal.
\end{theorem}</p>

<h2 id="random-matrix-constructions-labelsecmtx-contruction">Random Matrix Constructions \label{sec:mtx-contruction}</h2>

<p>To construct matrices satisfying definition \eqref{def:d-stable}, given \(m, n\) we generate \(A\) by \(A_{ij}\) being i.i.d random variables from distributions with the following conditions \cite{davenport2010signal}:</p>

<p>\begin{condition}[Norm preservation]
\(\mathbb{E} A_{ij}^2 = \frac{1}{m}\)
\label{cond:norm-pres}
\end{condition}</p>

<p>\begin{condition}[sub-Gaussian]
There exists a \(C &gt; 0\) such that:
\(\mathbb{E}\left( e^{A_{ij}t} \right) \leq e^{C^2 t^2 /2}\)
\label{cond:sub-Gauss}
for all \(t \in \mathbb{R}\).
\end{condition}</p>

<p>\begin{remark}
The term \(\mathbb{E}\left( e^{A_{ij}t} \right)\) in \eqref{cond:sub-Gauss} is the <em>moment generating function</em> of the sensing matrix. Condition \eqref{cond:sub-Gauss} says that the moment-generating function of the distribution producing the sensing matrix is dominated by that of a Gaussian distribution, which is also equivalent to requiring that the tails of our distribution decay at least as fast as the tails of a Gaussian distribution. Examples of sub-Gaussian distributions include the Gaussian distribution, the Rademacher distribution, and the uniform distribution. In general, any distribution with bounded support is sub-Gaussian. The constant \(C\) measures the rate of fall off of the tails of the sub-Gaussian distribution.
\end{remark}</p>

<p>Random variables \(A_{ij}\) satisfying conditions \eqref{cond:norm-pres} and \eqref{cond:sub-Gauss} satisfy the following concentration inequality \cite{baraniuk2008simple}:</p>

<p>\begin{lemma}[sub-Gaussian \cite{baraniuk2008simple}]
\(\mathbb{P}\Big( \biggl\lvert \|Ax\|_2^2 - \|x\|_2^2 \biggr\rvert \geq \varepsilon \|x\|_2^2 \Big) \leq 2e^{-cM\varepsilon^2}\)
\label{cond:sub-Gauss concetration}
\end{lemma}</p>

<p>\begin{remark}
Lemma \ref{cond:sub-Gauss concetration} says that sub-Gaussian random variables are random variables such that for any \(x \in \mathbb{R}^n\), the random variable \(\|Ax\|_2^2\) is highly concentrated about \(\|x\|_2^2\).
\end{remark}</p>

<p>Then in \cite{baraniuk2008simple} the following theorem is proved:</p>

<p>\begin{theorem}
Suppose that \(m\), \(n\) and \(0 &lt; \delta &lt; 1\) are given. If the probability distribution generating \(A\) satisfies condition \eqref{cond:sub-Gauss}, then there exist constants \(c_1, c_2\) depending only on \(\delta\) such that the RIP \eqref{def:RIP} holds for \(A\) with the prescribed \(\delta\) and any \(s \leq \frac{c_1 n}{\log{n/s}}\) with probability \(\geq 1-2e^{-c_2n}\).
\end{theorem}</p>

<p>For example, if we take \(A_{ij} \sim \mathcal{N}(0, 1/m)\), then the matrix \(A\) will satisfy the RIP, with probability \cite{baraniuk2008simple}:</p>

\[\geq 1 - 2\left(\frac{12}{\delta}\right)^k e^{-c_0\frac{\delta}{2}n}\]

<p>where \(\delta\) is the RIP constant, \(c_0\) is an arbitrary constant, and \(k\) is the sparsity of the signal being sensed.</p>

<h2 id="wishart-matrices-labelsecwishart">Wishart Matrices \label{sec:wishart}</h2>

<p>Let \(\{X_i\}_{i=1}^r\) be a set of i.i.d. \(1 \times p\) random vectors drawn from the multivariate normal distribution with mean 0 and covariance matrix \(H\).</p>

\[X_i = \left(x_1^{(i)}, \ldots, x_p^{(i)}\right) \sim N\left(0, H\right)\]

<p>We form the matrix \(X\) by concatenating the \(r\) random vectors into an \(r \times p\) matrix.</p>

<p>\begin{definition}[Wishart Matrix]
Let</p>

\[W = \sum_{j=1}^r X_j X_j^T = X X^T\]

<p>Then \(W \in \mathbb{R}^{r \times r}\) has the Wishart distribution with parameters:</p>

\[W_r(H, p)\]

<p>where \(p\) is the number of degrees of freedom.
\end{definition}</p>

<p>\begin{remark}
This distribution is a generalization of the Chi-squared distribution if \(p = 1\) and \(H = I\).
\end{remark}</p>

<p>\begin{theorem}[Expected Value] \label{thm:wishart-mean}
\(\mathbb{E}(W) = rH\)
\end{theorem}</p>

<p>\begin{proof}
\begin{align<em>}
\mathbb{E}(W) &amp;= \mathbb{E}\left(\sum_{j=1}^r X_j X_j^T\right) <br />
&amp;= \sum_{j=1}^r \mathbb{E}(X_j X_j^T) <br />
&amp;= \sum_{j=1}^r \left( \mathrm{Var}(X_j) + \mathbb{E}(X_j) \mathbb{E}(X_j^T) \right) <br />
&amp;= rH
\end{align</em>}
Where the last line follows as \(X_j\) is drawn from a distribution with zero mean.
\end{proof}</p>

<p>\begin{remark}
The matrix \(M = A^T A\), where \(A\) is constructed by the methods from section \ref{sec:mtx-contruction}, will have a Wishart distribution. In particular, it will have:</p>

<p>\(\mathbb{E}(M) = \frac{1}{m} I_n\)
\label{remark: exp AtA}
\end{remark}</p>

<p>The joint distribution of the eigenvalues is given by \cite{levequeMatrices}:</p>

\[p\left(\lambda_1, \ldots, \lambda_r\right) = c_r \prod_{i=1}^r e^{-\lambda_i} \prod_{i&lt;j} \left(\lambda_i - \lambda_j\right)^2\]

<p>The eigenvectors are uniform on the unit sphere in \(\mathbb{R}^r\).</p>

<h2 id="reconstruction-objectives">Reconstruction Objectives</h2>

<p>Compressive sensing places the computational load on reconstructing the coefficient sequence \(x\), from the set of compressive samples \(y\). This is in contrast to Nyquist sampling, where the bottleneck is in obtaining the samples themselves—reconstructing the signal is a relatively simple task.</p>

<p>Many recovery algorithms have been proposed, and all are based upon minimizing some functional of the data. This objective is based upon two terms: a data fidelity term, minimizing the discrepancy between the reconstructed and true signal, and a regularization term—biasing the reconstruction towards a class of solutions with desirable properties, for example sparsity. Typically the squared error \(\frac{1}{2} \|y - Ax\|_2^2\) is chosen as the data fidelity term, while several regularization terms have been introduced in the literature.</p>

<p>A particularly important functional is:</p>

<p>\(\arg \min_x \|x\|_1 \text{ s.t. } y = Ax\)
\label{program:bp}</p>

<p>This is known as Basis Pursuit \cite{Chen1998a}, with the following program known as the LASSO \cite{tibshirani1996regression} as a noisy generalization:</p>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_1\)
\label{program:lasso}</p>

<p>The statistical properties of LASSO have been well studied. The program performs both regularization and variable selection: the parameter \(\lambda\) trades off data fidelity and sparsity, with higher values of \(\lambda\) leading to sparser solutions.</p>

<p>The LASSO shares several features with Ridge regression \cite{hoerl1970ridge}:</p>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_2^2\)
\label{program:Ridge-regression}</p>

<p>and the Non-negative Garrote \cite{breiman1995better}, used for best subset regression:</p>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_0\)
\label{program:ell0}</p>

<p>The solutions to these programs can all be related to each other—it can be shown \cite{hastie2005elements} that the solution to \eqref{program:lasso} can be written as:</p>

<p>\(\hat{x} = S_{\lambda}(x^{OLS}) = x^{OLS} \text{ sign}(x_i - \lambda)\)
\label{soln:lasso}</p>

<p>where \(x^{OLS} = (A^T A)^{-1} A^T y\) is the ordinary least squares solution, whereas the solution to Ridge regression can be written as:</p>

<p>\(\hat{x} = \left( 1 + \lambda \right)^{-1} x^{OLS}\)
\label{soln:ridge}</p>

<table>
  <tbody>
    <tr>
      <td>and the solution to the best subset regression \eqref{program:ell0} where $$|x|_0 = {</td>
      <td>i</td>
      <td>: x_i \neq 0 }$$, can be written as:</td>
    </tr>
  </tbody>
</table>

<p>\(\hat{x} = H_{\lambda}(x^{OLS}) = x^{OLS} \mathbb{I}\left( |x^{OLS}| &gt; \lambda \right)\)
\label{soln:l0}</p>

<p>where \(\mathbb{I}\) is the indicator function. From \eqref{soln:l0} and \eqref{soln:ridge}, we can see that the solution to \eqref{program:lasso}, \eqref{soln:lasso}, translates coefficients towards zero by a constant factor, and sets coefficients to zero if they are too small; thus, the LASSO is able to perform both model selection (choosing relevant covariates) and regularization (shrinking model coefficients).</p>

<p><img src="l1l2.jpg" alt="Solutions to the Compressive Sensing optimization problem intersect the $$l_1$$ norm at points where all components (but one) of the vector are zero (i.e., it is sparsity promoting) \cite{Tibshirani1996}." />
\label{fig:l1l2}</p>

<p>Figure \ref{fig:l1l2} provides a graphical demonstration of why the LASSO promotes sparse solutions. \eqref{program:lasso} can also be thought of as the best convex approximation of the \(\ell_0\) problem \eqref{program:ell0}, as the \(\ell_1\)-norm is the convex hull of the points defined by \(\|x\|_p\) for \(p &lt; 1\) as \(p \rightarrow 0\).</p>

<p>Other examples of regularizers are:</p>

<ul>
  <li><strong>Elastic Net</strong>: This estimator is a blend of both \eqref{program:lasso} and \eqref{program:Ridge-regression}, found by minimizing:</li>
</ul>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_2^2 + \mu \|x\|_1\)
\label{program:enat}</p>

<p>The estimate has the benefits of both Ridge and LASSO regression: feature selection from the LASSO, and regularization for numerical stability (useful in the under-determined case we consider here) from Ridge regression. The Elastic Net will outperform the LASSO \cite{zou2005regularization} when there is a high degree of collinearity between coefficients of the true solution.</p>

<ul>
  <li><strong>TV regularization</strong>:</li>
</ul>

<p>\(\arg \min_x \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|\nabla x\|_1\)
\label{program:tc}</p>

<p>This type of regularization is used when preserving edges while simultaneously denoising a signal is required. It is used extensively in image processing, where signals exhibit large flat patches alongside large discontinuities between groups of pixels.</p>

<ul>
  <li>Candes and Tao in \cite{candes2007dantzig} propose an alternative functional:</li>
</ul>

<p>\(\min_{x \in \mathbb{R}^n} \|x\|_1 \text{ s.t. } \|A^T(Ax - y)\|_{\infty} \leq t\sigma\)
\label{program:danzig}</p>

<p>with \(t = c \sqrt{2 \log{n}}\). Similarly to the LASSO, this functional selects sparse vectors consistent with the data, in the sense that the residual \(r = y - Ax\) is smaller than the maximum amount of noise present. In \cite{candes2007dantzig} it was shown that the \(l_2\) error of the solution is within a factor of \(\log{n}\) of the ideal \(l_2\) error. More recent work by Bikel, Ritov, and Tsybakov \cite{bickel2009simultaneous} has shown that the LASSO enjoys similar properties.</p>

<h2 id="reconstruction-algorithms">Reconstruction Algorithms</h2>
<p>Broadly, reconstruction algorithms fall into three classes: convex-optimisation/linear programming, greedy algorithms, and Bayesian inference. Convex optimisation methods offer better performance, measured in terms of reconstruction accuracy, at the cost of greater computational complexity. Greedy methods are relatively simpler, but don’t have the reconstruction guarantees of convex algorithms. Bayesian methods offer the best reconstruction guarantees, as well as uncertainty estimates about the quality of reconstruction, but come with considerable computational complexity.</p>

<table>
  <thead>
    <tr>
      <th>Algorithm Type</th>
      <th>Accuracy</th>
      <th>Complexity</th>
      <th>Speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Greedy</td>
      <td>Low</td>
      <td>Low</td>
      <td>Fast</td>
    </tr>
    <tr>
      <td>Convex</td>
      <td>Medium</td>
      <td>Medium</td>
      <td>Medium</td>
    </tr>
    <tr>
      <td>Bayesian</td>
      <td>High</td>
      <td>High</td>
      <td>Slow</td>
    </tr>
  </tbody>
</table>

<h3 id="convex-algorithms">Convex Algorithms</h3>
<p>Convex methods cast the optimisation objective either as a linear program with linear constraints or as a second-order cone program with quadratic constraints. Both of these types of programs can be solved with first-order interior point methods. However, their practical application to compressive sensing problems is limited due to their polynomial dependence upon the signal dimension and the number of constraints.</p>

<p>Compressive sensing poses a few difficulties for convex optimisation-based methods. In particular, many of the unconstrained objectives are non-smooth, meaning methods based on descent down a smooth gradient are inapplicable.</p>

<p>To overcome these difficulties, a series of algorithms originally proposed for wavelet-based image de-noising have been applied to CS, known as iterative shrinkage methods. These have the desirable property that they boil down to matrix-vector multiplications and component-wise thresholding.</p>

<p>Iterative shrinkage algorithms replace searching for a minimal facet of a complex polytope with an iteratively denoised gradient descent. The choice of the (component-wise) denoiser is dependent upon the regulariser used in \(\eqref{program:lasso}\). These algorithms have an interpretation as Expectation-Maximisation \cite{figueiredo2003algorithm} — where the E-step is performed as gradient descent, and the M-step is the application of the denoiser.</p>

<h4 id="iterative-soft-thresholding-algorithm-ist">Iterative Soft Thresholding Algorithm (IST)</h4>
<p>```python
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{IST}{$y,A, \mu, \tau, \varepsilon$}
\State $x^0 = 0$
\While{$|x^{t} - x^{t-1}|<em>2^2 \leq \varepsilon$}
\State $x^{t+1} \gets S</em>{\mu\tau}\left(x^t + \tau A^T z^t \right) $
\State $z^t \gets y - A x^t$
\EndWhile
\State \textbf{return} $x^{t+1}$
\EndProcedure
\end{algorithmic}
\caption{The Iterative Soft Thresholding Algorithm \cite{donoho1995noising}}\label{alg:IST}
\end{algorithm}</p>
<h2 id="bayesian-algorithms">Bayesian Algorithms</h2>
<p>Bayesian methods reformulate the optimisation problem into an inference problem. These methods come with a unified theory and standard methods to produce solutions. The theory is able to handle hyper-parameters in an elegant way, provides a flexible modelling framework, and is able to provide desirable statistical quantities such as the uncertainty inherent in the prediction.</p>

<p>Previous sections have discussed how the weights \(x\) may be found through optimisation methods such as basis pursuit or greedy algorithms. Here, an alternative Bayesian model is described.</p>

<p>Equation \(\eqref{CSequation}\) implies that we have a Gaussian likelihood model:</p>

\[p \left(y \mid z, \sigma^2 \right) = (2 \pi \sigma^2)^{-K/2} \exp{\left(- \frac{1}{2 \sigma^2} \|y - Ax|_{2}^{2} \right)}\]

<p>The above has converted the CS problem of inverting sparse weight \(x\) into a linear regression problem with a constraint (prior) that \(x\) is sparse.</p>

<p>To seek the full posterior distribution over \(x\) and \(\sigma^2\), we can choose a sparsity-promoting prior. A popular sparseness prior is the Laplace density function:</p>

\[p\left(x \mid \lambda\right) = \left(\frac{\lambda}{2}\right)^N \exp{-\lambda \sum_{i=1}^{N} |x_i|}\]

<p>Note that the solution to the convex optimisation problem \(\eqref{program:lasso}\) corresponds to a maximum <em>a posteriori</em> estimate for \(x\) using this prior. I.e., this prior is equivalent to using the \(l_1\) norm as an optimisation function (see figure \ref{laplacenormal} \cite{Tibshirani1996}).</p>

<p>\begin{figure<em>}[h]
\centering
\includegraphics[height=7cm]{LaplaceandNormalDensity.png}
\caption{The Laplace (\(l_1\)-norm, bold line) and Normal (\(l_2\)-norm, dotted line) densities. Note that the Laplace density is sparsity-promoting as it penalises solutions away from zero more than the Gaussian density. \cite{Tibshirani1996}}
\label{laplacenormal}
\end{figure</em>}</p>

<p>The full posterior distribution on \(x\) and \(\sigma^2\) may be realised by using a hierarchical prior instead. To do this, define a zero-mean Gaussian prior on each element of \(e\):</p>

\[p\left(e \mid a\right) = \prod_{i=1}^{N} \mathbb{N}\left(n_i \mid 0, \alpha_{i}^{-1}\right)\]

<p>where \(\alpha\) is the precision of the distribution. A gamma prior is then imposed on \(\alpha\):</p>

\[p\left(\alpha \mid a, b \right) = \prod_{i=1}^{N} \Gamma\left( \alpha_i \mid a, b \right)\]

<p>The overall prior is found by marginalising over the hyperparameters:</p>

\[p\left( x \mid a, b \right) = \prod_{i=1}^{N} \int_{0}^{\infty} \mathbb{N}\left(w_i \mid 0, \alpha_{i}^{-1}\right) \Gamma\left( \alpha_i \mid a, b \right)\]

<p>This integral can be done analytically and results in a Student-t distribution. Choosing the parameters \(a\) and \(b\) appropriately, we can make the Student-t distribution peak strongly around \(x_i = 0\), i.e., sparsifying. This process can be repeated for the noise variance \(\sigma^2\). The hierarchical model for this process is shown in figure \ref{fig:bayesiancs}. This model, and other CS models which do not necessarily have closed-form solutions, can be solved via belief-propagation \cite{Baron2010}, or via Monte-Carlo methods.</p>

<p>\begin{figure}[h]
\centering
\includegraphics[height=7cm]{bayesiancs.png}
\caption{The hierarchical model for the Bayesian CS formulation \cite{Ji2008}}
\label{fig:bayesiancs}
\end{figure}</p>

<p>However, as with all methodologies, Bayesian algorithms have their drawbacks. Most notable is the use of the most computationally complex recovery algorithms. In particular, MCMC methods suffer in high-dimensional settings, such as those considered in compressive sensing. There has been an active line of work to address this: most notably, Hamiltonian Monte Carlo (see \cite{neal2011mcmc}) — an MCMC sampling method designed to follow the typical set of the posterior density.</p>

<p>Belief propagation (BP) \cite{Yedidia2011} is a popular iterative algorithm, offering improved reconstruction quality and undersampling performance. However, it is a computationally complex algorithm. It is also difficult to implement. Approximate message passing (AMP) (figure \ref{alg:amp}) solves this issue by blending BP and iterative thresholding (figure \ref{alg:IST}). The algorithm proceeds like iterative thresholding but computes an adjusted residual at each stage. The final term in the update:</p>

\[z^{t+1} = y - Ax^t + \frac{\|x\|_0}{m} z^t\]

<p>comes from a first-order approximation to the messages passed by BP \cite{metzler2014denoising}. This is in contrast to the update from IST (figure \ref{alg:IST}):</p>

\[z^{t+1} = y - Ax^t\]

<p>The choice of prior is key in Bayesian inference, as it encodes all knowledge about the problem. Penalising the least-squares estimate with the \(\ell_1\) norm,</p>

<h2 id="compressive-estimation-labelsecestimation">Compressive Estimation \label{sec:estimation}</h2>
<p>In this section, we develop some intuition into constructing estimators for the signal \(s\) directly from the compressive measurements:</p>

<p>\begin{theorem}</p>

<p>Given a set of measurements of the form:</p>

<p>\(y = As + e\)
<br />
where \(A \in \re^{m \times n}\), \(A_{ij} \sim \mathcal{N}\left(0,1/m\right)\), and \(e \in \re^n\) is AWGN, i.e., \(\sim N\left(0, \sigma^2 I\right)\). We again assume that \(s\) comes from a fixed set of models, parametrised by some set \(\Theta\).</p>

<p>Then, the maximum likelihood estimator of \(s\), for the case where \(s\) can be expanded in an orthonormal basis \(s = \sum_{i=1}^n \alpha_i\phi_i\):</p>

\[\hat{s} = \sum_{i=1}^n m \langle y, A\phi_i \rangle \phi_i\]

<p>\end{theorem}
\begin{proof}
The likelihood for this model is (as \(y\) is a normal random variable):</p>

\[f\left(y \mid s\right) = \left(\frac{1}{\left(2\pi\right)^{n/2}}\right) \exp{\left(- \frac{\left(y-As\right)^T  \left(y-As\right)}{2} \right)}\]

<p>Taking the logarithm and expanding, we find</p>

\[\ln{f\left(y \mid s\right)} = -y^Ty - s^TA^TAs + 2\langle y, As \rangle + c\]

<p>which is equal to:</p>

\[\ln{f} = - \|y\|_2^2 - \|As\|_2^2 + 2\langle y, As \rangle
\label{log-like}\]

<p>(where the constant has been dropped). The first term of \(\eqref{log-like}\) is constant, for the same reasons as in section \(\eqref{sec:estimation}\). The term</p>

\[\|As\|_2^2 = \langle As, As \rangle\]

<p>can be written as</p>

\[\langle A^TAs, s\rangle
\label{ata}\]

<p>We will assume that \(A^TA\) concentrates in the sense of \ref{cond:sub-Gauss concentration} and replace \ref{ata} with its expectation \(\ep{\left( \langle A^TAs, s\rangle \right)}\)</p>

<p>\begin{align<em>}
\ep{\left(\langle A^TAs, s\rangle\right)} &amp;=  \ep{\sum_{i=1}^n (A^TAs)^T_i s_i} <br />
&amp;= \sum_{i=1}^n \ep{(A^TAs)_i s_i} <br />
&amp;= \sum_{i=1}^n \left(\frac{1}{m} e_i s_i\right)^T_i s_i <br />
&amp;= \frac{1}{m} \langle s, s \rangle
\end{align</em>}</p>

<p>because</p>

\[\ep{A^TA} = \frac{1}{m} I\]

<p>as it is a Wishart matrix (see section \ref{sec:prelims}). 
<br />
So we can further approximate \eqref{log-like}:</p>

\[\ln{f\left(y \mid s\right)}  = - \|y\|_2^2 - \frac{1}{m} \|s\|_2^2 + 2 \langle y, As \rangle
\label{approx-log-like}\]

<p>The only non-constant part of \eqref{approx-log-like} is the third term, and so we define the estimator:</p>

<p>\(\hat{s} = \argmax_{\Theta} \langle y , As\left(\Theta\right)\rangle
\label{eq: compressive-estimator}\)
\end{proof}</p>

<p>\begin{corollary}
Consider the case where \(y = As\) (no noise). Then</p>

<p>\begin{align<em>}
y^TA\phi_j &amp;= \sum_i \alpha_i \phi_i^TA^TA\phi_j
\end{align</em>}</p>

<p>So</p>

<p>\begin{align<em>}
y^TA\phi_j &amp;= \sum_i \alpha_i \phi_i^TA^TA\phi_j \sim \frac{\alpha_i}{m} \delta_{ij}
\end{align</em>}</p>

<p>giving</p>

<p>\(\widehat{\alpha_i} = m \left( y^T A \phi_j \right)\)
\end{corollary}</p>

<p>\begin{remark}
The matrix \(M = A^TA\) is the projection onto the row-space of \(A\). It follows that \(\|Ms\|_2^2\) is simply the norm of the component of \(s\) which lies in the row-space of \(A\). This quantity is at most \(\|s\|_2^2\), but can also be \(0\) if \(s\) lies in the null space of \(A\). However, because \(A\) is random, we can expect that \(\|Ms\|_2^2\) will concentrate around \(\sqrt{m/n} \|s\|_2^2\) (this follows from the concentration property of sub-Gaussian random variables \eqref{cond:sub-Gauss concentration}).
\end{remark}</p>

<p>\begin{example}{Example: Single Spike}
We illustrate these ideas with a simple example: estimate which of \(n\) frequencies \(s\) is composed of.</p>

<p>A signal \(s \in \mathbb{R}^{300}\) is composed of a single (random) delta function, with coefficients drawn from a Normal distribution (with mean 100, and variance 1), i.e.,</p>

<p>\(s = \alpha_i \delta_i\)
<br />
with</p>

\[a_i \sim \mathcal{N}\left(100, 1\right)\]

<p>and the index \(i\) chosen uniformly at random from \([1, n]\).
<br />
The signal was measured via a random Gaussian matrix \(A \in \mathbb{R}^{100 \times 300}\), with variance \(\sigma^2 = 1/100\), and the inner product between \(y = As\) and all 300 delta functions projected onto \(\mathbb{R}^{100}\) was calculated:</p>

\[\hat{\alpha}_j = m \langle (A \alpha_i \delta_i), A \delta_j \rangle\]

<p>We plot the \(\hat{\alpha_j}\) below, figure \ref{fig:new_basis_25}, (red circles), with the original signal (in blue, continuous line). Note how the maximum of the \(\hat{\alpha_j}\) coincides with the true signal.</p>

<p>\begin{figure}[h]
\centering
\includegraphics[height=7.3cm]{1spike_legend.jpg}
\label{fig:new_basis_25}
\caption{An example estimation of a single spike using Compressive Inference methods. Note how the maximum of the estimator \(\hat{\alpha}_j\) corresponds to the true signal.}
\end{figure}
\end{example}</p>]]></content><author><name></name></author><category term="Compressive" /><category term="sensing." /><summary type="html"><![CDATA[Introduction \label{sec:csinto}]]></summary></entry><entry><title type="html">A Grab Bag of Approaches to Frequentist Multiple Testing.</title><link href="http://gregorygundersen.com/blog/2024/10/09/approaches-to-multiple-testing/" rel="alternate" type="text/html" title="A Grab Bag of Approaches to Frequentist Multiple Testing." /><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/10/09/approaches-to-multiple-testing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/10/09/approaches-to-multiple-testing/"><![CDATA[<p>We show how to handle more than one hypothesis at a time.</p>

<h2 id="introduction">Introduction</h2>

<p>This note is to explain the statistical background behind multiple testing, which experimenters (be they researchers in academia, NGOs, or tech)  need to  consider when implementing their experiments. Understanding multiple testing is crucial because it determines the reliability of your results and influences how much your end users can trust your papers or products.</p>

<p>The presentation will be in terms of p-values as the algorithms for multiple metric adjustments can be done by hand. Their purpose is to develop intuition for the decision theoretic issues underlying the algorithms. Here we are focussing on the why, rather than on any specific how.</p>

<h2 id="the-challenge-of-multiple-testing">The Challenge of Multiple Testing</h2>

<p>When a researcher runs an experiment, they will typically choose a single metric to decide whether the intervention has been successful or not. This is called the primary decision metric or endpoint in the experimentation literature. Some examples include: the ratio of the total number of conversions to unique visitors on a website (the conversion rate); or the test score of pupils in a classroom (as in the STAR study).</p>

<p>If you run A/B tests on your website and regularly check ongoing experiments for significant results, you might be falling prey to what statisticians call repeated significance testing errors. As a result, even though your dashboard says a result is statistically significant, there’s a good chance that it’s actually insignificant.</p>

<p>To determine the outcome of the experiment, the researcher specifies a binary decision rule tied to their primary metric. For example the conversion rate of the test variation increases by at least 5% (a typical hypothesis in tech). Or that pupils in smaller classes have 20% higher scores on a standardised test at the end of the school year (STAR study).</p>

<p>When an A/B testing dashboard says there is a “95% chance of beating original” or “90% probability of statistical significance,” it’s asking the following question: Assuming there is no underlying difference between A and B, how often will we see a difference like we do in the data just by chance? The answer to that question is called the significance level, and “statistically significant results” mean that the significance level is low, e.g. 5% or 1%. Dashboards usually take the complement of this (e.g. 95% or 99%) and report it as a “chance of beating the original” or something like that.</p>

<p>However, the significance calculation makes a critical assumption that you have probably violated without even realizing it: that the sample size was fixed in advance. If instead of deciding ahead of time, “this experiment will collect exactly 1,000 observations,” you say, “we’ll run it until we see a significant difference,” all the reported significance levels become meaningless. This result is completely counterintuitive and all the A/B testing packages out there ignore it, but I’ll try to explain the source of the problem with a simple example.</p>

<p>There is more than one thing that researchers can measure, and alongside the primary metric many other things are measured about the experimentation subjects. This includes financial metrics in our conversion rate example, or the rates at which pupils dropped out of education altogether in the STAR study.</p>

<p>It’s tempting, then, to have multiple decision metrics to decide if an intervention has been successful. For example we might decide that a change in web page design is an improvement if it increases the conversion rate by at least 5%, and also increases the average revenue per user (ARPU) by 3%. This is an absolutely reasonable thing to do. When we add more metrics with statistical information to an experiment we think that we are increasing the number of endpoints by 1; however this is incorrect, we are increasing the number of endpoints for our experiment by an order of magnitude. Consider going from 1 to 2 metrics in an experiment.  Think of it like this: with one metric, under any binary decision rule you have two endpoints for the experiment. Yes, or no. 0 or 1. When you add a second metric, you now have four endpoints which the experiment can take: 00, 01, 10, 11. With three metrics you now have eight endpoints: 000, 001, 010, 100, 011, 101, 110, 111. Since statistical tests are probabilistic, with each independent test you add you increase the probability that one of them will say yes, even if there is no effect present in the data.  This is the challenge of multiple testing: how can you add more metrics whilst controlling the overall experiment-wise false probability rate?</p>

<p>There are a number of statistical approaches to handle this inflation of error rates, so that the conclusions from any statistical analysis are sound.</p>

<h3 id="what-is-a-false-positive-and-why-is-it-important">What is a false positive and why is it important?</h3>

<p>In hypothesis testing, a false positive occurs when a statistical result returns a ‘yes’ decision even when this is not warranted by the data. In other words, we conclude an effect is present in the data when it actually isn’t. It’s important to remember that statistical tests are not infallible, they are instead probabilistic and so, no matter how stringent you make the requirements of the analysis, they can always go awry. 
This type of error is crucial because it can lead to unnecessary actions or interventions based on a perceived effect that isn’t real. There is a feeling that false positives are costless, and that increasing the number of false positives is no big matter. However, consider that most statistical tests are two sided and that they do not distinguish between the upper and lower tails of the distribution. In this situation a false positive could be in the negative direction, and in fact lose the company money.</p>

<h4 id="what-is-the-family-wise-error-rate">What is the Family-wise error rate?</h4>

<p>The false positive rate (FPR) of a single metrics is the proportion of all negatives that still yield positive test outcomes. It’s the probability that a null hypothesis is incorrectly rejected given that it is true. The FPR is commonly set at 5% (0.05), meaning we are willing to accept a 5% chance of falsely claiming a significant effect or association.</p>

<p>When we start adding more metics to our statistical analyses the probability that any single negative yields a positive outcome is called the family-wise error rate (FWER). The FWER is the probability of making at least one Type I error (i.e., falsely rejecting a true null hypothesis) across a family of tests. In other words, it controls the total rate of false positive results to keep it below a predefined threshold. Many primary sources use FWER and the FPR for multiple tests interchangeably. Keeping the FWER at an acceptable level is challenging, as it is a very stringent requirement.</p>

<h3 id="family-wise-error-rate-fwer">Family Wise Error Rate (FWER)</h3>
<p>One of the best-known methods to control the FWER is the Bonferroni correction, which involves dividing the desired significance level by the number of comparisons. For example: if you set an overall alpha of 0.1 and you wanted statistical information on 10 metrics in your experiment, then you would then use an alpha of 0.01 (i.e. divide by 10) for all of the 10 independent tests. This way the overall FWER is constrained to 0.1.  This method is quite stringent (but by far the safest) as it dramatically reduces the window that any single test is a false positive. Though can reduce the risk of any false positives at the cost of potentially missing true positive results (increasing Type II errors). 
For a business AB testing platform, this means that we could reduce the number of rollouts by a factor proportional to the number of metrics we are looking at. In medicine this may be a good trade-off to make.</p>

<h4 id="what-is-the-false-discovery-rate">What is the false discovery rate?</h4>

<p>In maths one useful move that’s always available to you is to define yourself out of a problem. The false discovery rate is an example of one such move. The false discovery rate (FDR) is the expected proportion of false positives among all declared positives. Unlike the FPR, the FDR controls the expected proportion of false discoveries, rather than the chance of any false discoveries.</p>

<p>Colloquially, what this means is that you tolerate a cost of slightly more false positives overall, with the benefit that any adjustment scheme for the FDR is more forgiving.</p>

<h3 id="false-discovery-rate-fdr">False Discovery Rate (FDR)</h3>
<p>The FDR, on the other hand, is the expected proportion of Type I errors among all rejected hypotheses. Instead of trying to avoid any single false positive like FWER, the FDR control procedure tries to limit the proportion of false positives among all discoveries.</p>

<p>The Benjamini-Hochberg procedure is a common method used to control the FDR. It is generally less conservative than methods that control the FWER, allowing for more false positives but increasing the power to detect true positives. It works by calculating a sequence of increasing adjusted p-values for each different metric.</p>

<p>In essence, the key difference lies in what each rate seeks to control: FWER controls the probability of at least one false positive, while FDR controls the expected proportion of false positives. Your choice between the two would depend on the balance you wish to strike between avoiding false positives and not missing true positives.</p>

<h3 id="what-is-a-false-negative-and-why-is-it-important">What is a false negative and why is it important?</h3>

<p>A false negative, on the other hand, occurs when we fail to reject a false null hypothesis. In simpler terms, we conclude that there is no effect or association when there actually is. False negatives are important because they can prevent us from taking necessary actions or recognizing significant associations or effects.
This error is when we leave money on the table. Statistical tests are designed to give us the most statistical power (to minimise the number of false negatives), subject to constraints on the chance of detecting a false positive. Again, remember that no matter the parameters of your system (for example, you might increase the sample size dramatically to reduce the probabilities above) you will always be faced with some inherent uncertainty with a statistical test.</p>

<h3 id="how-do-they-vary-by-the-number-of-comparisons">How do they vary by the number of comparisons?</h3>

<p>As we perform more and more independent statistical comparisons, we increase the likelihood that our results are false positives (and also increase the number of false negatives we leave by the wayside). This is referred to as the “multiple testing problem.” For each individual test, the probability of a false positive or false negative may be small. However, when we perform multiple tests, these probabilities multiply, exponentially  increasing the chance of one or more false results. For example, if we have 10 independent tests, the probability that a single one is positive purely by change is 1 - (1-0.05)^10 ~ 40%.</p>

<p>There are a few ways to avoid this, but first we need to define a few terms:</p>

<h3 id="how-can-we-prevent-this-from-happening">How can we prevent this from happening?</h3>

<p>We can mitigate the issue of multiple testing by applying various correction methods. Some popular ones include the Bonferroni Correction for the FWER and the Benjamini-Hochberg procedure for the FDR. These techniques adjust our threshold for significance (e.g., lowering the p-value) based on the number of comparisons being made to maintain an appropriate error rate.</p>

<h4 id="methods-to-control-the-family-wise-error-rate">Methods to control the Family Wise Error Rate</h4>

<h4 id="methods-to-control-the-false-discovery-rate">Methods to control the False Discovery Rate</h4>

<h3 id="must-we-apply-the-adjustments-equally">Must we apply the adjustments equally.</h3>

<p>No. It’s not required that we apply adjustments equally. In fact the Bejamini-Hochberg algorithm explicitly does not apply equally to all metrics.</p>

<p>A good example is to consider the following situation. You have 11 metrics you would like to compute p-values for in an experiment, with an overall alpha of 0.1. However, for one of them you wish to allocate an alpha of 0.5, and for the rest of them you want to allocate a p-value of 0.005 (0.05/10). This is a perfectly reasonable situation, as it keeps the overall alpha to 0.1 (the sum of all the individual alpha values). You could also extend this: your primary metric gets an alpha of 0.5, some set of secondary metrics gets an alpha of 0.3 (and the p-value thresholds are calculated via the Benjamini-Hochberg procedure) and even more tertiary metrics have an alpha of 0.2 (and the p-value thresholds are calculated via the Benjamini-Hochberg procedure). This tiering strategy is also appropriate. What wouldn’t be appropriate would be leaving the primary metric unadjusted, and still claiming the overall alpha is 0.1. In that case, the overall alpha would be inflated (possibly as high as 0.2).</p>]]></content><author><name></name></author><category term="hypothesis-testing," /><category term="multiple-testing" /><summary type="html"><![CDATA[We show how to handle more than one hypothesis at a time.]]></summary></entry><entry><title type="html">Must we adjust p-values if we test multiple hypotheses?</title><link href="http://gregorygundersen.com/blog/2024/05/16/Must-we-adjust-p-values-if-we-test-multiple-hypotheses/" rel="alternate" type="text/html" title="Must we adjust p-values if we test multiple hypotheses?" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/05/16/Must-we-adjust-p-values-if-we-test-multiple-hypotheses</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/05/16/Must-we-adjust-p-values-if-we-test-multiple-hypotheses/"><![CDATA[<p>Jerzy Neyman and Egon Pearson developed their idea of hypothesis testing in 1933. This has since become the dominant paradigm of statistical inference. The goal of Neyman and Pearson’s method were statistical tests which control errors. So scientist’s could adopt claims without being wrong too often. A single number, alpha, controls the long run amount of wrong decisions. It is set before a scientist conducts any experiment. One corollary of this is that the scientific literature contains errors. Be careful about trusting scientific claims.</p>

<p>If you want to test more than one hypothesis, then you can’t use the overall alpha for all tests. Using the same alpha for all tests inflates the false positive error rate. A false positive is when a statistical test claims that an effect is present, but no effect is present. When we add an endpoint to our experiment we are not increasing the number of endpoints by one. We increase the number of endpoints by an order of magnitude. With one dependent variable, you have two endpoints for the experiment: either true (T) or false (F). Adding a second dependent variable, the experiment table: FF, FT, TF, TT. As you add more hypotheses it becomes more likely that any single one will be a false positive. To prevent this, you need to control the error rate at the level of the claim. Statisticians recommend that researchers use some form of multiplicity correction in these situations.</p>

<p>Adding an endpoint to your analysis is legitimate. For example, testing whether a drug is effective for depression and anxiety. But leaving your significance threshold uncorrected is not legitimate. Some scientific disciplines question whether they need multiplicity corrections. Any paper which doesn’t advocate using multiplicity correction is wrong. The need for adjustments is a straightforward consequence of the Neyman-Pearson approach.
There are two cases which require multiplicity corrections:</p>
<ul>
  <li>When your experiment has multiple endpoints.</li>
  <li>When your experiment has multiple levels of an intervention.</li>
</ul>

<p>The simplest approach to multiple comparisons is by controlling the familywise error rate (FWER). The FWER is the probability of making at least one error across a family of tests. It ensures that the total rate of false positives results is always below a strict threshold. Keeping the FWER at an acceptable level is challenging, so it is a stringent rule. The best-known method to control the FWER is the Bonferroni correction. This correction divides the significance level by the number of tests. This controls the alpha of the overall family of tests. Researchers use Bonferroni corrections as they are simple. The resulting significance threshold can be known ahead of time. The drawback of the Bonferroni correction is that it applies the harshest correction. It is so conservative that it can miss true effects.</p>

<p>What a family of tests is, requires some thought. Error control does not aim to constrain the number of erroneous statistical inferences. It also aims to constrain the number of erroneous theoretical inferences. So we need to understand which tests relate to a single theoretic inference. Researchers don’t correct for multiple comparisons becuase they don’t understand their theoretical inference.</p>

<p>We can control error rates for all tests in an experiment. This is the experimentwise error rate. Or we can contorl error rates for a specific group of tests the familywise error rate. Broad questions have many possible answers. As an example consider running a 2x2x2 ANOVA. We test for three main effects, three two-way interactions, and one three-way interaction. This makes seven tests in total. We want to know if there is ‘an effect’ out of any of the interactions we can test for. Rejecting the null-hypothesis in any test would suggest the answer to our question is ‘yes’. Here the experimentwise error rate is what we need to correct for. This is the probability of deciding there is any effect, when all null hypotheses are true. A 5% alpha level for every test in the ANOVA would suggest that there is an effect 30% of the time. This is even when all null hypotheses are true.</p>

<p>Researchers are often vague about their claims. They do not specify their hypotheses clearly enough. So the issue of multiple comparisons remains.. Suppose we want to compare predictions from two competing theories. So we design an experiment to test them. Theory A predicts an interaction in a 2x2 ANOVA. Theory B predicts no interaction, but at least one significant main effect.</p>

<p>The researcher will perform three statistical tests. But don’t assume that we need to control the error rate across all three by using a/3! A ‘family’ depends on a set of related tests. In this case, where we test two theories, there are two families of tests. The first family consists of a single interaction effect. The second family of two main effects. For both families the overall alpha level is 5%. We will decide to accept Theory A when p &lt; α for the interaction, and when p &gt; α we will decide not to accept Theory A . If the null is true, at most 5% of these decisions we make in the long run will be incorrect. So we control the percentage of decision errors. For the second theory, we need to apply a Bonferroni correction. We will decide to accept Theory B when p &lt; α/2 for either of the two main effects, and not accept theory B when p &gt; α/2. When the null hypothesis is true, we will decide to accept Theory B when it is not true at most 5% of the time. We could still accept neither theory, or even both, if the experiment was not the decisive test.</p>

<p>Here’s another example. Let’s assume we collect data from 100 participants in a control and treatment condition. We collect 3 dependent variables (dv1, dv2, and dv3). The true effect size is 0). We will analyse the three dv’s in independent t-tests. We must specify our alpha level, and decide if we need to correct for multiplicity. How we control error rates depends on the claim we want to make. One claim is that the treatment works if there is an effect on any of the three variables. This means we corroborate the claim when the p-value of the any t-test is smaller than alpha level. We could also want to make three different predictions. Here we have three different hypotheses, and predict there will be an effect on dv1, dv2, and dv3. The criterion for each t-test is the same, but we now have three hypotheses to check (H1, H2, and H3). Each of these claims can be corroborated, or not.</p>

<p>You don’t always need to control the FWER. Limiting the number of false positives among all discoveries is good enough most of the time. There is a name for this rate, and it’s called the false discovery rate. In maths you can often define yourself out of a problem, and the false discovery rate is one such move. The false discovery rate (FDR) is the average number of false positives in all positive tests. Any adjustment scheme for the FDR is more forgiving than a scheme for the FWER. This comes at the cost of more false positives overall.</p>

<p>The Benjamini-Hochberg procedure is a common method used to control the FDR. It calculates a sequence of increasing adjusted p-values for each different hypothesis. Unlike the Bonferroni correction it is data-dependent. So the outcome cannot be known in advance.</p>]]></content><author><name></name></author><category term="GAMs" /><summary type="html"><![CDATA[Jerzy Neyman and Egon Pearson developed their idea of hypothesis testing in 1933. This has since become the dominant paradigm of statistical inference. The goal of Neyman and Pearson’s method were statistical tests which control errors. So scientist’s could adopt claims without being wrong too often. A single number, alpha, controls the long run amount of wrong decisions. It is set before a scientist conducts any experiment. One corollary of this is that the scientific literature contains errors. Be careful about trusting scientific claims.]]></summary></entry><entry><title type="html">Bayesian Sequential Hypothesis Testing</title><link href="http://gregorygundersen.com/blog/2024/02/04/bayesian-sequential-hypothesis-testing/" rel="alternate" type="text/html" title="Bayesian Sequential Hypothesis Testing" /><published>2024-02-04T00:00:00+00:00</published><updated>2024-02-04T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/02/04/bayesian-sequential-hypothesis-testing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/02/04/bayesian-sequential-hypothesis-testing/"><![CDATA[<p>We show how mSPRT can be thought of as a Bayesian Algorithm.</p>

<h3 id="a-bayesian-model">A Bayesian Model</h3>
<p>Consider the following Bayesian model for the data:
\(Y_1|\alpha, \delta, M_1 \sim \mathcal{N}\left(\frac{1}{n_1} \alpha + \frac{1}{2n_1} \delta, \sigma^2 I_{n_1}\right)\)</p>

\[Y_2|\alpha, \delta, M_1 \sim \mathcal{N}\left(\frac{1}{n_2} \alpha - \frac{1}{2n_2} \delta, \sigma^2 I_{n_2}\right)\]

\[\delta|M_1 \sim \mathcal{N} (0, \tau^2)\]

\[p(\alpha|M_1) \propto 1\]

<p>Note that the prior distribution for the lift \(\delta\) is equal to the mixing distribution in the mSPRT. If you are not familiar with this vectorized notation, an alternative notation is</p>

<table>
  <tbody>
    <tr>
      <td>$$y_{1i}</td>
      <td>\alpha, \delta, M_1 \sim \mathcal{N} (\alpha + \frac{\delta}{2}, \sigma^2) \text{ } \forall i = 1, \ldots, n_1$$</td>
    </tr>
    <tr>
      <td>$$y_{2j}</td>
      <td>\alpha, \delta, M_1 \sim \mathcal{N} (\alpha - \frac{\delta}{2}, \sigma^2) \text{ } \forall j = 1, \ldots, n_2$$</td>
    </tr>
  </tbody>
</table>

<p>In this model for the “alternative” we have a grand mean \(\alpha\) and a lift parameter \(\delta\).</p>

<p>Following a Bayesian approach, we assign \(\alpha\) a uniform prior, and \(\delta\) a Normal prior. Under this model, the interpretation of \(\delta\) is still unchanged as \(E[y_1]-E[y_2]\) i.e., the expected difference in means. Moreover, it also follows under this model that \(\bar{Y}_1 - \bar{Y}_2 \sim \mathcal{N} (\delta, \sigma^2(\frac{1}{n_1} + \frac{1}{n_2}))\). Notice, however, that we have conditioned on this model being the alternative model \(M_1\). The “null” model can be expressed as</p>

<table>
  <tbody>
    <tr>
      <td>$$Y_1</td>
      <td>\alpha, M_0 \sim \mathcal{N} (\frac{1}{n_1} \alpha, \sigma^2 I_{n_1})$$</td>
    </tr>
    <tr>
      <td>$$Y_2</td>
      <td>\alpha, M_0 \sim \mathcal{N} (\frac{1}{n_2} \alpha, \sigma^2 I_{n_2})$$</td>
    </tr>
    <tr>
      <td>$$p(\alpha</td>
      <td>M_0) \propto 1$</td>
    </tr>
  </tbody>
</table>

<p>in which there is no \(\delta\) i.e., it is the same as the alternative model except that our prior on \(\delta\) concentrates all of its mass at zero. A Bayesian would test the hypothesis that \(\delta \neq 0\) by looking at the Bayes Factor between models \(M_1\) and \(M_0\)</p>

\[\frac{p(Y_1, Y_2|M_1)}{p(Y_1, Y_2|M_0)} = \frac{\int p(Y_1, Y_2|\alpha, \delta)d\alpha d\delta}{\int p(Y_1, Y_2|\alpha)d\alpha},\]

<p>note that we are explicitly integrating out the nuisance intercept parameter \(\alpha\) w.r.t. a uniform prior. Let’s go ahead and compute these integrals!</p>

<p>To make the mathematics easier, it is helpful to stack the observations \(Y_1, Y_2\) into a larger multivariate normal:</p>

\[Y = [Y_1', Y_2']' \sim \mathcal{N} (1_n\alpha + X\delta, \sigma^2I_n)\]

<p>where \(n = n_1 + n_2\), \(1_n\) is a vector of 1’s, \(I_n\) is an identity matrix and \(X' = \frac{1}{2}[1_{n_1}', -1_{n_2}']\).</p>

<p>The first trick is to marginalize out the intercept parameter by computing the component of \(Y\) that is orthogonal to the column space of \(1_n\). Dropping the \(n\)’s from now on, let \(P_1 = \frac{1}{n}1(1'1)^{-1}1' = \frac{1}{n} 11'\) be the projection operator onto the column space of \(1\). This neatly isolates the component of \(\alpha\) in the quadratic form i.e.</p>

\[\begin{aligned}
\|Y - 1\alpha - X\delta\|^2_2 &amp;= (Y - 1\alpha - X\delta)'(Y - 1\alpha - X\delta) \\
&amp;= (Y - 1\alpha - X\delta)'(P_1 + I - P_1)(Y - 1\alpha - X\delta) \\
&amp;= \|P_1(Y - 1\alpha - X\delta)\|^2_2 + \|(I - P_1)(Y - 1\alpha - X\delta)\|^2_2 \\
&amp;= n(\alpha - \bar{Y} - \bar{X}\delta)^2 + \|Y_c - X_c\delta\|^2_2
\end{aligned}\]

<p>where \(Y_c\) and \(X_c\) are the centered observations and design matrix</p>

\[Y_c = (I - P_1)Y = Y - 1 \bar{Y}\]

\[X_c = (I - P_1)X = \frac{1}{2}
\begin{bmatrix}
1_{n_1} \\
-1_{n_2}
\end{bmatrix}
- \frac{1}{2}
\frac{n_1 - n_2}{n_1 + n_2}
\begin{bmatrix}
1_{n_1} \\
1_{n_2}
\end{bmatrix}
= \frac{1}{n_1 + n_2}
\begin{bmatrix}
n_2 1_{n_1} \\
-n_1 1_{n_2}
\end{bmatrix}.\]

<p>As a foreshadowing of a result later it is also suggestive to note at this point that</p>

\[X_c'X_c = \frac{n_1n_2}{n_1 + n_2} = \frac{1}{\frac{1}{n_1} + \frac{1}{n_2}}.\]

<p>This has rearranged the quadratic form in the likelihood into two terms, one a quadratic in \(\alpha\), which makes the integration step easy.</p>

\[p(Y_1, Y_2|\delta, M_1) = \frac{1}{n} \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n-1}{2}} \exp\left(-\frac{1}{2\sigma^2} \|Y_c - X_c\delta\|^2\right)\]

\[p(Y_1, Y_2|M_0) = \frac{1}{n} \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n-1}{2}} \exp\left(-\frac{1}{2\sigma^2} \|Y_c\|^2\right)\]

<p>Integrating out the intercept from the model had the effect of losing a degree of freedom and working with the centered observations and design matrix. The final integration is the marginalization step of \(\delta\) w.r.t. the prior \(N (0, \tau^2)\):</p>

\[p(Y_1, Y_2|M_1) = \frac{1}{n} \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n-1}{2}} \exp\left(-\frac{1}{2\sigma^2} \|Y_c\|^2\right) \left(\frac{1}{2\pi\tau^2}\right)^{\frac{1}{2}} \exp\left(\frac{1}{2} \left(X_c'X_c\sigma^2 + 1\tau^2\right)^{-1} (X_c'Y_c\sigma^2)^2\right)\]

<p>When forming the Bayes factor, most of these terms cancel:</p>

\[\frac{p(Y_1, Y_2|M_1)}{p(Y_1, Y_2|M_0)} = \left(\frac{1}{\tau^2} \frac{1}{\tau^2 + X_c'X_c\sigma^2}\right)^{\frac{1}{2}} \exp\left(\frac{1}{2} \left(X_c'X_c\sigma^2 + 1\tau^2\right)^{-1} (X_c'Y_c\sigma^2)^2\right),\]

<p>yet \(X_c'X_c/\sigma^2 = 1/(\sigma^2(\frac{1}{n_1} + \frac{1}{n_2}))\) which is \(1/\rho^2\) under our earlier definition of \(\rho^2\) and</p>

\[X_c'Y_c =X_c'Y_c + X_c'1 \bar{Y} = X_c'Y = \frac{n_2n_1}{n_1 + n_2} (\bar{Y}_1 - \bar{Y}_2) = \frac{\bar{Y}_1 - \bar{Y}_2}{\frac{1}{n_1} + \frac{1}{n_2}}.\]

<p>The Bayes factor then simplifies to
\(\frac{p(Y_1, Y_2|M_1)}{p(Y_1, Y_2|M_0)} = \left(\frac{\rho^2}{\rho^2 + \tau^2}\right)^{\frac{1}{2}} \exp\left(\frac{1}{2} \frac{\tau^2}{\rho^2 + \tau^2} \frac{(\bar{Y}_1 - \bar{Y}_2)^2}{\rho^2}\right)
=\Lambda\)</p>]]></content><author><name></name></author><category term="hypothesis-testing" /><summary type="html"><![CDATA[We show how mSPRT can be thought of as a Bayesian Algorithm.]]></summary></entry><entry><title type="html">Sequential Hypothesis Testing</title><link href="http://gregorygundersen.com/blog/2024/02/02/sequential-hypothesis-testing/" rel="alternate" type="text/html" title="Sequential Hypothesis Testing" /><published>2024-02-02T00:00:00+00:00</published><updated>2024-02-02T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/02/02/sequential-hypothesis-testing</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/02/02/sequential-hypothesis-testing/"><![CDATA[<p>We introduce the mSPRT and give a derivation from a Bayesian point of view.</p>

<h1 id="sequential-testing">Sequential Testing</h1>

<p>Normally when you go about hypothesis testing, after a random sample is observed one of two possible actions are taken: accept the null hypothesis \(H_0\), or accepct the tive hypothesis \(H_1\). In some cases the evidence may strongly support one of the hypotheses, whilst in other cases the evidence may be less convincing. Nevertheless, a decision must he made. All this assumes that all the data has been collected, and no more is available. It doesn’t have to be this way. There is a class of hypothesis tests where you can safely collect more data when the evidence is ambiguous. Such a test typically continues until the evidence strongly favors one of the two hypotheses.</p>

<p>In case of simple hypotheses the strength of the evidence for \(H\) is given by the ratio of the probability of the data under \(H_0\), to the probability of the data under \(H_1\). We denote this likelihood ratio by \(\Lambda\). The Neyman-Pearson lemma implies that for a given amount of information the likelihood ratio test is the most powerful test. Such a rule decides to accept \(H_1\) if \(\Lambda\) is big enough, and decides to accept \(H_0\) otherwise. How big \(\Lambda\) must get to lead to the decicion \(H_1\), danends on, amond other things, its sampling distribution under \(H_0\) and \(H_1\). It is not unusual for “big enough” to mean \(\Lambda \geq 1\). Such tests could easily decide \(H_0\) or \(H_1\) when the actual evidence is neutral.</p>

<h2 id="sprt">SPRT</h2>

<p>In 1943 Wald proposed the <em>sequential probability ratio test</em> (SPRT). Suppose that \(Y\) is a random variable with unknown distribution \(f\). We want to test the following hypotheses:</p>

<ul>
  <li>
\[H_0: f=f_0\]
  </li>
  <li>
\[H_1: f=f_1\]
  </li>
</ul>

<p>where \(f_0\) and \(f_1\) are specified. We observe values of \(Y\) successively: \(y_1, y_2, \ldots\) the random variables \(Y_i\) corresponding to the \(y_i\) are i.i.d with common distribution \(f\). Let</p>

\[\Lambda = \prod_{i=1}^n \frac{f_1\left(x_i\right)}{f_0\left(x_i\right)}\]

<p>be the likelihood ratio at stage \(n\). We choose two decision boundaries \(A\) and \(B\) such that \(0 &lt; B &lt; A &lt; \infty\),  we accept \(H_0\) if \(\Lambda \leq B\) and \(H_1\) if \(\Lambda \geq A\), and we continue if \(B \leq \Lambda \leq A\). The constants \(A\) and \(B\) are determined by the desired false positive and false negative rates of the experimenter. In general it can  be shown that the boundaries A and B can be calculated as with very good approximation as</p>

\[A=\log\left(\frac{\beta}{1=\alpha}\right)\]

\[B=\log\left(\frac{1-\beta}{\alpha}\right)\]

<p>so the SPRT is really very simple to apply in practice.</p>

<h3 id="example-iid-case">Example: IID case</h3>

<p>Assume that \(X_1 , X_2 \ldots\) are independent and identically distributed with distributions \(P\), and \(Q\), under \(H_0\) and \(H_1\) respectively. Then \(L_n =\Lambda\left(X_n\right)\). Let \(Z_i=\log{\left(\Lambda\left(X_n\right)\right)}\). Since \(\log{L_n} = \sum_i^n Z_i\) the SPRT can be viewed as z random walk (or more properly a family of random walks), with steps \(Z_i\) which proceeds until it crosses \(\log{B}\) or \(\log{A}\).</p>

<p>We now focus on the more general case where \(P\) and \(Q\), have either densities or probability mass functions of the form:</p>

\[f\left(x; \theta\right) = h\left(x\right)\exp{C\left(\theta\right)x - D\left(\theta\right)}\]

<p>where \(\theta\) is a real valued parameter. The <em>exponential families</em> are to work with and include many common distributions. Let \(P\), be determined by \(f\left(x; \theta_0\right)\), and let \(Q\), be determined by \(f\left(x; \theta_1\right)\). Then 
\(Z_i = \left[C\left(\theta_1\right) - C\left(\theta_0\right) \right]X_i - \left[D\left(\theta_1\right)-D\left(\theta_0\right)\right]\).</p>

<p>In terms of the random walk with steps \(Z_i\) the SPRT continues until fixed boundaries are crossed.</p>

<p>It is somtimes easier to perform the test using the sums of the \(X_i\)’s. Let \(S_n = \sum X_i\). Assuming that \(C\left(\theta_1\right) &gt; C\left(\theta_0\right)\) the test continues until</p>

\[S_n \geq \frac{ \log{A} }{ C\left(\theta_1\right) - C\left(\theta_0\right) } + n\frac{ D\left(\theta_1\right)-D\left(\theta_0\right) }{ C\left(\theta_1\right) - C\left(\theta_0\right) }\]

<p>Or</p>

\[S_n \leq \frac{ \log{A} }{ C\left(\theta_1\right) - C\left(\theta_0\right) } + n\frac{ D\left(\theta_1\right)-D\left(\theta_0\right) }{ C\left(\theta_1\right) - C\left(\theta_0\right) }\]

<p>The following tableshow the SPRT for some common distributions:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Distribution</th>
      <th style="text-align: right">\(f\left(x; \theta\right)\)</th>
      <th style="text-align: right">\(C\left(\theta\right)\)</th>
      <th style="text-align: right">\(D\left(\theta\right)\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Normal</td>
      <td style="text-align: right">\(\frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta\right)^2/2\right)\)</td>
      <td style="text-align: right">\(\theta\)</td>
      <td style="text-align: right">\(\frac{\theta^2}{2}\)</td>
    </tr>
    <tr>
      <td style="text-align: left">Bernoulli</td>
      <td style="text-align: right">\(\theta^{x}\left(1-\theta\right)^{(1-x)}\)</td>
      <td style="text-align: right">\(\log{\left(\frac{\theta}{1-\theta}\right)}\)</td>
      <td style="text-align: right">\(-\log{\left(1-\theta\right)}\)</td>
    </tr>
    <tr>
      <td style="text-align: left">Exponential</td>
      <td style="text-align: right">\(\frac{1}{\theta}e^-x/\theta\)</td>
      <td style="text-align: right">\(\log{\theta}\)</td>
      <td style="text-align: right">\(\theta\)</td>
    </tr>
    <tr>
      <td style="text-align: left">Poisson</td>
      <td style="text-align: right">\(\frac{e^-\theta \theta^x}{x!}\)</td>
      <td style="text-align: right">\(-\frac{1}{\theta}\)</td>
      <td style="text-align: right">\(\log{\theta}\)</td>
    </tr>
  </tbody>
</table>

<h3 id="example-exponential-distribution">Example: Exponential Distribution</h3>

<p>A textbook example is parameter estimation of a probability distribution function. Consider the exponential distribution:</p>

\[f\left(x; \theta\right) = \frac{1}{\theta}e^-x/\theta\]

<p>The hypotheses are</p>

<ul>
  <li>
\[H_0: \theta = \theta_0\]
  </li>
  <li>
\[H_1: \theta = \theta_1\]
  </li>
</ul>

<p>Where \(\theta_1 &gt; \theta_1\).</p>

<p>Then the log-likelihood function (LLF) for one sample is</p>

\[\begin{aligned}
\log{\Lambda\left(x\right)} &amp;= \log{ \frac{1}{\theta_1}e^-x/\theta_1  }{ \frac{1}{\theta_0}e^-x/\theta_0  } 
&amp;= -\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}x
\end{aligned}\]

<p>The cumulative sum of the LLFs for all x is</p>

\[S_n = \sum_n \log{\Lambda\left(x_i\right)} = -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i\]

<p>Accordingly, the stopping rule is:</p>

\[a &lt;  -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b\]

<p>After re-arranging we finally find</p>

\[a + n\log{\frac{\theta_1}{\theta_0}} &lt; \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b + n\log{\frac{\theta_1}{\theta_0}}\]

<p>The thresholds are simply two parallel lines with slope \(\log{\frac{\theta_1}{\theta_0}}\). Sampling should stop when the sum of the samples makes an excursion outside the continue-sampling region.</p>

<h3 id="example-binomial-distribution">Example: Binomial Distribution</h3>

<h3 id="1-likelihood-ratio-for-bernoulli-distribution">1. Likelihood Ratio for Bernoulli Distribution</h3>

<p>For a Bernoulli trial, \(X \in \{0, 1\}\) with probability \(\theta\) for success (\(\theta = 1\)) and \(1 - \theta\) for failure (\(\theta = 0\)).</p>

<ul>
  <li>
    <p>Under hypothesis \(H_1\) with parameter \(\theta_1\):<br />
\(P(X = x | \theta_1) = \theta_1^x (1 - \theta_1)^{1 - x}\)</p>
  </li>
  <li>
    <p>Under hypothesis \(H_0\) with parameter \(\theta_0\):<br />
\(P(X = x | \theta_0) = \theta_0^x (1 - \theta_0)^{1 - x}\)</p>
  </li>
</ul>

<p>The likelihood ratio is:
\(\Lambda(x) = \frac{P(X = x | \theta_1)}{P(X = x | \theta_0)} = \frac{\theta_1^x (1 - \theta_1)^{1 - x}}{\theta_0^x (1 - \theta_0)^{1 - x}}\)</p>

<h3 id="2-log-likelihood-ratio">2. Log-Likelihood Ratio</h3>

<p>The log-likelihood ratio is:
\(\log \Lambda(x) = \log \left( \frac{\theta_1^x (1 - \theta_1)^{1 - x}}{\theta_0^x (1 - \theta_0)^{1 - x}} \right)\)</p>

<p>Breaking this into cases for \(x = 0\) and \(x = 1\):</p>

<p>If \(x = 1\):</p>

\[\log \Lambda(1) = \log \frac{\theta_1}{\theta_0}\]

<p>If \(x = 0\):</p>

\[\log \Lambda(0) = \log \frac{1 - \theta_1}{1 - \theta_0}\]

<p>Thus, the general form for \(x\) is:</p>

\[\log \Lambda(x) = x \log \frac{\theta_1}{\theta_0} + (1 - x) \log \frac{1 - \theta_1}{1 - \theta_0}\]

<h3 id="3-cumulative-log-likelihood-for-n-observations">3. Cumulative Log-Likelihood for \(n\) Observations</h3>

<p>Now we calculate the cumulative log-likelihood ratio for ( n ) independent observations ( x_1, x_2, \ldots, x_n ):
\(S_n = \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i + \log \frac{1 - \theta_1}{1 - \theta_0} \sum_{i=1}^{n} (1 - x_i)\)</p>

<p>This simplifies to:
[
S_n = \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i + \log \frac{1 - \theta_1}{1 - \theta_0} \sum_{i=1}^{n} (1 - x_i)
]</p>

<h3 id="4-stopping-rule">4. Stopping Rule</h3>

<p>The stopping rule in this context is when the cumulative sum ( S_n ) crosses specific thresholds. The inequality becomes:
\(a &lt; \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i + \log \frac{1 - \theta_1}{1 - \theta_0} \sum_{i=1}^{n} (1 - x_i) &lt; b\)</p>

<p>Rearranging this gives:
\(a - n \log \frac{1 - \theta_1}{1 - \theta_0} &lt; \log \frac{\theta_1}{\theta_0} \sum_{i=1}^{n} x_i &lt; b - n \log \frac{1 - \theta_1}{1 - \theta_0}\)</p>

<h3 id="5-interpretation">5. Interpretation</h3>

<p>In this case, the thresholds are two parallel lines with slopes based on the log-ratios \(\log \frac{\theta_1}{\theta_0}\) and \(\log \frac{1 - \theta_1}{1 - \theta_0}\). The sampling process stops when the sum of the observed successes \(\sum x_i\) exits the continuation region bounded by these two lines.</p>

<p>We can summarise this in the following algorithm:</p>

<p>Set \(LR \leftarrow 1\) and \(j \leftarrow 0\)</p>

<ol>
  <li>Increment \(j\)</li>
  <li>If \(𝑋_j=1\) then set \(LR \leftarrow LR \frac{\theta_1}{\theta_0}\)</li>
  <li>If \(𝑋_j=0\) then set \(LR \leftarrow LR \frac{(1−\theta_1)}{(1−\theta_0)}\)</li>
</ol>

<p>What’s \(LR\) at stage 𝑚? Let \(T_m≡ \sum_{j=1}^m 𝑋_j\) then:</p>

\[\frac{\theta_{1𝑚}}{\theta_{0m}} = \frac{\theta_1^{𝑇_𝑚}(1−\theta_1)^{𝑚−𝑇_𝑚}}{\theta_0^{𝑇_𝑚}(1−\theta_0)^{𝑚−𝑇_𝑚}}\]

<p>This is the ratio of binomial probability when \(\theta=\theta_1\) to binomial probability when \(\theta=\theta_0\) (the binomial coefficients in the numerator and denominator cancel). It simplifies further to</p>

\[\frac{\theta_{1𝑚}}{\theta_{0𝑚}}=\left(\frac{(\theta_0}{\theta_1}\right)^{𝑇_m}\left(\frac{1−\theta_0}/{1−\theta_1})\right)^{𝑚−𝑇_𝑚}\]

<p>We conclude \(\theta &gt; \theta_0\) if</p>

\[\frac{\theta_1m}{\theta_0m} \geq \frac{1-\beta}{\alpha}\]

<p>And we conclude \(\theta &lt; \theta_0\) if</p>

\[\frac{\theta_1m}{\theta_0m} \geq \frac{\beta}{1-\alpha}\]

<p>Otherwise, we draw again.</p>

<p>The SPRT approximately minimizes the expected sample size when \(\theta &gt; \theta_0\) or \(\theta &gt; \theta_1\). For values in \(\left(\theta_1,\theta_0\right)\), it can have larger sample sizes than fixed-sample-size tests.</p>

<h3 id="example-normal-distribution">Example: Normal Distribution</h3>

<p>Consider the Normal distribution:</p>

\[f\left(x; \theta\right) = \frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta\right)^2/2\right)\]

<p>The hypotheses are</p>

<ul>
  <li>
\[H_0: \theta = \theta_0\]
  </li>
  <li>
\[H_1: \theta = \theta_1\]
  </li>
</ul>

<p>Where \(\theta_1 &gt; \theta_1\).</p>

<p>Then the log-likelihood function (LLF) for one sample is</p>

\[\begin{aligned}
\log{\Lambda\left(x\right)} &amp;= \log{ \frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta_1\right)^2/2\right)  }{ \\frac{1}{\sqrt{2\pi}}e^\left(-\left(x-\theta_0\right)^2/2\right)  } 
&amp;= -\frac{1}{2}\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{2\theta_1\theta_0}x_i^2 - \frac{\theta_1 - \theta_0}{2}
\end{aligned}\]

<p>The cumulative sum of the LLFs for all x is</p>

\[S_n = \sum_n \log{\Lambda\left(x_i\right)} = -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i\]

<p>Accordingly, the stopping rule is:</p>

\[a &lt;  -n\log{\frac{\theta_1}{\theta_0}} + \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b\]

<p>After re-arranging we finally find</p>

\[a + n\log{\frac{\theta_1}{\theta_0}} &lt; \frac{\theta_1 - \theta_0}{\theta_1\theta_0}\sum_n x_i &lt; b + n\log{\frac{\theta_1}{\theta_0}}\]

<p>The thresholds are simply two parallel lines with slope \(\log{\frac{\theta_1}{\theta_0}}\). Sampling should stop when the sum of the samples makes an excursion outside the continue-sampling region.</p>

<h2 id="the-mixture-sequential-probability-ratio-test-msprt">The Mixture Sequential Probability Ratio Test (mSPRT)</h2>

<p>The main limitation of the Sequential Probability Ratio Test (SPRT) is its requirement for specifying an explicit alternative hypothesis, which might not always align with the goal of merely rejecting a null hypothesis. In contrast, the modified Sequential Probability Ratio Test (mSPRT) offers flexibility in determining sample sizes at the start of an experiment, unlike the traditional SPRT, where sample size calculations are not feasible. Consequently, parameters such as \(N\), \(\alpha\), and \(\beta\) are fixed at the outset of an experiment.</p>

<p>The mixture Sequential Probability Ratio Test (mSPRT) amends these limitations. The test is defined by a “mixing” distribution \(H\) over a parameter space \(\Theta\), where \(H\) is assumed to have a density \(h\) that is positive everywhere. Utilizing \(H\), we first compute the following mixture of likelihood ratios against the null hypothesis \(\theta = \theta_0\):</p>

\[\Lambda_n(s_n) = \int_{\Theta} \left(\frac{f_{\theta}(s_n)}{f_{\theta_0}(s_n)}\right)^n dH(\theta).\]

<p>The mSPRT is parameterized by a mixing distribution \(H\) over \(\Theta\), which is restricted to have an everywhere continuous and positive derivative. Given an observed sample average \(s_n\) up to time \(n\), the likelihood ratio of \(\theta\) against \(\theta_0\) is \(\left(\frac{f_{\theta}(s_n)}{f_{\theta_0}(s_n)}\right)^n\). Thus, we define the mixture likelihood ratio with respect to \(H\) as:</p>

\[\Lambda_n(s_n) = \int_{\Theta} \left(\frac{f_{\theta}(s_n)}{f_{\theta_0}(s_n)}\right)^n dH(\theta).\]

<h3 id="application-to-normal-data">Application to Normal Data</h3>

<p>Considering normal data, for any \(\mu_A\) and \(\mu_B\), the difference \(Z_n = Y_n - X_n\) follows a normal distribution \(\sim N(\theta, 2\sigma^2)\). We can apply the one-variation mSPRT to the sequence \(\{Z_n\}\), leading to the following definition:</p>

\[\sqrt{\frac{2\sigma^2}{2\sigma^2 + n\tau^2}} \exp{\left(\frac{n^2\tau^2\left(X_n - Y_n - \theta_0\right)^2}{4\sigma^2\left(2\sigma^2 + n\tau^2\right)}\right)}.\]

<p>Here’s a small simulation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_mSPRT</span><span class="p">(</span><span class="n">y_data</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">sigma_squared</span><span class="p">,</span> <span class="n">tau_squared</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
    <span class="n">lambda_n</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tau_squared</span><span class="p">))</span> <span class="o">*</span> \
               <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tau_squared</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_mean</span> <span class="o">-</span> <span class="n">x_mean</span> <span class="o">-</span> <span class="n">theta_0</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_squared</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tau_squared</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">lambda_n</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Parameters for synthetic data generation
</span><span class="n">mu_X</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mu_Y</span> <span class="o">=</span> <span class="mi">3</span>  
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>  
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>  

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_X</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sigma_squared</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">tau_squared</span> <span class="o">=</span> <span class="mi">1</span>  
<span class="n">theta_0</span> <span class="o">=</span> <span class="mi">0</span>  

<span class="n">msprt_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">lambda_n</span> <span class="o">=</span> <span class="n">compute_mSPRT</span><span class="p">(</span><span class="n">y_data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">x_data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">sigma_squared</span><span class="p">,</span> <span class="n">tau_squared</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">)</span>
    <span class="n">msprt_values</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambda_n</span><span class="p">)</span>

<span class="n">p_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lambda_val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">msprt_values</span><span class="p">):</span>
    <span class="n">p_values</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">p_values</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_val</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Always Valid p-values'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of Observations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'p-value'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Always Valid p-values Over Time'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">msprt_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'likelihood'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of Observations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'likelihood'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Likelihood Over Time'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="hypothesis-testing" /><summary type="html"><![CDATA[We introduce the mSPRT and give a derivation from a Bayesian point of view.]]></summary></entry><entry><title type="html">“What p-values really mean“</title><link href="http://gregorygundersen.com/blog/2024/01/28/what-p-values-mean/" rel="alternate" type="text/html" title="“What p-values really mean“" /><published>2024-01-28T00:00:00+00:00</published><updated>2024-01-28T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2024/01/28/what-p-values-mean</id><content type="html" xml:base="http://gregorygundersen.com/blog/2024/01/28/what-p-values-mean/"><![CDATA[<p>Most people get <em>p</em>-values wrong. This is how to understand and apply them correctly.</p>

<h2 id="introduction">Introduction</h2>

<p>In the branch of statistics known as hypothesis testing,  a statistical hypothesis is a claim about the value of a parameter in a statistical model: for example  the mean difference in revenue from two random groups of customers visiting a website is exactly zero. In statistics we call the hypothesis you want to reject the null hypothesis. A p-value is a summary of your data; it tells you how likely you are to see data at least as extreme as the data you have collected when your null hypothesis is correct.</p>

<p>A statistical hypothesis test is an algorithm used to calculate a test statistic and a p-value. A large p-value just means that the data was likely to occur under your hypothesis. Whereas a small value means the data was unlikely to have occurred under your hypothesis. This means that p-values are better at rejecting bad hypotheses than confirming good ones.</p>

<p>The above discussion is mostly concerned with how well the data fits the model, and not whether the hypotheses in question are true. Statistics alone cannot answer such questions. Because the data you collect can be consistent with many similar hypotheses, statistics can only tell you which hypothesis could best have generated the data. Not whether the hypotheses were true. Data analysis alone cannot tell you whether the government program was a success, whether the drug should be prescribed to patients, or whether the website really is better with this new design. Statistics can tell you how many people were saved from poverty, how effective the drug is, or whether more people buy your products after a website redesign.</p>

<p>This might be why p-values are the most misunderstood, misinterpreted, and occasionally miscalculated of statistical quantities. No matter how hard statisticians try, what the p-value really means hasn’t broken through to the wider scientific consciousness. In fact, p-values are so difficult to interpret that when the Journal of the American Medical Association surveyed its members in 2007 about how to interpret p-values, none of the available options were correct <a class="citation" href="#windish2007medicine">(Windish et al., 2007)</a>..</p>

<p>Applied researchers are certainly not to blame for misinterpreting p-values: p-values are, after all,  the problem child of statistics—a quantity straightforward to calculate but philosophically and practically difficult to interpret. That being said, we still conduct experiments so we still have to interpret their outcomes—that means we have to interpret p-values.</p>

<h2 id="how-to-interpret-experiment-results-this-example-was-supplied-by-httpswwwscribbrcomstatisticsp-value">How to Interpret Experiment Results (‘This example was supplied by: https://www.scribbr.com/statistics/p-value/’)</h2>

<p>Let’s say you want to know whether there’s a difference in longevity between two groups of mice fed on different diets, diet A and diet B. You can statistically test the difference between these two diets using a two-tailed t-test. We consider the following hypotheses:</p>

<ul>
  <li><strong>Null hypothesis (H0)</strong>: there is no difference in longevity between the two groups.</li>
  <li><strong>Alternative hypothesis (H1)</strong>: there is a difference in longevity between the two groups.</li>
</ul>

<p>If the mice live equally long on either diet, then your test statistic will closely match the test statistic from the null hypothesis (that there is no difference between groups). The resulting p-value could be anything between 0 and 1. However, if there is an average difference in longevity between the two groups, then your test statistic will move further away from the values predicted by the null hypothesis, and the p-value will get smaller. The p-value will never reach zero, because there’s always a possibility that the null hypothesis could generate the data you have seen.</p>

<p>You run the experiment: you randomise the mice into different groups and they receive either diet A or diet B. You find that the lifespan on diet A (M = 2.1 years; SD = 0.12) was shorter than the lifespan on diet B (M = 2.6 years; SD = 0.1), with an average difference of 6 months (t(80) = -12.75; p &lt; 0.01). Your comparison of the two diets results in a p-value of less than 0.01 below your alpha value of 0.05; therefore, you determine that there’s a statistically significant difference between the two diets.</p>

<p>But we should be cautious here. The reported p-value means that there is a 1% chance that these data would occur under the null hypothesis. This is not the same thing as saying that the null hypothesis is not true. It just means that you can reject the hypothesis that both diets produce similar outcomes. Building up a fuller picture would require knowledge about the specific content of the diets. So we can’t rely on the p-value alone to tell us much about how diet affects lifespan.</p>

<p>One of the biggest mistakes in statistics is to over-interpret your p-values. Deciding a policy based on the criteria that p &lt; 0.05, thereby rejecting the null hypothesis , is tantamount to saying that the effect is irrelevant. It puts the cart before the horse. Remember you did the experiment in the first place to find out something else: to measure the magnitude and direction of the effect size.</p>

<p>As an expert you will need to combine the formal results of the experiment with your domain knowledge and come to a conclusion. Sometimes the conclusion might be that the null hypothesis holds true, despite a significant result; and sometimes the treatment works despite the test statistic disagreeing. Beliefs and actions cannot come directly from statistical results, especially results from a single experiment.</p>

<p>Hypothesis tests aren’t the only form of evidence out there, and you will have to gather many different strands to make a decision. Deciding upfront that if your p-value is  &lt; 0.05 then your research program is a success, and a failure otherwise, is unhealthy. It leads researchers to focus their actions on the outcomes at the expense of the process. Only healthy processes will create good results.</p>

<h1 id="references">References</h1>
<ol class="bibliography"><li><span id="windish2007medicine">Windish, D. M., Huot, S. J., &amp; Green, M. L. (2007). Medicine residents’ understanding of the biostatistics and results in the medical literature. <i>Jama</i>, <i>298</i>(9), 1010–1022.</span></li></ol>

<p>Bibliography:</p>

<p>‘Windish DM, Huot SJ, Green ML: Medicine residents’ understanding of the biostatistics and results in the medical literature. JAMA 298:1010- 1022, 2007’</p>]]></content><author><name></name></author><category term="statistics," /><category term="hypothesis" /><category term="testing" /><summary type="html"><![CDATA[Most people get p-values wrong. This is how to understand and apply them correctly.]]></summary></entry><entry><title type="html">An introduction to Gradient Descent</title><link href="http://gregorygundersen.com/blog/2021/10/10/gradient-descent/" rel="alternate" type="text/html" title="An introduction to Gradient Descent" /><published>2021-10-10T00:00:00+00:00</published><updated>2021-10-10T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2021/10/10/gradient-descent</id><content type="html" xml:base="http://gregorygundersen.com/blog/2021/10/10/gradient-descent/"><![CDATA[<p>Gradient Descent is the simplest learning algorithm. It is very easy to implement, is robust to complex cost functions, and is the foundation of an enormous zoo of variations.</p>

<h2 id="introduction">Introduction</h2>

<p>Some ideas are so ubiquitous and all-encompassing that it’s difficult to remember that they were once invented. Other ideas are so simple and apparently obvious that they seem less invented and more handed down by God. Gradient descent is one such idea. It there is hardly a branch of applied mathematics which doesn’t use it, or not it seems to have appeared fully formed as an algorithmic idea for both the primordial mathematical time.</p>

<p>It’s worth noting how Agoras trusted an idea, gradient descent is is it allows the researcher to numerically compute an optimal answer even when they can’t analytically describe the function over which they are optimizing. Seen further, all that is required is some measurement of how our solution does as we form the optimal solution, and that the function we are optimizing over be smooth over our domain of interest.</p>

<p>It’s seems too good to be true! Yet the core issue in many applications such as signal processing, operations research, economics, and statistics boil down to the following minimization problem</p>

\[\mathrm{min}_{\theta \in \mathcal{R^m}} J\left(\theta\right)\]

<p>where \(J\left(\theta\right)\) is what is called the <em>cost function,</em> which measures how well the model parameters \(\theta\) fit to a given dataset. A few examples would be</p>

<ul>
  <li>Logistic regression</li>
</ul>

\[J\left(\theta\right) = \sum_{n} \log{\left(1 + \exp{y_i X_{i}^T\theta}\right)}\]

<ul>
  <li>Linear regression</li>
</ul>

\[J\left(\theta\right) = \frac{1}{2}\lvert\lvert X\theta - y \rvert\rvert^2\]

<ul>
  <li>Composite</li>
</ul>

\[J\left(\theta\right) = f\left(\theta\right) + g\left(\theta\right)\]

<p>with $ f $ being ‘well behaved’ and $ g $ causing us some trouble.</p>

<p>We assume that \(J\left(\theta\right)\) is bounded from below (i.e. the minimum is not \(-\infty\)). This just guarantees the existence of a solution. If $ J $ is convex this is no problem, but if $ J $ is not (and also not smooth) then it’s much more difficult. Always check that a solution exists before wasting time minimising!</p>

<p>There are a couple of ways you could find such an $ \theta $, given a cost function. The most straigtforward is to start with some initital value, and then move in the direction of the negative gradient of the cost funtion:</p>

\[\theta_{k+1} = \theta_{k} - \eta\nabla_{\theta} J\left(\theta\right)\]

<p>with $ \Theta_0 = 0 $. Here. $\eta$ is the learning rate - a tuneable parameter. We terminate the algorithm once</p>

\[\lvert \theta_{k+1} -  \theta_{k} \rvert \leq \varepsilon\]

<p>Intuitively, \(J\left(\theta\right)\) defines a surface over \(\theta\\). We, the end-users, want to find the lowest point on that surface (or the lowest point subject to some intersection constraint). Gradient descent finds that lowest point by constructing a sequence of approximations to \(\theta^*\) (the optimal point), with eath \(\theta_{k}\) (the interations of the algorithm) always in the direction of steepest descent from \(\theta_{k-1}\). We will formalise this intuition and provide performance guarantees later.</p>

<h2 id="linear-regression">Linear Regression</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span> 
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s">'retina'</span>
<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">([</span><span class="s">'seaborn-colorblind'</span><span class="p">,</span> <span class="s">'seaborn-darkgrid'</span><span class="p">])</span>
</code></pre></div></div>

<p>We’ll generate some data that we’ll use for the rest of this post:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">2.25</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">6.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.5</span><span class="p">):</span>
    <span class="s">"""Generate n data points approximating given line.
    m, b: line slope and intercept.
    stddev: standard deviation of added error.
    Returns pair x, y: arrays of length n.
    """</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Suppose you have a set of observations of some process you wanted to model, for example the size of a house labelled as \(x_i \in \mathrm{R}^n\), and the house price labeleld as \(y_i \in \mathrm{R}\), \(i = 1 \ldots m\) (i.e. you have \(m\) examples). One good choice for a model is linear:</p>

\[\hat{y}\left(x\right) = Ax + b\]

<p>The goal is to find some suitable \(A \in \mathcal{R}^n\) and \(b \in \mathcal{R}\), to model this process corectly.</p>

<p>For convenience, let’s make some new definitions:</p>

<p>\(\Theta = [A: b]\)
\(X = [x_1: \ldots : x_m :1]\)
\(Y = [y_1: \ldots : y_m :1]\)</p>

<p>What we’ve done her is to stack our parameters into \(\Theta\) so that it’s a \(\mathrm{R}^{\left(n+1\right)\times 1}\) vector, we have stacked our examples into an \(\mathrm{R}^{\left(n+1\right) \times m}\) matrix, and we have stacked all the outputs into a \(\mathrm{R}^{\left(n+1\right) \times 1}\) vector. Now our hypothesis can be written as</p>

<p>\(\hat{Y} = \Theta^T X\).</p>

<p>Say you also had good reason to believe that the best reconstruction of $$ x $ you could possilby hope to achieve was to minimise the following (mean squared) error measure:</p>

\[J\left(\Theta\right) = \frac{1}{n} \lVert \hat{Y} - Y\rVert_2^2\]

<p>A function to compute the cost is below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MSE</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
    <span class="s">"""Function to compute MSE between true values and estimate
    
    y: true values
    yhat: estimate
    """</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>There are a couple of ways you could find such an $4 \hat{Y} $$, given a cost function. The most straigtforward is to start with some initital value, and then move in the direction of the negative gradient of the cost funtion:</p>

\[\Theta_{k+1} = \Theta_{k} - \alpha\nabla_{\Theta} J\left(\Theta\right)\]

<p>with \(\Theta_0 = 0\). Here. \(\alpha\) is the learning rate - a tuneable parameter. We’ll have more to say about that later.</p>

<p>The following function does exactly this,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">250</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">theta</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
    <span class="k">yield</span> <span class="n">theta</span><span class="p">,</span> <span class="n">cost</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">yhat</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">theta</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>
        <span class="n">yhatt</span> <span class="o">=</span> <span class="n">yhat</span><span class="p">.</span><span class="n">T</span>
        <span class="n">nabla</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">yhatt</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">nabla</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">theta</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">+=</span>  <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">nabla</span>
        <span class="k">yield</span> <span class="n">theta</span><span class="p">,</span> <span class="n">cost</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">yhat</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<p>Some notes on the algorithm: it may be mysterious as to why we divide the learning rate y the number of samples \(m\). But this is a way to average the gradient across all samples. This approach has several important implications:</p>

<p>Scaling of the Gradient: When you compute the gradient of the cost function, you’re essentially summing up the gradients calculated for each individual sample. If you don’t average this sum by dividing by \(m\), the magnitude of the gradient could become very large, especially with a large number of samples. This large gradient could lead to very large steps when updating your parameters, potentially causing overshooting and failure to converge to the minimum of the cost function.</p>

<p>Consistent Learning Rate By dividing by \(m\): you ensure that the learning rate behaves consistently regardless of the number of samples. This means that the learning rate you choose is less dependent on the size of your dataset. Without this scaling, you might need to adjust your learning rate based on the size of your dataset, which is not ideal.</p>

<p>Numerical Stability: Scaling by \(m\) also contributes to numerical stability. It prevents the gradient values from becoming too large, which can cause numerical issues in computation (like overflow or underflow problems).</p>

<p>Interpretability: It makes the learning rate parameter more interpretable and independent of the sample size. This is beneficial when you want to use the same learning rate for datasets of different sizes or when comparing the performance of the algorithm on different datasets.</p>

<p>Now we check graphically if the fitted parameters make sense:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">ones</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">MSE</span><span class="p">)</span>
<span class="n">final</span> <span class="o">=</span> <span class="p">[(</span><span class="n">t</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">]</span>
<span class="n">costs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">final</span><span class="p">]</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">final</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="costs-figure-goes-here">Costs figure goes here</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span>
    <span class="n">array</span><span class="p">([[</span> <span class="mf">2.27769915</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">5.90213934</span><span class="p">]])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s">"r-"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="fit-goes-here">Fit goes here.</h2>

<p>We could also seek to minimise the least absolute deviations of our predictions from the data:</p>

\[J\left(\Theta\right)  = \frac{1}{n} \lVert \hat{Y} - Y\rVert_1\]

<p>a function to do this is included below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MAE</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
    <span class="s">"""Function to compute LAE between true values and estimate
    
    y: true values
    yhat: estimate
    """</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>Finally, if you are minimising the MSE you can compute it analytically via the normal equations (left as an exercise to the reader):</p>

\[\hat{\theta} = (X^T X)^{-1} X^Ty\]

<p>We can do this with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normal_equation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ones</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">"""
    linear regression using the normal equation.
    X: Feature matrix
    y: Target variable array
    ones: Add a col of ones to Feature matrix
    Returns the calculated theta values.
    """</span>
    <span class="c1"># Adding a column of ones to X for the intercept term
</span>    <span class="k">if</span> <span class="n">ones</span><span class="p">:</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">ones</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute the coefficients
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">theta</span>
  
<span class="n">theta_analytic</span> <span class="o">=</span> <span class="n">normal_equation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">theta</span><span class="p">,</span> <span class="n">theta_analytic</span> 
</code></pre></div></div>

<p>You can derive this equation as follows:</p>

<ol>
  <li>Take the (vector) derivative of the cost function to find:</li>
</ol>

\[\nabla_{\theta} J\left(\theta\right) = X\left(X^T\theta - y\right)\]

<ol>
  <li>Set the derivative to zero and rearrange:</li>
</ol>

\[X\left(X^T\theta - y\right) = 0\]

\[X^TX \theta = X^T y\]

\[\theta = \left( X^TX  \right)^{-1} X^T y\]

<p>Intuitivelyy the product \(X^Ty\) is the projection of \(y\) onto the space spanned by the columns of \(X\). This operation translates the target values into the “language” of our features.</p>

<p>The matrix \(X^TX\) is a feature-feature correlation matrix. It’s symmetrical and captures how each feature relates to every other feature. Taking the inverse of this matrix is akin to understanding how to uniquely weigh each feature to best describe the target \(y\). The inverse undoes the mixing of feature influences, allowing us to isolate the effect of each feature.</p>

<p>When you multiply the inverse of \(X^TX\) with \(X^Ty\), you apply the unique weighting to the projection of \(y\) in feature space. This results in the coefficients \(\Theta\) that best map your features to your target in a least-squares sense.</p>

<p>Note that even though this has a nice interpretation calculating \(\left( X^TX  \right)^{-1}\) is prohibitively expensive (it’s on \(\mathcal{O}\left(n^3\right)\) operation). In practice gradient descent is always used.</p>

<h2 id="logistic-regression">Logistic Regression</h2>

<p>Logistic regression is used when we need our outputs to be strictly within the interval \([0, 1]\). It’s usually taught as a classification algorithm, but it’s best thought of as a regression which predictst <strong>probabilities</strong>. Logistic regression minimises the following loss function:</p>

\[J\left(w\right) = y * p\left( y_i | x_i ; w \right) + (1-y) * (1 - p\left( y_i | x_i ; w \right))\]

<p>There is a mathematical justification for why this is the right loss to use, but heuristically, this loss minimises the probability error between the predicition classes and the true classes.</p>

<p>The other major difference is that we choose the sigmoid function. $ \sigma\left(X\theta\right) $ with</p>

\[\sigma\left(z\right) = \frac{1}{1+e^{-z}}\]

<p>instead of \(X^t\theta\) as the hypothesis function. The sigmoid function maps any real-valued number into the range \((0, 1)\), making it useful for a probability estimate. Essentially what we are doing here with logistic regression is doing a linear regression but then transforming the outputs to probability space. We’ll predict the class of each point using softmax (multinomial logistic) regression. The model has a matrix \(W\) of weights, which measures for each feature how likely that feature is to be in a particular. It is of size \(\mathrm{n_{features}} \times \mathrm{n_{classes}}\). The goal of softmax regression is to learn such a matrix. Given a matrix of weights, \(W\), and matrix of points, \(X\), it predicts the probability od each class given the samples.</p>

<p>In code we can do it like this:</p>

<p>```python
	def sigmoid(z):
    return 1 / (1 + jnp.exp(-z))</p>

<p>def logistic_cost(theta, X, y):
    m = len(y)
    h = sigmoid(X @ theta)
    cost = -(1/m) * jnp.sum(y * jnp.log(h) + (1 - y) * jnp.log(1 - h))
    return cost</p>

<p>def logistic_regression(X, y, cost_fn, learning_rate=0.01, num_iters=1000):
    m, n = X.shape
    theta = np.zeros((n, 1))
    y = y[:, 0]
    y = y.reshape(-1, 1)
    cost_history = []</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i in range(num_iters):
    z = X @ theta
    h = sigmoid(z)
    gradient = (1/m) * X.T @ (h - y)
    theta -= learning_rate * gradient
    
    cost = cost_fn(theta, X, y)
    cost_history.append(cost)

return theta, cost_history
</code></pre></div></div>

<p>def make_blobs(num_samples=1000, num_features=2, num_classes=2):
    mu = np.random.rand(num_classes, num_features)
    sigma = np.ones((num_classes, num_features)) * 0.1
    samples_per_class = num_samples // num_classes
    x = np.zeros((num_samples, num_features))
    y = np.zeros((num_samples, num_classes))
    for i in range(num_classes):
        class_samples = np.random.normal(mu[i, :], sigma[i, :], (samples_per_class, num_features))
        x[i * samples_per_class:(i+1) * samples_per_class] = class_samples
        y[i * samples_per_class:(i+1) * samples_per_class, i] = 1
    return x, y</p>

<p>def plot_clusters(x, y, num_classes=2):
    temp = np.argmax(y, axis=1)
    colours = [‘r’, ‘g’, ‘b’]
    for i in range(num_classes):
        x_class = x[temp == i]
        plt.scatter(x_class[:, 0], x_class[:, 1], color=colours[i], s=1)
    plt.show()</p>

<p>NUM_FEATURES=50
NUM_CLASSES=2
NUM_SAMPLES=1000</p>

<p>X, y, = make_blobs(num_samples=NUM_SAMPLES, num_features=NUM_FEATURES, num_classes=NUM_CLASSES)
plot_clusters(X, y, num_classes=NUM_CLASSES)
``</p>

<h2 id="gradient-descent-without-gradients">Gradient Descent without Gradients</h2>

<p>We can see the outline of what we need: a function which represents the cost of our operation, and something to compute the gradients. Fortunately <code class="language-plaintext highlighter-rouge">autograd</code> does exactly this!</p>

<p>According to the webiste, autograd</p>

<blockquote>
  <p>Autograd can automatically differentiate native Python and Numpy code. It can handle a large subset of Python’s features, including loops, ifs, recursion and closures, and it can even take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation), which means it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments, as well as forward-mode differentiation, and the two can be composed arbitrarily. The main intended application of Autograd is gradient-based optimization.</p>
</blockquote>]]></content><author><name></name></author><category term="optimisation" /><summary type="html"><![CDATA[Gradient Descent is the simplest learning algorithm. It is very easy to implement, is robust to complex cost functions, and is the foundation of an enormous zoo of variations.]]></summary></entry><entry><title type="html">Structural Time Series in PyMC!</title><link href="http://gregorygundersen.com/blog/2021/08/13/Structural-Time-Series-in-PyMC3/" rel="alternate" type="text/html" title="Structural Time Series in PyMC!" /><published>2021-08-13T00:00:00+00:00</published><updated>2021-08-13T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2021/08/13/Structural%20Time%20Series%20in%20PyMC3</id><content type="html" xml:base="http://gregorygundersen.com/blog/2021/08/13/Structural-Time-Series-in-PyMC3/"><![CDATA[<p>Bayesian Structural Time Series let us model time series component by component. Here’s how to do it in PyMC.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="n">mpl</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.dates</span> <span class="k">as</span> <span class="n">mdates</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="kn">from</span> <span class="nn">pandas.plotting</span> <span class="kn">import</span> <span class="n">register_matplotlib_converters</span>
<span class="n">register_matplotlib_converters</span><span class="p">()</span>

<span class="n">sns</span><span class="p">.</span><span class="n">set_context</span><span class="p">(</span><span class="s">"notebook"</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">set_style</span><span class="p">(</span><span class="s">"darkgrid"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">passengers</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'passengers.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>
<span class="n">passengers</span><span class="p">[</span><span class="s">'Passengers'</span><span class="p">]</span> <span class="o">=</span> <span class="n">passengers</span><span class="p">[</span><span class="s">'Passengers'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">passengers</span><span class="p">[</span><span class="s">'Month'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">passengers</span><span class="p">[</span><span class="s">'Month'</span><span class="p">])</span>
<span class="n">passengers</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'Month'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">passengers</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x134a79d50&gt;
</code></pre></div></div>

<p><img src="2021-08-13-Structural%20Time%20Series%20in%20PyMC3_files/2021-08-13-Structural%20Time%20Series%20in%20PyMC3_1_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">passengers</span><span class="p">[</span><span class="s">'Passengers'</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>112.0
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">GaussianRandomWalk</span><span class="p">(</span><span class="s">'delta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">144</span><span class="p">,))</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">GaussianRandomWalk</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">delta</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">143</span><span class="p">,),</span> <span class="n">observed</span><span class="o">=</span><span class="n">passengers</span><span class="p">[</span><span class="s">'Passengers'</span><span class="p">])</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [delta]
</code></pre></div></div>

<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
        .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
            background: #F44336;
        }
    </style>
  <progress value="24000" class="" max="24000" style="width:300px; height:20px; vertical-align: middle;"></progress>
  100.00% [24000/24000 00:11&lt;00:00 Sampling 4 chains, 0 divergences]
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 20 seconds.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/tomkealy/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:91: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
  FutureWarning,
/Users/tomkealy/opt/anaconda3/lib/python3.7/site-packages/arviz/plots/traceplot.py:195: UserWarning: rcParams['plot.max_subplots'] (20) is smaller than the number of variables to plot (144), generating only 20 plots
  UserWarning,





array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132062690&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13210eed0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x134d33350&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1321ceed0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13595a9d0&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132154a90&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13229f650&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132384b50&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1323c2f90&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1324bc7d0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1324fcb50&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1325eaed0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132639bd0&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132727cd0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132773850&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132862d10&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1328a6fd0&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13299d990&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1329e0c10&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132ad8610&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132b1ad90&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132c08f50&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132c91a10&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132d57ad0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132da6690&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132e96b50&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x132ed8f50&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x132fcf7d0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x133014b50&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x133101ed0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13314dbd0&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13323ccd0&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1332d0850&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x133395d10&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1333d9fd0&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1334cf990&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x133511c10&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x13360c610&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x13364cd90&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x133739f50&gt;]],
      dtype=object)
</code></pre></div></div>

<p><img src="2021-08-13-Structural%20Time%20Series%20in%20PyMC3_files/2021-08-13-Structural%20Time%20Series%20in%20PyMC3_4_2.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_forecast</span><span class="p">(</span><span class="n">data_df</span><span class="p">,</span>
                  <span class="n">col_name</span><span class="p">,</span>
                  <span class="n">forecast_start</span><span class="p">,</span>
                  <span class="n">forecast_mean</span><span class="p">,</span> 
                  <span class="n">forecast_scale</span><span class="p">,</span> 
                  <span class="n">forecast_samples</span><span class="p">,</span>
                  <span class="n">title</span><span class="p">,</span> 
                  <span class="n">x_locator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                  <span class="n">x_formatter</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""Plot a forecast distribution against the 'true' time series."""</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()</span>
    <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">[</span><span class="n">col_name</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">index</span>

    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_steps_forecast</span> <span class="o">=</span> <span class="n">forecast_mean</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">num_steps_train</span> <span class="o">=</span> <span class="n">num_steps</span> <span class="o">-</span> <span class="n">num_steps_forecast</span>

    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ground truth'</span><span class="p">)</span>

    <span class="n">forecast_steps</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">forecast_start</span><span class="p">:].</span><span class="n">index</span>

    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">forecast_steps</span><span class="p">,</span> 
            <span class="n">forecast_samples</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> 
            <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
            <span class="n">color</span><span class="o">=</span><span class="n">c2</span><span class="p">,</span> 
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">forecast_steps</span><span class="p">,</span> 
            <span class="n">forecast_mean</span><span class="p">,</span> 
            <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
            <span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> 
            <span class="n">color</span><span class="o">=</span><span class="n">c2</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s">'forecast'</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">forecast_steps</span><span class="p">,</span>
                   <span class="n">forecast_mean</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">forecast_scale</span><span class="p">,</span>
                   <span class="n">forecast_mean</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">forecast_scale</span><span class="p">,</span> 
                   <span class="n">color</span><span class="o">=</span><span class="n">c2</span><span class="p">,</span> 
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">forecast_samples</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> <span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">forecast_samples</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">yrange</span> <span class="o">=</span> <span class="n">ymax</span><span class="o">-</span><span class="n">ymin</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="n">ymin</span> <span class="o">-</span> <span class="n">yrange</span><span class="o">*</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">+</span> <span class="n">yrange</span><span class="o">*</span><span class="mf">0.1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_forecast</span><span class="p">(</span>
    <span class="n">passengers</span><span class="p">,</span>
    <span class="s">'Passengers'</span><span class="p">,</span>
    <span class="s">'1959-01-01'</span><span class="p">,</span>
    <span class="n">forecast_mean</span><span class="p">,</span> 
    <span class="n">forecast_scale</span><span class="p">,</span> 
    <span class="n">forecast_samples</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s">'Airplane Passenger Numbers'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"upper left"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Passenger Numbers"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Month"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">autofmt_xdate</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-4-d3135643ac66&gt; in &lt;module&gt;
     54     'Passengers',
     55     '1959-01-01',
---&gt; 56     forecast_mean,
     57     forecast_scale,
     58     forecast_samples,


NameError: name 'forecast_mean' is not defined
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name></name></author><category term="time-series" /><summary type="html"><![CDATA[Bayesian Structural Time Series let us model time series component by component. Here’s how to do it in PyMC.]]></summary></entry><entry><title type="html">Time Series with GAMs</title><link href="http://gregorygundersen.com/blog/2021/08/13/Time-Series-with-GAMS/" rel="alternate" type="text/html" title="Time Series with GAMs" /><published>2021-08-13T00:00:00+00:00</published><updated>2021-08-13T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2021/08/13/Time-Series-with-GAMS</id><content type="html" xml:base="http://gregorygundersen.com/blog/2021/08/13/Time-Series-with-GAMS/"><![CDATA[<p>Generalised Additive Models’ %} are a flexible class of models which improve on polynomial regression. This post shows how to fit time series data with GAMs a bit like <code class="language-plaintext highlighter-rouge">prophet</code>.</p>

<h2 id="introduction">Introduction</h2>

<p>This is a short post introducing Generalised Additive Models (GAMs) - not the nuts and bolts, but some things you can do with them. We will be follwoing this post: https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/ but we won’t go so deep into the theory, all the data come from the github repository linked in the post.</p>

<p>GAMs are a very flexible modelling technique, but unfortunately there isn’t a Python package as good as R’s <code class="language-plaintext highlighter-rouge">mgcv</code> yet. It’s something we’re working on! In this post, I’ll fit a simple GAM using <code class="language-plaintext highlighter-rouge">PyGAM</code> and in a later post I’ll talk about some theory, and some extensions.</p>

<p>GAMs are smooth, semi-parametric models of the form:</p>

\[y = \sum_{i=0}^{n-1} \beta_i f_i\left(x_i\right)\]

<p>where \(y\) is the dependent variable, \(x_i\) are the independent variables, \(\beta\) are the model coefficients, and \(f_i\) are the feature functions. We build the \(f_i\) using a type of function called a spline; splines allow us to automatically model non-linear relationships without having to manually try out many different transformations on each variable.</p>

<p>Nedt we’ll load some data and fit a GAM!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">feather</span>
<span class="kn">from</span> <span class="nn">pygam</span> <span class="kn">import</span> <span class="n">LinearGAM</span>
<span class="kn">from</span> <span class="nn">pygam.utils</span> <span class="kn">import</span> <span class="n">generate_X_grid</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s">'retina'</span>
<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">([</span><span class="s">'seaborn-colorblind'</span><span class="p">,</span> <span class="s">'seaborn-darkgrid'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">feather</span><span class="p">.</span><span class="n">read_dataframe</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>

    <span class="n">weekday_map</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Monday'</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s">'Tuesday'</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s">'Wednesday'</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="s">'Thursday'</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="s">'Friday'</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="s">'Saturday'</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span> <span class="s">'Sunday'</span><span class="p">:</span><span class="mi">7</span><span class="p">}</span>
    <span class="n">df</span><span class="p">[</span><span class="s">'weekday'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'week'</span><span class="p">].</span><span class="nb">map</span><span class="p">(</span><span class="n">weekday_map</span><span class="p">)</span>

    <span class="n">n_type</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'type'</span><span class="p">])</span>
    <span class="n">n_date</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'date_time'</span><span class="p">])</span>
    <span class="n">n_weekdays</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'weekday'</span><span class="p">])</span>
    <span class="n">period</span> <span class="o">=</span> <span class="mi">48</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="s">"2012-02-27"</span>
    <span class="n">end</span> <span class="o">=</span> <span class="s">"2012-03-12"</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'date_time'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">begin</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'date_time'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'type'</span><span class="p">]</span> <span class="o">==</span> <span class="n">n_type</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s">'DT_4_ind.dms'</span><span class="p">)</span>

<span class="n">data</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'date_time'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'value'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
  app.launch_new_instance()





&lt;matplotlib.axes._subplots.AxesSubplot at 0x11f0bbac8&gt;
</code></pre></div></div>

<p><img src="2021-08-13-Time-Series-with-GAMS_files/2021-08-13-Time-Series-with-GAMS_2_2.png" alt="png" /></p>

<p>The above code loads some data, and does a little bit of preprocessing - makes weekday names more legible to humans, and just selects a few weeks of data about ‘Commerical Properties’. You can see that the time series has a lot of structure - exhibiting daily, but also weekly periodicity. There are 48 measurements during the day and 7 days during the week so that will be our independent variables to model response variable - electricity load. Let’s build it!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">period</span> <span class="o">=</span> <span class="mi">48</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># number of observations in the train set
</span><span class="n">window</span> <span class="o">=</span> <span class="n">N</span> <span class="o">/</span> <span class="n">period</span> <span class="c1"># number of days in the train set
</span>
<span class="n">weekly</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'weekday'</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">period</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">daily</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">window</span><span class="p">))</span>

<span class="n">matrix_gam</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'daily'</span><span class="p">,</span> <span class="s">'weekly'</span><span class="p">,</span> <span class="s">'load'</span><span class="p">])</span>
<span class="n">matrix_gam</span><span class="p">[</span><span class="s">'load'</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'value'</span><span class="p">]</span>
<span class="n">matrix_gam</span><span class="p">[</span><span class="s">'daily'</span><span class="p">]</span> <span class="o">=</span> <span class="n">daily</span>
<span class="n">matrix_gam</span><span class="p">[</span><span class="s">'weekly'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weekly</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gam</span> <span class="o">=</span> <span class="n">LinearGAM</span><span class="p">(</span><span class="n">n_splines</span><span class="o">=</span><span class="mi">10</span><span class="p">).</span><span class="n">gridsearch</span><span class="p">(</span><span class="n">matrix_gam</span><span class="p">[[</span><span class="s">'daily'</span><span class="p">,</span> <span class="s">'weekly'</span><span class="p">]],</span> <span class="n">matrix_gam</span><span class="p">[</span><span class="s">'load'</span><span class="p">])</span>
<span class="n">XX</span> <span class="o">=</span> <span class="n">generate_X_grid</span><span class="p">(</span><span class="n">gam</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100% (11 of 11) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">set_figheight</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">set_figwidth</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s">'daily'</span><span class="p">,</span> <span class="s">'weekly'</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">pdep</span><span class="p">,</span> <span class="n">confi</span> <span class="o">=</span> <span class="n">gam</span><span class="p">.</span><span class="n">partial_dependence</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="p">.</span><span class="mi">95</span><span class="p">)</span>
    <span class="n">confi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">confi</span><span class="p">)</span>
    <span class="n">confi</span> <span class="o">=</span> <span class="n">confi</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">XX</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">pdep</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">XX</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">confi</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="2021-08-13-Time-Series-with-GAMS_files/2021-08-13-Time-Series-with-GAMS_6_0.png" alt="png" /></p>

<p>This is good! You can see that the electricity load follows an approximate <code class="language-plaintext highlighter-rouge">sin</code> pattern during the day, and that the electricity load falls off during the week! If we’d tried using a linear model to do this, we’d have had to build these features manually - the good thing about GAMs is that they do this for us. Let’s visualise the fit.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="n">gam</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">matrix_gam</span><span class="p">[[</span><span class="s">'daily'</span><span class="p">,</span> <span class="s">'weekly'</span><span class="p">]])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'date_time'</span><span class="p">],</span> <span class="n">matrix_gam</span><span class="p">[</span><span class="s">'load'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'date_time'</span><span class="p">],</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="s">'vertical'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'True'</span><span class="p">,</span> <span class="s">'Predicted'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x11eb87f28&gt;
</code></pre></div></div>

<p><img src="2021-08-13-Time-Series-with-GAMS_files/2021-08-13-Time-Series-with-GAMS_9_1.png" alt="png" /></p>

<p>Alas, this isn’t the best fit, but it’ll do!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name></name></author><category term="curve-fitting," /><category term="GAMs" /><summary type="html"><![CDATA[Generalised Additive Models’ %} are a flexible class of models which improve on polynomial regression. This post shows how to fit time series data with GAMs a bit like prophet.]]></summary></entry><entry><title type="html">Understanding the Guts of Generalized Additive Models (GAMs) with Hands-on Examples.</title><link href="http://gregorygundersen.com/blog/2021/08/10/The-Guts-Of-GAMs/" rel="alternate" type="text/html" title="Understanding the Guts of Generalized Additive Models (GAMs) with Hands-on Examples." /><published>2021-08-10T00:00:00+00:00</published><updated>2021-08-10T00:00:00+00:00</updated><id>http://gregorygundersen.com/blog/2021/08/10/The-Guts-Of-GAMs</id><content type="html" xml:base="http://gregorygundersen.com/blog/2021/08/10/The-Guts-Of-GAMs/"><![CDATA[<p>Generalised Additive Models look harder than they actually are. We fit a single dimension GAM from scratch, including generating splines.</p>

<p>This post will explain  the internals of (generalised) additive models (GAMs): how to estimate the feature functions. We first explain <em>why</em> we want to fit additive models, and then we go on to explain how they are estimated in practice via splines (we will also explain what a spline is).  Finally we’ll fit simple splines on wage data, then we’ll go on to fit more complicated splines on some accelerometer data with a highly non-linear realtionship between in the input and the output.</p>

<h1 id="introduction">Introduction</h1>

<p>The additive model for regression expresses the conditional expectation function as a sum of partial response functions, one for each predictor variable. The idea is that each input feature makes a separate contribution to the response, and these contributions just add up (hence the name “partial response function”). However, these contributions don’t have to be strictly proportional to the inputs.</p>

<p>Formally, when the vector \(\mathbf{X}\) of predictor variables has \(p\) dimensions, \(x_1, \ldots, x_p\), the model is given by:</p>

\[E[Y|\mathbf{X} = \mathbf{x}] = \alpha + \sum_{j=1}^{p} f_j(x_j)\]

<p>This  includes the linear model as a special case, where \(f_j(x_j) = \beta_j x_j\), but it’s clearly more general because the \(f_j\)’s can be arbitrary nonlinear functions.</p>

<p>To make the model identifiable, we add a restriction. Without loss of generality, we assume:</p>

\[E[Y] = \alpha \quad \text{and} \quad E[f_j(X_j)] = 0\]

<p>To see why this restriction is necessary, consider a simple case where \(p = 2\). If we add constants \(c_1\) to \(f_1\) and \(c_2\) to \(f_2\), but subtract \(c_1 + c_2\) from \(\alpha\), nothing observable changes in the model. This kind of degeneracy or lack of identifiability is similar to the way collinearity prevents us from defining true slopes in linear regression. However, it’s less harmful than collinearity because we can resolve it by imposing this constraint.</p>

<h1 id="why-would-we-do-this">Why would we do this!?</h1>

<p>Before we dive into how additive models work in practice, let’s take a step back and understand <em>why</em> we might want to use them. Think of regression models as existing on a spectrum, with linear models on one end and fully nonparametric models on the other. Additive models sit somewhere in between.</p>

<p>Linear regression is fast—it converges quickly as we get more data, but it assumes the world is simple and linear, which it almost never is. Even with infinite data, a linear model will make systematic mistakes due to its inability to capture non-linear relationships. Its mean squared error (MSE) shrinks at a rate of \(O(n^{-1})\), and this speed is unaffected by the number of parameters.</p>

<p>At the opposite extreme, fully nonparametric methods like kernel regression or k-nearest neighbors make no such assumptions. They can capture complex shapes, but they pay for that flexibility with much slower convergence, especially in higher dimensions. In one dimension, these models converge at a rate of \(O(n^{-4/5})\), but in higher dimensions, the rate drops dramatically to something like \(O(n^{-1/26})\) in 100 dimensions—this is the notorious “curse of dimensionality.” Essentially, the more features you have, the more data you need to maintain the same level of precision, and the data demands grow exponentially.</p>

<p>So where do additive models fit in? They balance these two extremes. Additive models allow for some non-linearity while avoiding the curse of dimensionality by estimating each component function \(f_j(x_j)\) independently, using one-dimensional smoothing techniques. The result is a model that converges almost as fast as parametric ones—at a rate of \(O(n^{-4/5})\), rather than the much slower rate of fully nonparametric models. Sure, there’s a little approximation bias, but the trade-off is worth it. In practice, additive models often outperform linear models once you have enough data, because they capture more of the underlying structure without demanding the impossible amount of data required by fully nonparametric approaches.</p>

<p>This is where additive models shine: they hit that sweet spot between simplicity and flexibility, giving you enough structure to work with high-dimensional data, but with far less approximation bias than linear models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">patsy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="n">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="n">smf</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>
<h1 id="splines">Splines</h1>

<p>We build the \(f_i\) using a type of function called a spline; splines allow us to automatically model non-linear relationships without having to manually try out many different transformations on each variable. The name “spline” comes from a tool used by craftsmen to draw smooth curves—a thin, flexible strip of material like soft wood. You pin it down at specific points, called knots, and let it bend between them.</p>

<p>Bending a spline takes energy—the stiffer the material, the more energy is needed to bend it into shape. A stiffer spline creates a straighter curve between points. For smoothing splines, increasing \(\lambda\) corresponds to using a stiffer material.</p>

<p>We have data points \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\), and we want to find a good approximation \(\hat{\mu}\) to the true conditional expectation or regression function \(\mu\). In many regression setting we control how smooth we made \(\hat{\mu}\) indirectly. For instance, in kernel regression, we use a bandwidth parameter to adjust the size of the neighborhood around each data point that contributed to the prediction. A larger bandwidth would create a smoother curve by averaging over more data points, but this could lead to underfitting, where the model fails to capture important details in the data. On the other hand, a smaller bandwidth creates a more jagged, detailed curve, which might overfit the data by following every minor fluctuation.</p>

<p>In \(k\)-nearest neighbors (k-NN) regression, the smoothness of the prediction depends on how many neighbors \(k\) we use to make predictions. A higher value of \(k\) effectively increases the “bandwidth,” as we are averaging over more points, which smooths out the predictions. This can lead to a smoother but less flexible model that might miss important patterns in the data (underfitting). A lower value of \(k\) results in predictions based on fewer neighbors, leading to more jagged, less smooth results, as the model reacts to small variations in the data (overfitting).</p>

<p>Why not be more direct, and control smoothness itself?</p>

<p>A natural way to do this is by minimizing the spline objective function:</p>

\[L(m, \lambda) \equiv \frac{1}{n} \sum_{i=1}^{n} \left( y_i - m(x_i) \right)^2 + \lambda \int \left( m''(x) \right)^2 dx \tag{spline_cost}\]

<p>The first term here is just the mean squared error (MSE) of using the curve \(m(x)\) to predict \(y\). The second term, penalises the MSE in the direction of smoother curves. \(m''(x)\) is the second derivative of \(m\) with respect to \(x\)—it would be zero if \(m\) were linear, so this term measures the curvature of \(m\) at \(x\). The sign of \(m''(x)\) tells us whether the curvature is concave or convex, but we don’t care about that, so we square it. Then, we integrate this over all \(x\) to get the total curvature. Finally, we multiply by \(lambda\) and add that to the MSE. This adds a penalty to the MSE criterion—given two functions with the same MSE, we prefer the one with less average curvature. In practice, we’ll accept changes in \(m\) that increase the MSE by 1 unit if they reduce the average curvature by at least \(lambda\).</p>

<p>The curve or function that solves this minimization problem:</p>

\[\hat{\mu}_{\lambda} = \arg \min_m L(m, \lambda) \tag{7.2}\]

<p>is called a smoothing spline or spline curve.</p>

<p>All solutions to the spline cost equation no matter what the data, are piecewise cubic polynomials that are continuous and have continuous first and second derivatives. That is, \(\hat{\mu}\), \(\hat{\mu}'\), and \(\hat{\mu}''\) are all continuous. The boundaries between the pieces are the original data points, and, like the craftsman’s spline, these boundary points are called the knots of the smoothing spline. The function remains continuous beyond the largest and smallest data points but is always linear in those regions.</p>

<p>Let’s think a little more about the optimisation proboem. There are two limits to consider: as \(\lambda \to \infty\) and As \(\lambda \to 0\). As \(\lambda \to \infty\), any curvature at all becomes infinitely costly, and only linear functions are allowed. This makes sense because minimizing mean squared error (MSE) with linear functions is simply <strong>Ordinary Least Squares (OLS)</strong> regression. So, we understand that in this limit, the smoothing spline behaves just like a linear regression model.</p>

<p>On the other hand, as \(\lambda \to 0\), we don’t care about curvature at all. In this case, we can always come up with a function that just <strong>interpolates between the data points</strong>—this is called an <strong>interpolation spline</strong>, which passes exactly through each point. More specifically, of the infinitely many functions that interpolate between those points, we pick the one with the minimum average curvature.</p>

<p>At intermediate values of \(\lambda\), the smoothing spline \(\hat{\mu}_{\lambda}\) becomes a compromise between fitting the data closely and keeping the curve smooth. The larger we make \(\lambda\), the more heavily we penalize curvature. There’s a clear <strong>bias-variance trade-off</strong> here:</p>

<ul>
  <li>As \(\lambda\) grows, the spline becomes less sensitive to the data. It’s <strong>less wiggly</strong>, meaning lower variance in its predictions but higher bias since it may fail to capture intricate patterns.</li>
  <li>As \(\lambda\) shrinks, the bias decreases, but the spline becomes <strong>more sensitive</strong> to fluctuations in the data, resulting in higher variance.</li>
</ul>

<p>For example, if \(\lambda\) is large, the spline may resemble a simple line that overlooks small dips and peaks in the data. If \(\lambda\) is small, the spline may start capturing noise and overfitting, creating a curve that twists excessively between points.</p>

<p>For consistency, we want to let \(\lambda \to 0\) as \(n \to \infty\), just as we allow the bandwidth \(h \to 0\) in kernel smoothing when \(n \to \infty\). This allows the spline to capture more complexity as we gather more data, while still avoiding overfitting.</p>

<p>The way to think about smoothing splines is as functions that minimize MSE, but with a constraint on the average curvature. This follows a general principle in optimization where <strong>penalized optimization</strong> corresponds to <strong>optimization under constraints</strong>.  The short version is that each level of \(\lambda\) corresponds to setting a cap on how much curvature the function is allowed to have on average. The spline we fit for a particular \(\lambda\) is the MSE-minimizing curve, subject to that constraint. As we get more data, we can <strong>relax the constraint</strong> (by letting \(\lambda\) shrink) without sacrificing reliable estimation.</p>

<p>It should come as no surprise that we select \(\lambda\) by <strong>cross-validation</strong>. Ordinary <strong>k-fold cross-validation</strong> works, but <strong>leave-one-out CV</strong> performs well for splines. In fact, the default in most spline software is either leave-one-out CV or the faster approximation called <strong>generalized cross-validation (GCV)</strong>.</p>

<h1 id="fitting-splines">Fitting Splines</h1>

<p>Splines are piecewise cubic polynomials. To see how to fit them, let’s think about how to fit a global cubic polynomial. We would define four basis functions,</p>

\[B_1(x) = 1\]

\[B_2(x) = x\]

\[B_3(x) = x^2\]

\[B_4(x) = x^3\]

<p>and choose to only consider regression functions that are linear combinations of the basis functions:</p>

\[\mu(x) = \sum_{j=1}^4 \beta_j B_j(x)\]

<p>Such regression functions would be linear in the transformed variables \(B_1(x), \dots, B_4(x)\), even though it is nonlinear in \(x\).</p>

<p>To estimate the coefficients of the cubic polynomial, we would apply each basis function to each data point \(x_i\) and gather the results in an \(n \times 4\) matrix \(B\):</p>

\[B_{ij} = B_j(x_i)\]

<p>Then we would do OLS using the \(B\) matrix in place of the usual data matrix \(x\):</p>

\[\hat{\beta} = (B^T B)^{-1} B^T y\]

<p>Since splines are piecewise cubics, things proceed similarly, but we need to be a little more careful in defining the basis functions. Recall that we have \(n\) values of the input variable \(x\), \(x_1 , x_2 , \dots, x_n\). Assume that these are in increasing order, because it simplifies the notation. These \(n\) “knots” define \(n + 1\) pieces or segments: \(n - 1\) of them between the knots, one from \(-\infty\) to \(x_1\), and one from \(x_n\) to \(+\infty\). A third-order polynomial on each segment would seem to need a constant, linear, quadratic, and cubic term per segment. So the segment running from \(x_i\) to \(x_{i+1}\) would need the basis functions</p>

\[1(x_i,x_{i+1})(x), \, (x - x_i) 1(x_i,x_{i+1})(x), \, (x - x_i)^2 1(x_i,x_{i+1})(x), \, (x - x_i)^3 1(x_i,x_{i+1})(x)\]

<p>where the indicator function \(1(x_i,x_{i+1})(x)\) is 1 if \(x \in (x_i,x_{i+1})\) and 0 otherwise. This makes it seem like we need \(4(n + 1) = 4n + 4\) basis functions.</p>

<p>However, we know from linear algebra that the number of basis vectors we need is equal to the number of dimensions of the vector space. The number of adjustable coefficients for an arbitrary piecewise cubic with \(n + 1\) segments is indeed \(4n + 4\), but splines are constrained to be smooth. The spline must be continuous, which means that at each \(x_i\), the value of the cubic from the left, defined on \((x_{i-1},x_i)\), must match the value of the cubic from the right, defined on \((x_i, x_{i+1})\). This gives us one constraint per data point, reducing the number of adjustable coefficients to at most \(3n + 4\). Since the first and second derivatives are also continuous, we are down to just \(n + 4\) coefficients. Finally, we know that the spline function is linear outside the range of the data, i.e., on \((-\infty,x_1)\) and on \((x_n,\infty)\), lowering the number of coefficients to \(n\). There are no more constraints, so we end up needing only \(n\) basis functions. And in fact, from linear algebra, any set of \(n\) piecewise cubic functions which are linearly independent can be used as a basis.</p>

<p>One common choice is:</p>

\[B_1(x) = 1\]

\[B_2(x) = x\]

\[B_{i+2}(x) = \frac{(x - x_i)^3_+ - (x - x_n)^3_+}{x_n - x_i} - \frac{(x - x_{n-1})^3_+ - (x - x_n)^3_+}{x_n - x_{n-1}}\]

<p>where \((a)_+ = a\) if \(a &gt; 0\), and \(= 0\) otherwise. This rather unintuitive-looking basis has the nice property that the second and third derivatives of each \(B_j\) are zero outside the interval \((x_1, x_n)\).</p>

<p>Now that we have our basis functions, we can once again write the spline as a weighted sum of them:</p>

\[m(x) = \sum_{j=1}^m \beta_j B_j(x)\]

<p>and put together the matrix \(B\) where \(B_{ij} = B_j(x_i)\). We can write the spline objective function in terms of the basis functions:</p>

\[L = (y - B\beta)^T (y - B\beta) + n\lambda\beta^T \Omega\beta\]

<p>where the matrix \(\Omega\) encodes information about the curvature of the basis functions:</p>

\[\Omega_{jk} = \int B_j''(x) B_k''(x) dx \tag{7.16}\]

<p>Notice that only the quadratic and cubic basis functions will make non-zero contributions to \(\Omega\). With the choice of basis above, the second derivatives are non-zero on, at most, the interval \((x_1,x_n)\), so each of the integrals in \(\Omega\) is going to be finite. This is something we (or, realistically, R) can calculate once, no matter what \(\lambda\) is. Now we can find the smoothing spline by differentiating with respect to \(\beta\):</p>

\[0 = -2B^T y + 2B^T B\hat{\beta} + 2n\lambda\Omega\hat{\beta}\]

\[B^T y = (B^T B + n\lambda\Omega) \hat{\beta}\]

\[\hat{\beta} = (B^T B + n\lambda\Omega)^{-1} B^T y\]

<p>Notice, incidentally, that we can now show splines are linear smoothers:</p>

\[\hat{\mu}(x) = B\hat{\beta} = B(B^T B + n\lambda\Omega)^{-1} B^T y \tag{7.20}\]

<p>Once again, if this were ordinary linear regression, the OLS estimate of the coefficients would be \((x^T x)^{-1} x^T y\). In comparison to that, we’ve made two changes. First, we’ve substituted the basis function matrix \(B\) for the original matrix of independent variables, \(x\)—a change we’d have made already for a polynomial regression. Second, the “denominator” is not \(x^T x\), or even \(B^T B\), but \(B^T B + n\lambda\Omega\). Since \(x^T x\) is \(n\) times the covariance matrix of the independent variables, we are taking the covariance matrix of the spline basis functions and adding some extra covariance—how much depends on the shapes of the functions (through \(\Omega\)) and how much smoothing we want to do (through \(\lambda\)). The larger we make \(\lambda\), the less the actual data matters to the fit.</p>

<p>In addition to explaining how splines can be fit quickly (do some matrix arithmetic), this illustrates two important tricks. One, which we won’t explore further here, is to turn a nonlinear regression problem into one which is linear in another set of basis functions. This is like using not just one transformation of the input variables, but a whole library of them, and letting the data decide which transformations are important. There remains the issue of selecting the basis functions, which can be quite tricky. In addition to the spline basis, most choices are various sorts of waves—sine and cosine waves of different frequencies, various wave-forms of limited spatial extent (“wavelets”), etc. The ideal is to choose a function basis where only a few non-zero coefficients would need to be estimated, but this requires some understanding of the data.</p>

<p>The other trick is that of stabilizing an unstable estimation problem by adding a penalty term. This reduces variance at the cost of introducing some bias.</p>

<h1 id="example-wage-data">Example: Wage Data</h1>

<p>First of all, we’ll use <code class="language-plaintext highlighter-rouge">patsy</code> to construct a few spline bases and fit generalised linear models with <code class="language-plaintext highlighter-rouge">statsmodels</code>. Then, we’ll dive into constructing splines ourselves; following Simon Wood’s book we’ll use penalised regression splines.</p>

<p>Firstly, we’ll use <code class="language-plaintext highlighter-rouge">patsy</code> to create some basic pline models. The data we’re using comes from https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Wage.html. It’s plotted below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'Wage.csv'</span><span class="p">)</span>
<span class="n">age_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">max</span><span class="p">()).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.collections.PathCollection at 0x11d0a5898&gt;
</code></pre></div></div>

<p><img src="2021-08-10-The-Guts-Of-GAMs_files/2021-08-10-The-Guts-Of-GAMs_3_1.png" alt="png" /></p>

<p>GAMs are essentially linear models, but in a very special (and useful!) basis made of regression splines. We can use the <code class="language-plaintext highlighter-rouge">bs()</code> function in <code class="language-plaintext highlighter-rouge">patsy</code> to create such a basis for us:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transformed_x1</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"bs(df.age, knots=(25,40,60), degree=3, include_intercept=False)"</span><span class="p">,</span> <span class="p">{</span><span class="s">"df.age"</span><span class="p">:</span> <span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">)</span>
<span class="n">fit1</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">transformed_x1</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fit1</span><span class="p">.</span><span class="n">params</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Intercept                                                               60.493714
bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[0]     3.980500
bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[1]    44.630980
bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[2]    62.838788
bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[3]    55.990830
bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[4]    50.688098
bs(df.age, knots=(25, 40, 60), degree=3, include_intercept=False)[5]    16.606142
dtype: float64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">age_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">max</span><span class="p">()).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">fit1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"bs(age_grid, knots=(25,40,60), include_intercept=False)"</span><span class="p">,</span>
<span class="p">{</span><span class="s">"age_grid"</span><span class="p">:</span> <span class="n">age_grid</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Specifying three knots'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">85</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">350</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'age'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'wage'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0,0.5,'wage')
</code></pre></div></div>

<p><img src="2021-08-10-The-Guts-Of-GAMs_files/2021-08-10-The-Guts-Of-GAMs_7_1.png" alt="png" /></p>

<p>Here we have prespecified knots at ages 25, 40, and 60. This produces a spline with six basis functions. A cubic spline has 7 degrees of freedom: one for the intercept, and two for each order. We could also have specified knot points at uniform quantiles of the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Specifying 6 degrees of freedom
</span><span class="n">transformed_x2</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"bs(df.age, df=6, include_intercept=False)"</span><span class="p">,</span>
<span class="p">{</span><span class="s">"df.age"</span><span class="p">:</span> <span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">)</span>
<span class="n">fit2</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">transformed_x2</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>
<span class="n">fit2</span><span class="p">.</span><span class="n">params</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Intercept                                       56.313841
bs(df.age, df=6, include_intercept=False)[0]    27.824002
bs(df.age, df=6, include_intercept=False)[1]    54.062546
bs(df.age, df=6, include_intercept=False)[2]    65.828391
bs(df.age, df=6, include_intercept=False)[3]    55.812734
bs(df.age, df=6, include_intercept=False)[4]    72.131473
bs(df.age, df=6, include_intercept=False)[5]    14.750876
dtype: float64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">age_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">max</span><span class="p">()).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">fit2</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"bs(age_grid, df=6, include_intercept=False)"</span><span class="p">,</span>
<span class="p">{</span><span class="s">"age_grid"</span><span class="p">:</span> <span class="n">age_grid</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Specifying three knots'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">85</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">350</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'age'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'wage'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0,0.5,'wage')
</code></pre></div></div>

<p><img src="2021-08-10-The-Guts-Of-GAMs_files/2021-08-10-The-Guts-Of-GAMs_10_1.png" alt="png" /></p>

<p>Finally, we can also fit natural splines with the <code class="language-plaintext highlighter-rouge">cr()</code> function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Specifying 4 degrees of freedom
</span><span class="n">transformed_x3</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"cr(df.age, df=4)"</span><span class="p">,</span> <span class="p">{</span><span class="s">"df.age"</span><span class="p">:</span> <span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">)</span>
<span class="n">fit3</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">transformed_x3</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>
<span class="n">fit3</span><span class="p">.</span><span class="n">params</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Intercept             -6.970341e+13
cr(df.age, df=4)[0]    6.970341e+13
cr(df.age, df=4)[1]    6.970341e+13
cr(df.age, df=4)[2]    6.970341e+13
cr(df.age, df=4)[3]    6.970341e+13
dtype: float64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred</span> <span class="o">=</span> <span class="n">fit3</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"cr(age_grid, df=4)"</span><span class="p">,</span> <span class="p">{</span><span class="s">"age_grid"</span><span class="p">:</span> <span class="n">age_grid</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Natural spline df=4'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">85</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">350</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'age'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'wage'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0,0.5,'wage')
</code></pre></div></div>

<p><img src="2021-08-10-The-Guts-Of-GAMs_files/2021-08-10-The-Guts-Of-GAMs_13_1.png" alt="png" /></p>

<p>Let’s see how these fits all stack together:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generate a sequence of age values spanning the range
</span><span class="n">age_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">.</span><span class="nb">max</span><span class="p">()).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Make some predictions
</span><span class="n">pred1</span> <span class="o">=</span> <span class="n">fit1</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"bs(age_grid, knots=(25,40,60), include_intercept=False)"</span><span class="p">,</span>
<span class="p">{</span><span class="s">"age_grid"</span><span class="p">:</span> <span class="n">age_grid</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">))</span>
<span class="n">pred2</span> <span class="o">=</span> <span class="n">fit2</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"bs(age_grid, df=6, include_intercept=False)"</span><span class="p">,</span>
<span class="p">{</span><span class="s">"age_grid"</span><span class="p">:</span> <span class="n">age_grid</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">))</span>
<span class="n">pred3</span> <span class="o">=</span> <span class="n">fit3</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">"cr(age_grid, df=4)"</span><span class="p">,</span> <span class="p">{</span><span class="s">"age_grid"</span><span class="p">:</span> <span class="n">age_grid</span><span class="p">},</span> <span class="n">return_type</span><span class="o">=</span><span class="s">'dataframe'</span><span class="p">))</span>
<span class="c1"># Plot the splines and error bands
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">wage</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'None'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span> <span class="n">pred1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Specifying three knots'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span> <span class="n">pred2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Specifying df=6'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span> <span class="n">pred3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Natural spline df=4'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">85</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">350</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'age'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'wage'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0,0.5,'wage')
</code></pre></div></div>

<p><img src="2021-08-10-The-Guts-Of-GAMs_files/2021-08-10-The-Guts-Of-GAMs_15_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">patsy</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="n">sp</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">statsmodels</span> <span class="kn">import</span> <span class="n">api</span> <span class="k">as</span> <span class="n">sm</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'mcycle.csv'</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Unnamed: 0'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">blue</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">times</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">accel</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'time'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Acceleration'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0,0.5,'Acceleration')
</code></pre></div></div>

<p><img src="2021-08-10-The-Guts-Of-GAMs_files/2021-08-10-The-Guts-Of-GAMs_18_1.png" alt="png" /></p>

<p>As discussed earlier: GAMs are smooth, semi-parametric models of the form:
​
\(y = \sum_{i=0}^{n-1} \beta_i f_i\left(x_i\right)\)
​
where \(y\) is the dependent variable, \(x_i\) are the independent variables, \(\beta\) are the model coefficients, and \(f_i\) are the feature functions.
​
We build the \(f_i\) using a type of function called a spline. Since our data is 1D, we can model it as:</p>

\[y = \beta_0 + f\left( x \right) + \varepsilon\]

<p>We must also choose a basis for \( f \):</p>

\[f \left( x \right) = \beta_1 B_1\left(x\right) + \ldots + \beta_k B_k\left(x\right)\]

<p>We define</p>

\[X = \left[1, x_1,  \ldots,  x_k \right]\]

<p>so we can write:</p>

<p>$ y = \beta_0 + f\left( x \right) + \varepsilon = X\beta + \varepsilon $$</p>

<p>We choose to minimise the sum of squares again, this time with a regularisation term:</p>

\[\frac{1}{2} \lVert y - X\beta \rVert + \lambda \int_0^1 f''\left(x\right)^2 dx\]

<p>You can show (you, not me!) that the second term can always be written:</p>

\[\int_0^1 f''\left(x\right)^2 dx = \beta^T S \beta\]

<p>where \( S \) is a postive (semi)-definiate matrix (i.e. all it’s eigenvalues are positive or 0). Therefore our objective function becomes:</p>

\[\frac{1}{2} \lVert y - X\beta \rVert + \lambda \beta^T S \beta dx\]

<p>and we can use the techniques we’ve developed fitting linear models to fit additive models! We’ll start by fitting a univariate spline, then maybe something more complicated.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">R</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">z</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">12</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">12</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span> <span class="o">-</span> <span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">7</span> <span class="o">/</span> <span class="mi">240</span><span class="p">)</span> <span class="o">/</span> <span class="mi">24</span>

<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">frompyfunc</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">R_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">R</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">knots</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">knots</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">times</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">q</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="n">dmatrices</span><span class="p">(</span><span class="s">'accel ~ times + R_(times)'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">q</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">q</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">S</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="n">R_</span><span class="p">(</span><span class="n">knots</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="n">B</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">real_if_close</span><span class="p">(</span><span class="n">sp</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">sqrtm</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">2</span><span class="p">:]),</span> <span class="n">tol</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">8</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/thomas.kealy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ComplexWarning: Casting complex values to real discards the imaginary part
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># build the augmented matrices
</span>    <span class="n">y_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">q</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
    <span class="n">X_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)</span> <span class="o">*</span> <span class="n">B</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="n">X_</span><span class="p">).</span><span class="n">fit</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">min_time</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">times</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span>
<span class="n">max_time</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">times</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>

<span class="n">plot_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_time</span><span class="p">,</span> <span class="n">max_time</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plot_X</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s">'times + R_(times)'</span><span class="p">,</span> <span class="p">{</span><span class="s">'times'</span><span class="p">:</span> <span class="n">plot_x</span><span class="p">})</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">blue</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">times</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">accel</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plot_x</span><span class="p">,</span> <span class="n">results</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">plot_X</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'time'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'accel'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\lambda = {}$'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5,1,'$\\lambda = 1.0$')
</code></pre></div></div>

<p><img src="2021-08-10-The-Guts-Of-GAMs_files/2021-08-10-The-Guts-Of-GAMs_27_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name></name></author><category term="GAMs" /><summary type="html"><![CDATA[Generalised Additive Models look harder than they actually are. We fit a single dimension GAM from scratch, including generating splines.]]></summary></entry></feed>